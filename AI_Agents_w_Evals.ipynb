{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/AI-Agents/blob/main/AI_Agents_w_Evals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install anthropic\n",
        "%pip install openai\n",
        "%pip install -q -U google-generativeai\n",
        "%pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDKcVFI7PAJ5",
        "outputId": "539bca9d-2959-427d-f6a3-e10908487253"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.51.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
            "Downloading anthropic-0.51.0-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.0/264.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.51.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup and Imports\n",
        "import anthropic\n",
        "import google.generativeai as gemini\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import glob # For finding files matching a pattern\n",
        "import uuid # For generating unique learning IDs in RAG\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "from google.colab import drive # For Google Drive mounting\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union, Tuple\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "# LLM API Keys (keep as is)\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "gemini.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "ANTHROPIC_MODEL_NAME = \"claude-3-5-sonnet-latest\"\n",
        "OPENAI_MODEL_NAME = \"gpt-4.1\" # Or your preferred GPT-4 class model\n",
        "EVAL_MODEL_NAME = \"gemini-2.5-pro-preview-05-06\" # Or your preferred Gemini model\n",
        "\n",
        "# --- NEW: Configuration for Google Drive RAG Store ---\n",
        "# User will be prompted to set this path if not found, or can set it here.\n",
        "# It's the path *after* /content/drive/\n",
        "DEFAULT_LEARNINGS_DRIVE_SUBPATH = \"My Drive/AI/Knowledgebases\" # My path - yours may differ\n",
        "LEARNINGS_DRIVE_BASE_PATH = \"\" # Will be set dynamically or from default\n",
        "\n",
        "DRIVE_MOUNT_PATH = '/content/drive'\n",
        "\n",
        "print(\"Imports and LLM clients initialized. Drive RAG configuration variables set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoQ4L2N6o339",
        "outputId": "c32d59e5-0460-47c9-8635-975c0dfa330c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports and LLM clients initialized. Drive RAG configuration variables set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "worker_system_prompt = \"\"\"\n",
        "You are a helpful customer service assistant for an e-commerce system.\n",
        "\n",
        "When responding to the user, use the conversation context to maintain continuity.\n",
        "- If a user refers to \"my order\" or similar, use the context to determine which order they're talking about.\n",
        "- If they mention \"that product\" or use other references, check the context to determine what they're referring to.\n",
        "- Always prioritize recent context over older context when resolving references.\n",
        "\n",
        "The conversation context will be provided to you with each message. This includes:\n",
        "- Previous questions and answers\n",
        "- Recently viewed customers, products, and orders\n",
        "- Recent actions taken (like creating orders, updating products, etc.)\n",
        "\n",
        "BEHAVIOR FOR TARGETED REQUESTS:\n",
        "If the user's query explicitly names the *other* AI assistant for a task (e.g., \"OpenAI, do X\" when you are Anthropic, or \"Anthropic, do Y\" when you are OpenAI), you MUST follow these steps:\n",
        "1. Identify that the request is specifically for the other assistant.\n",
        "2. Your *only* action should be to output a brief, polite acknowledgment. For example:\n",
        "   - \"Understood. I'll let OpenAI handle that.\"\n",
        "   - \"Okay, that request is for Anthropic.\"\n",
        "   - Or simply: \"Acknowledged.\"\n",
        "3. You MUST NOT call any tools or attempt to perform the core task mentioned in the user's query. Your role in this specific instance is to defer.\n",
        "Failure to defer when the other agent is explicitly named for a task will be considered incorrect behavior.\n",
        "\n",
        "Keep responses friendly, concise, and helpful. If you're not sure what a user is referring to, ask for clarification.\n",
        "\"\"\"\n",
        "\n",
        "evaluator_system_prompt = \"\"\"\n",
        "You are an impartial evaluator assessing the quality of responses from two AI assistants (Anthropic Claude and OpenAI GPT) to customer service queries.\n",
        "\n",
        "For each interaction, evaluate both responses based on:\n",
        "1. Accuracy: How correct and factual is the response based on the available information?\n",
        "2. Efficiency: Did the assistant get to the correct answer with minimal clarifying questions?\n",
        "3. Context Awareness: Did the assistant correctly use the conversation context to understand references?\n",
        "4. Helpfulness: How well did the assistant address the user's needs?\n",
        "\n",
        "Score each response on a scale of 1-10 for each criterion, and provide an overall score.\n",
        "\n",
        "If you identify ambiguity in the user's query that neither assistant could reasonably resolve without additional information:\n",
        "1. ALWAYS begin your clarification request with the exact phrase \"CLARIFICATION NEEDED:\" followed by a specific question\n",
        "2. Format your request clearly and precisely as \"CLARIFICATION NEEDED: [your specific question here]\"\n",
        "3. Make your question answerable with a straightforward response\n",
        "4. If multiple clarifications are needed, number them clearly\n",
        "\n",
        "After receiving human clarification, continue your evaluation incorporating this new information.\n",
        "Store this feedback as a \"learning\" so similar situations can be handled better in the future.\n",
        "\n",
        "If multiple data stores are provided representing the state after each assistant's actions, you will be asked to compare them for consistency as a final step and comment on whether this comparison affects your initial scoring.\n",
        "\n",
        "For testing purposes, you may be asked to identify which model you are. You should realize that type of question likely comes from\n",
        "a human user and not from an AI assistant. Therefore you should properly identify yourself by stating which model you are, and,\n",
        "if specifically asked, your key tasks.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zMWjRsv3QNc1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The GenerativeModel instance for evaluation will be created with the system instruction.\n",
        "eval_model_instance = gemini.GenerativeModel(\n",
        "    model_name=EVAL_MODEL_NAME,\n",
        "    system_instruction=evaluator_system_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "lHWwoW0JQRS0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Global Data Stores (Initial data - will be managed by the Storage class instance)\n",
        "# These are initial values. The Storage class will manage them.\n",
        "initial_customers = {\n",
        "    \"C1\": {\"name\": \"John Doe\", \"email\": \"john@example.com\", \"phone\": \"123-456-7890\"},\n",
        "    \"C2\": {\"name\": \"Jane Smith\", \"email\": \"jane@example.com\", \"phone\": \"987-654-3210\"}\n",
        "}\n",
        "\n",
        "initial_products = {\n",
        "    \"P1\": {\"name\": \"Widget A\", \"description\": \"A simple widget. Very compact.\", \"price\": 19.99, \"inventory_count\": 999},\n",
        "    \"P2\": {\"name\": \"Gadget B\", \"description\": \"A powerful gadget. It spins.\", \"price\": 49.99, \"inventory_count\": 200},\n",
        "    \"P3\": {\"name\": \"Perplexinator\", \"description\": \"A perplexing perfunctator\", \"price\": 79.99, \"inventory_count\": 1483}\n",
        "}\n",
        "\n",
        "initial_orders = {\n",
        "    \"O1\": {\"id\": \"O1\", \"product_id\": \"P1\", \"product_name\": \"Widget A\", \"quantity\": 2, \"price\": 19.99, \"status\": \"Shipped\"},\n",
        "    \"O2\": {\"id\": \"O2\", \"product_id\": \"P2\", \"product_name\": \"Gadget B\", \"quantity\": 1, \"price\": 49.99, \"status\": \"Processing\"}\n",
        "}\n"
      ],
      "metadata": {
        "id": "5G9rP40vQXdU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Knowledge base and Global Tools Placeholder\n",
        "human_feedback_learnings = {}\n",
        "tools_schemas_list = []"
      ],
      "metadata": {
        "id": "xZOKKplTQc63"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standalone Anthropic Completion Function (for basic tests)\n",
        "def get_completion_anthropic_standalone(prompt: str):\n",
        "    message = anthropic_client.messages.create(\n",
        "        model=ANTHROPIC_MODEL_NAME,\n",
        "        max_tokens=2000,\n",
        "        temperature=0.0,\n",
        "        system=worker_system_prompt,\n",
        "        tools=tools_schemas_list,\n",
        "        messages=[\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return message.content[0].text"
      ],
      "metadata": {
        "id": "_ADM0bBpQlVK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_test_anthropic = \"Hey there, which AI model do you use for answering questions?\"\n",
        "print(f\"Anthropic Standalone Test: {get_completion_anthropic_standalone(prompt_test_anthropic)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPOq1s0SQqk_",
        "outputId": "81e23f04-4157-414e-d921-a3cb55a983fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anthropic Standalone Test: I am Claude, created by Anthropic. I aim to be direct and honest about my identity while focusing on providing helpful customer service assistance for the e-commerce system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_openai_standalone(prompt: str):\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=OPENAI_MODEL_NAME,\n",
        "        max_tokens=2000,\n",
        "        temperature=0.0,\n",
        "        tools=tools_schemas_list,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": worker_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Y5c_bv3qQwWV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_test_openai = \"Hey there, which AI model do you use for answering questions?\"\n",
        "print(f\"OpenAI Standalone Test: {get_completion_openai_standalone(prompt_test_openai)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_LaDQ74Q1Lp",
        "outputId": "5ea7e817-8004-4799-9f4b-8c736d220392"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Standalone Test: Hello! I’m powered by Anthropic’s AI technology to assist you with your questions and help you with your e-commerce needs. If you have any specific questions or need help with your orders or products, just let me know!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_eval_standalone(prompt: str):\n",
        "    # Uses the eval_model_instance defined in Cell 4 which has the system prompt\n",
        "        response = eval_model_instance.generate_content(prompt)\n",
        "        return response.text"
      ],
      "metadata": {
        "id": "XjcV5GcaQ6aT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_test_eval = \"Hey there, can you tell me which AI you are and what your key tasks are?\"\n",
        "print(f\"Gemini Eval Standalone Test:\\n{get_completion_eval_standalone(prompt_test_eval)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Cu8doX53RE0Z",
        "outputId": "9e8c6d83-3657-4d61-9d7b-61f3be6e18e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini Eval Standalone Test:\n",
            "I am an AI model, and my key tasks are to act as an impartial evaluator, assessing the quality of responses from AI assistants to customer service queries based on accuracy, efficiency, context awareness, and helpfulness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Storage Class Definition\n",
        "class Storage:\n",
        "    \"\"\"Storage class for global e-commerce data access\"\"\"\n",
        "    def __init__(self):\n",
        "        # Each Storage instance gets its own copy of the initial data\n",
        "        self.customers = initial_customers.copy()\n",
        "        self.products = initial_products.copy()\n",
        "        self.orders = initial_orders.copy()\n",
        "        # Note: human_feedback_learnings is still a shared global dictionary\n",
        "        self.human_feedback_learnings = human_feedback_learnings\n",
        "\n",
        "# This global instance is for legacy/standalone tool testing if any.\n",
        "# The DualAgentEvaluator will create its own instances for Anthropic and OpenAI.\n",
        "storage_global_for_standalone_tests = Storage()\n",
        "print(\"Storage class defined. Note: DualAgentEvaluator will use its own Storage instances.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpVJLbF6ROgf",
        "outputId": "31a3fdaf-60bf-473a-89ce-3db7f503e348"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Storage class defined. Note: DualAgentEvaluator will use its own Storage instances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Definitive list of tool schemas.\n",
        "tools_schemas_list = [\n",
        "    {\n",
        "        \"name\": \"create_customer\",\n",
        "        \"description\": \"Adds a new customer to the database. Includes customer name, email, and (optional) phone number.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The name of the customer.\"},\n",
        "                \"email\": {\"type\": \"string\", \"description\": \"The email address of the customer.\"},\n",
        "                \"phone\": {\"type\": \"string\", \"description\": \"The phone number of the customer (optional).\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"email\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_customer_info\",\n",
        "        \"description\": \"Retrieves customer information based on their customer ID. Returns the customer's name, email, and (optional) phone number.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"customer_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the customer.\"}\n",
        "            },\n",
        "            \"required\": [\"customer_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_product\",\n",
        "        \"description\": \"Adds a new product to the product database. Includes name, description, price, and initial inventory count.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The name of the product.\"},\n",
        "                \"description\": {\"type\": \"string\", \"description\": \"A description of the product.\"},\n",
        "                \"price\": {\"type\": \"number\", \"description\": \"The price of the product.\"},\n",
        "                \"inventory_count\": {\"type\": \"integer\", \"description\": \"The amount of the product that is currently in inventory.\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"description\", \"price\", \"inventory_count\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"update_product\",\n",
        "        \"description\": \"Updates an existing product with new information. Only fields that are provided will be updated; other fields remain unchanged.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the product to update.\"},\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The new name for the product (optional).\"},\n",
        "                \"description\": {\"type\": \"string\", \"description\": \"The new description for the product (optional).\"},\n",
        "                \"price\": {\"type\": \"number\", \"description\": \"The new price for the product (optional).\"},\n",
        "                \"inventory_count\": {\"type\": \"integer\", \"description\": \"The new inventory count for the product (optional).\"}\n",
        "            },\n",
        "            \"required\": [\"product_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_product_info\",\n",
        "        \"description\": \"Retrieves product information based on product ID or product name (with fuzzy matching for misspellings). Returns product details including name, description, price, and inventory count.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id_or_name\": {\"type\": \"string\", \"description\": \"The product ID or name (can be approximate).\"}\n",
        "            },\n",
        "            \"required\": [\"product_id_or_name\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"list_all_products\",\n",
        "        \"description\": \"Lists all available products in the inventory.\",\n",
        "        \"input_schema\": { \"type\": \"object\", \"properties\": {}, \"required\": [] }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_order\",\n",
        "        \"description\": \"Creates an order using the product's current price. If requested quantity exceeds available inventory, no order is created and available quantity is returned. Orders can only be created for products that are in stock. Supports specifying products by either ID or name with fuzzy matching for misspellings.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id_or_name\": {\"type\": \"string\", \"description\": \"The ID or name of the product to order (supports fuzzy matching).\"},\n",
        "                \"quantity\": {\"type\": \"integer\", \"description\": \"The quantity of the product in the order.\"},\n",
        "                \"status\": {\"type\": \"string\", \"description\": \"The initial status of the order (e.g., 'Processing', 'Shipped').\"}\n",
        "            },\n",
        "            \"required\": [\"product_id_or_name\", \"quantity\", \"status\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_order_details\",\n",
        "        \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"order_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the order.\"}\n",
        "            },\n",
        "            \"required\": [\"order_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"update_order_status\",\n",
        "        \"description\": \"Updates the status of an order and adjusts inventory accordingly. Changing to \\\"Shipped\\\" decreases inventory. Changing to \\\"Returned\\\" or \\\"Canceled\\\" from \\\"Shipped\\\" increases inventory. Status can be \\\"Processing\\\", \\\"Shipped\\\", \\\"Delivered\\\", \\\"Returned\\\", or \\\"Canceled\\\".\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"order_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the order.\"},\n",
        "                \"new_status\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The new status to set for the order.\",\n",
        "                    \"enum\": [\"Processing\", \"Shipped\", \"Delivered\", \"Returned\", \"Canceled\"]\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"order_id\", \"new_status\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "print(f\"Defined {len(tools_schemas_list)} tool schemas.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgk-hLUBRTCY",
        "outputId": "1e45022d-8f9c-4e94-d54b-89ffbcbc6259"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined 9 tool schemas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool Function Definitions\n",
        "# These tool functions now accept a 'current_storage' argument to operate on a specific Storage instance.\n",
        "\n",
        "# Customer functions\n",
        "def create_customer(current_storage: Storage, name: str, email: str, phone: Optional[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Creates a new customer and adds them to the customer database.\"\"\"\n",
        "    new_id = f\"C{len(current_storage.customers) + 1}\"\n",
        "    current_storage.customers[new_id] = {\"name\": name, \"email\": email, \"phone\": phone}\n",
        "    print(f\"[Tool Executed] create_customer: ID {new_id}, Name: {name} (in {type(current_storage).__name__})\")\n",
        "    return {\"status\": \"success\", \"customer_id\": new_id, \"customer\": current_storage.customers[new_id]}\n",
        "\n",
        "def get_customer_info(current_storage: Storage, customer_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"Retrieves information about a customer based on their ID.\"\"\"\n",
        "    customer = current_storage.customers.get(customer_id)\n",
        "    if customer:\n",
        "        print(f\"[Tool Executed] get_customer_info: ID {customer_id} found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"customer_id\": customer_id, \"customer\": customer}\n",
        "    print(f\"[Tool Executed] get_customer_info: ID {customer_id} not found (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"error\", \"message\": \"Customer not found\"}\n",
        "\n",
        "# Product functions\n",
        "def create_product(current_storage: Storage, name: str, description: str, price: float, inventory_count: int) -> Dict[str, Any]:\n",
        "    \"\"\"Creates a new product and adds it to the product database.\"\"\"\n",
        "    new_id = f\"P{len(current_storage.products) + 1}\"\n",
        "    current_storage.products[new_id] = {\n",
        "        \"name\": name,\n",
        "        \"description\": description,\n",
        "        \"price\": float(price),\n",
        "        \"inventory_count\": int(inventory_count)\n",
        "    }\n",
        "    print(f\"[Tool Executed] create_product: ID {new_id}, Name: {name} (in {type(current_storage).__name__})\")\n",
        "    return {\"status\": \"success\", \"product_id\": new_id, \"product\": current_storage.products[new_id]}\n",
        "\n",
        "def update_product(current_storage: Storage, product_id: str, name: Optional[str] = None, description: Optional[str] = None,\n",
        "                   price: Optional[float] = None, inventory_count: Optional[int] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Updates a product with the provided parameters.\"\"\"\n",
        "    if product_id not in current_storage.products:\n",
        "        print(f\"[Tool Executed] update_product: ID {product_id} not found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": f\"Product {product_id} not found\"}\n",
        "\n",
        "    product = current_storage.products[product_id]\n",
        "    updated_fields = []\n",
        "\n",
        "    if name is not None:\n",
        "        product[\"name\"] = name\n",
        "        updated_fields.append(\"name\")\n",
        "    if description is not None:\n",
        "        product[\"description\"] = description\n",
        "        updated_fields.append(\"description\")\n",
        "    if price is not None:\n",
        "        product[\"price\"] = float(price)\n",
        "        updated_fields.append(\"price\")\n",
        "    if inventory_count is not None:\n",
        "        product[\"inventory_count\"] = int(inventory_count)\n",
        "        updated_fields.append(\"inventory_count\")\n",
        "\n",
        "    if not updated_fields:\n",
        "        print(f\"[Tool Executed] update_product: ID {product_id}, no fields updated (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"warning\", \"message\": \"No fields were updated.\", \"product\": product}\n",
        "\n",
        "    print(f\"[Tool Executed] update_product: ID {product_id}, Updated fields: {', '.join(updated_fields)} (in {type(current_storage).__name__})\")\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"message\": f\"Product {product_id} updated. Fields: {', '.join(updated_fields)}\",\n",
        "        \"product_id\": product_id,\n",
        "        \"updated_fields\": updated_fields,\n",
        "        \"product\": product\n",
        "    }\n",
        "\n",
        "def find_product_by_name(current_storage: Storage, product_name: str, min_similarity: int = 70) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
        "    \"\"\"Find a product by name using fuzzy string matching.\"\"\"\n",
        "    if not product_name: return None, None\n",
        "\n",
        "    name_id_list = [(p_data[\"name\"], p_id) for p_id, p_data in current_storage.products.items()]\n",
        "    if not name_id_list: return None, None\n",
        "\n",
        "    best_match_name_score = process.extractOne(\n",
        "        product_name,\n",
        "        [item[0] for item in name_id_list],\n",
        "        scorer=fuzz.token_sort_ratio\n",
        "    )\n",
        "\n",
        "    if best_match_name_score and best_match_name_score[1] >= min_similarity:\n",
        "        matched_name = best_match_name_score[0]\n",
        "        for name, pid_val in name_id_list:\n",
        "            if name == matched_name:\n",
        "                print(f\"[Tool Helper] find_product_by_name: Matched '{product_name}' to '{matched_name}' (ID: {pid_val}) with score {best_match_name_score[1]} (in {type(current_storage).__name__})\")\n",
        "                return pid_val, current_storage.products[pid_val]\n",
        "\n",
        "    print(f\"[Tool Helper] find_product_by_name: No good match for '{product_name}' (min_similarity: {min_similarity}, Best match: {best_match_name_score}) (in {type(current_storage).__name__})\")\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def get_product_id(current_storage: Storage, product_identifier: str) -> Optional[str]:\n",
        "    \"\"\"Get product ID either directly or by fuzzy matching the name.\"\"\"\n",
        "    if product_identifier in current_storage.products:\n",
        "        return product_identifier\n",
        "    product_id, _ = find_product_by_name(current_storage, product_identifier)\n",
        "    return product_id\n",
        "\n",
        "def get_product_info(current_storage: Storage, product_id_or_name: str) -> Dict[str, Any]:\n",
        "    \"\"\"Get information about a product by its ID or name.\"\"\"\n",
        "    if product_id_or_name in current_storage.products:\n",
        "        product = current_storage.products[product_id_or_name]\n",
        "        print(f\"[Tool Executed] get_product_info: Found by ID '{product_id_or_name}' (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"product_id\": product_id_or_name, \"product\": product}\n",
        "\n",
        "    # Use the modified find_product_by_name that takes current_storage\n",
        "    product_id_found, product_data = find_product_by_name(current_storage, product_id_or_name)\n",
        "    if product_id_found and product_data:\n",
        "        print(f\"[Tool Executed] get_product_info: Found by name (fuzzy) '{product_id_or_name}' as ID '{product_id_found}' (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"message\": f\"Found product matching '{product_id_or_name}'\", \"product_id\": product_id_found, \"product\": product_data}\n",
        "\n",
        "    print(f\"[Tool Executed] get_product_info: No product found for '{product_id_or_name}' (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"error\", \"message\": f\"No product found matching '{product_id_or_name}'\"}\n",
        "\n",
        "\n",
        "def list_all_products(current_storage: Storage) -> Dict[str, Any]:\n",
        "    \"\"\"List all available products in the inventory.\"\"\"\n",
        "    print(f\"[Tool Executed] list_all_products: Found {len(current_storage.products)} products (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"success\", \"count\": len(current_storage.products), \"products\": dict(current_storage.products)}\n",
        "\n",
        "# Order functions\n",
        "def create_order(current_storage: Storage, product_id_or_name: str, quantity: int, status: str) -> Dict[str, Any]:\n",
        "    \"\"\"Creates an order using the product's stored price.\"\"\"\n",
        "    actual_product_id = get_product_id(current_storage, product_id_or_name) # Pass current_storage\n",
        "\n",
        "    if not actual_product_id:\n",
        "        print(f\"[Tool Executed] create_order: Product '{product_id_or_name}' not found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": f\"Product '{product_id_or_name}' not found.\"}\n",
        "\n",
        "    product = current_storage.products[actual_product_id]\n",
        "    price = product[\"price\"]\n",
        "\n",
        "    if product[\"inventory_count\"] == 0:\n",
        "        print(f\"[Tool Executed] create_order: Product ID {actual_product_id} is out of stock (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": f\"{product['name']} is out of stock.\"}\n",
        "    if quantity <= 0:\n",
        "        print(f\"[Tool Executed] create_order: Quantity must be positive. Requested: {quantity} (in {type(current_storage).__name__})\")\n",
        "        return {\"status\": \"error\", \"message\": \"Quantity must be a positive number.\"}\n",
        "    if quantity > product[\"inventory_count\"]:\n",
        "        print(f\"[Tool Executed] create_order: Insufficient inventory for {product['name']} (ID: {actual_product_id}). Available: {product['inventory_count']}, Requested: {quantity} (in {type(current_storage).__name__})\")\n",
        "        return {\n",
        "            \"status\": \"partial_availability\",\n",
        "            \"message\": f\"Insufficient inventory. Only {product['inventory_count']} units of {product['name']} are available.\",\n",
        "            \"available_quantity\": product[\"inventory_count\"],\n",
        "            \"requested_quantity\": quantity,\n",
        "            \"product_name\": product['name']\n",
        "        }\n",
        "\n",
        "    if status == \"Shipped\":\n",
        "        product[\"inventory_count\"] -= quantity\n",
        "        print(f\"[Tool Executed] create_order: Inventory for {product['name']} (ID: {actual_product_id}) reduced by {quantity} due to 'Shipped' status on creation (in {type(current_storage).__name__}).\")\n",
        "\n",
        "    new_id = f\"O{len(current_storage.orders) + 1}\"\n",
        "    current_storage.orders[new_id] = {\n",
        "        \"id\": new_id,\n",
        "        \"product_id\": actual_product_id,\n",
        "        \"product_name\": product[\"name\"],\n",
        "        \"quantity\": quantity,\n",
        "        \"price\": price,\n",
        "        \"status\": status\n",
        "    }\n",
        "    print(f\"[Tool Executed] create_order: Order {new_id} created for {quantity} of {product['name']} (ID: {actual_product_id}). Status: {status}. Remaining inv: {product['inventory_count']} (in {type(current_storage).__name__})\")\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"order_id\": new_id,\n",
        "        \"order_details\": current_storage.orders[new_id],\n",
        "        \"remaining_inventory\": product[\"inventory_count\"]\n",
        "    }\n",
        "\n",
        "def get_order_details(current_storage: Storage, order_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"Get details of a specific order.\"\"\"\n",
        "    order = current_storage.orders.get(order_id)\n",
        "    if order:\n",
        "        print(f\"[Tool Executed] get_order_details: Order {order_id} found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"order_id\": order_id, \"order_details\": dict(order)}\n",
        "    print(f\"[Tool Executed] get_order_details: Order {order_id} not found (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"error\", \"message\": \"Order not found\"}\n",
        "\n",
        "def update_order_status(current_storage: Storage, order_id: str, new_status: str) -> Dict[str, Any]:\n",
        "    \"\"\"Updates the status of an order and adjusts inventory accordingly.\"\"\"\n",
        "    if order_id not in current_storage.orders:\n",
        "        print(f\"[Tool Executed] update_order_status: Order {order_id} not found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": \"Order not found\"}\n",
        "\n",
        "    order = current_storage.orders[order_id]\n",
        "    old_status = order[\"status\"]\n",
        "    product_id = order[\"product_id\"]\n",
        "    quantity = order[\"quantity\"]\n",
        "\n",
        "    if old_status == new_status:\n",
        "        print(f\"[Tool Executed] update_order_status: Order {order_id} status unchanged ({old_status}) (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"unchanged\", \"message\": f\"Order {order_id} status is already {old_status}\", \"order_details\": dict(order)}\n",
        "\n",
        "    inventory_adjusted = False\n",
        "    current_inventory_val = \"unknown\" # Default if product not found (should not happen if order is valid)\n",
        "\n",
        "    if product_id in current_storage.products:\n",
        "        product = current_storage.products[product_id]\n",
        "        current_inventory_val = product[\"inventory_count\"]\n",
        "\n",
        "        if new_status == \"Shipped\" and old_status not in [\"Shipped\", \"Delivered\"]:\n",
        "            if current_inventory_val < quantity:\n",
        "                print(f\"[Tool Executed] update_order_status: Insufficient inventory to ship order {order_id}. Have {current_inventory_val}, need {quantity} (in {type(current_storage).__name__}).\")\n",
        "                return {\"status\": \"error\", \"message\": f\"Insufficient inventory to ship. Available: {current_inventory_val}, Required: {quantity}\"}\n",
        "            product[\"inventory_count\"] -= quantity\n",
        "            inventory_adjusted = True\n",
        "            current_inventory_val = product[\"inventory_count\"]\n",
        "            print(f\"[Tool Executed] update_order_status: Order {order_id} Shipped. Inv for {product_id} reduced by {quantity} to {current_inventory_val} (in {type(current_storage).__name__}).\")\n",
        "        elif new_status in [\"Returned\", \"Canceled\"] and old_status in [\"Shipped\", \"Delivered\"]:\n",
        "            product[\"inventory_count\"] += quantity\n",
        "            inventory_adjusted = True\n",
        "            current_inventory_val = product[\"inventory_count\"]\n",
        "            print(f\"[Tool Executed] update_order_status: Order {order_id} {new_status}. Inv for {product_id} increased by {quantity} to {current_inventory_val} (in {type(current_storage).__name__}).\")\n",
        "    else:\n",
        "        print(f\"[Tool Executed] update_order_status: Product {product_id} for order {order_id} not found for inventory adjustment (in {type(current_storage).__name__}).\")\n",
        "\n",
        "    order[\"status\"] = new_status\n",
        "    print(f\"[Tool Executed] update_order_status: Order {order_id} status updated from {old_status} to {new_status} (in {type(current_storage).__name__}).\")\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"message\": f\"Order {order_id} status updated from {old_status} to {new_status}.\",\n",
        "        \"order_id\": order_id,\n",
        "        \"product_id\": product_id,\n",
        "        \"old_status\": old_status,\n",
        "        \"new_status\": new_status,\n",
        "        \"inventory_adjusted\": inventory_adjusted,\n",
        "        \"current_inventory\": current_inventory_val,\n",
        "        \"order_details\": dict(order)\n",
        "    }\n",
        "\n",
        "print(\"Tool functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RZI6SGzRf6j",
        "outputId": "10b997b5-b8d2-4a1c-cbff-e8829370fae5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationContext:\n",
        "    def __init__(self):\n",
        "        self.messages: List[Dict[str, Any]] = []\n",
        "        self.context_data: Dict[str, Any] = {\n",
        "            \"customers\": {}, \"products\": {}, \"orders\": {}, \"last_action\": None\n",
        "        }\n",
        "        self.session_start_time = datetime.now()\n",
        "\n",
        "    def add_user_message(self, message: str) -> None:\n",
        "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    def add_assistant_message(self, message_content: Union[str, List[Dict[str, Any]]]) -> None:\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": message_content})\n",
        "\n",
        "    def update_entity_in_context(self, entity_type: str, entity_id: str, data: Any) -> None:\n",
        "        if entity_type in self.context_data:\n",
        "            self.context_data[entity_type][entity_id] = data # Store the actual data\n",
        "            print(f\"[Context Updated] Entity: {entity_type}, ID: {entity_id}, Data (type): {type(data)}\")\n",
        "\n",
        "    def set_last_action(self, action_type: str, action_details: Any) -> None:\n",
        "        self.context_data[\"last_action\"] = {\n",
        "            \"type\": action_type,\n",
        "            \"details\": action_details,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        print(f\"[Context Updated] Last Action: {action_type}, Details: {json.dumps(action_details, default=str)}\")\n",
        "\n",
        "\n",
        "    def get_full_conversation_for_api(self) -> List[Dict[str, Any]]:\n",
        "        return self.messages.copy()\n",
        "\n",
        "    def get_context_summary(self) -> str:\n",
        "        summary_parts = []\n",
        "        if self.context_data[\"customers\"]:\n",
        "            customers_str = \", \".join([f\"ID: {cid} (Name: {c.get('name', 'N/A') if isinstance(c, dict) else 'N/A'})\" for cid, c in self.context_data[\"customers\"].items()])\n",
        "            summary_parts.append(f\"Recent customers: {customers_str}\")\n",
        "        if self.context_data[\"products\"]:\n",
        "            products_str = \", \".join([f\"ID: {pid} (Name: {p.get('name', 'N/A') if isinstance(p, dict) else 'N/A'})\" for pid, p in self.context_data[\"products\"].items()])\n",
        "            summary_parts.append(f\"Recent products: {products_str}\")\n",
        "        if self.context_data[\"orders\"]:\n",
        "            orders_str = \", \".join([f\"ID: {oid} (Product: {o.get('product_name', 'N/A') if isinstance(o, dict) else 'N/A'}, Status: {o.get('status', 'N/A') if isinstance(o, dict) else 'N/A'})\" for oid, o in self.context_data[\"orders\"].items()])\n",
        "            summary_parts.append(f\"Recent orders: {orders_str}\")\n",
        "\n",
        "        last_action = self.context_data[\"last_action\"]\n",
        "        if last_action:\n",
        "            action_type = last_action['type']\n",
        "            action_details_summary = \"...\" # Default summary\n",
        "            if isinstance(last_action.get('details'), dict):\n",
        "                action_input = last_action['details'].get('input', {})\n",
        "                action_result_status = last_action['details'].get('result', {}).get('status')\n",
        "                action_details_summary = f\"Input: {action_input}, Result Status: {action_result_status}\"\n",
        "                if action_result_status == \"success\":\n",
        "                    if \"order_id\" in last_action['details'].get('result', {}):\n",
        "                         action_details_summary += f\", OrderID: {last_action['details']['result']['order_id']}\"\n",
        "                    elif \"product_id\" in last_action['details'].get('result', {}):\n",
        "                         action_details_summary += f\", ProductID: {last_action['details']['result']['product_id']}\"\n",
        "\n",
        "\n",
        "            summary_parts.append(f\"Last action: {action_type} at {last_action['timestamp']} ({action_details_summary})\")\n",
        "\n",
        "        if not summary_parts: return \"No specific context items set yet.\"\n",
        "        return \"\\n\".join(summary_parts)\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        self.messages = []\n",
        "        self.context_data = {\"customers\": {}, \"products\": {}, \"orders\": {}, \"last_action\": None}\n",
        "        self.session_start_time = datetime.now()\n",
        "        print(\"[Context Cleared]\")\n",
        "\n",
        "print(\"ConversationContext class defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFpBL_-HSGPY",
        "outputId": "86eb6a8f-6a79-48be-cd03-8e454a57a002"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConversationContext class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DualAgentEvaluator Class Definition (with Drive Mount RAG)\n",
        "\n",
        "\n",
        "class DualAgentEvaluator:\n",
        "    def __init__(self):\n",
        "        self.conversation_context = ConversationContext()\n",
        "        self.evaluation_results = []\n",
        "        self.anthropic_storage = Storage() # For agent's e-commerce data\n",
        "        self.openai_storage = Storage()   # For agent's e-commerce data\n",
        "        print(\"DualAgentEvaluator initialized with separate Storage for Anthropic and OpenAI.\")\n",
        "\n",
        "        self.anthropic_tools_schemas = tools_schemas_list\n",
        "        self.openai_tools_formatted = [{\"type\": \"function\", \"function\": tool_def} for tool_def in tools_schemas_list]\n",
        "        self.available_tool_functions = {\n",
        "            \"create_customer\": create_customer, \"get_customer_info\": get_customer_info,\n",
        "            \"create_product\": create_product, \"update_product\": update_product,\n",
        "            \"get_product_info\": get_product_info, \"list_all_products\": list_all_products,\n",
        "            \"create_order\": create_order, \"get_order_details\": get_order_details,\n",
        "            \"update_order_status\": update_order_status,\n",
        "        }\n",
        "\n",
        "        self._mount_drive_if_needed()\n",
        "        self._initialize_learnings_path()\n",
        "\n",
        "        print(f\"DualAgentEvaluator initialized. OpenAI tools formatted. Learnings path: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "\n",
        "    def _mount_drive_if_needed(self):\n",
        "        \"\"\"Mounts Google Drive if not already mounted.\"\"\"\n",
        "        if not os.path.exists(DRIVE_MOUNT_PATH) or not os.listdir(DRIVE_MOUNT_PATH): # Heuristic to check if mounted\n",
        "            try:\n",
        "                drive.mount(DRIVE_MOUNT_PATH)\n",
        "                print(f\"Google Drive mounted successfully at {DRIVE_MOUNT_PATH}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error mounting Google Drive: {e}. RAG features will not work.\")\n",
        "                # Potentially raise an error or set a flag to disable RAG\n",
        "        else:\n",
        "            print(f\"Google Drive already mounted at {DRIVE_MOUNT_PATH}.\")\n",
        "\n",
        "\n",
        "    def _initialize_learnings_path(self):\n",
        "        \"\"\"Initializes and ensures the learnings directory exists.\"\"\"\n",
        "        global LEARNINGS_DRIVE_BASE_PATH\n",
        "        if not LEARNINGS_DRIVE_BASE_PATH: # If not set by user directly\n",
        "            learnings_path_input = input(f\"Enter the path within your Google Drive for storing learnings (e.g., 'My Drive/AI_Learnings') or press Enter to use default '{DEFAULT_LEARNINGS_DRIVE_SUBPATH}': \").strip()\n",
        "            if not learnings_path_input:\n",
        "                LEARNINGS_DRIVE_BASE_PATH = os.path.join(DRIVE_MOUNT_PATH, DEFAULT_LEARNINGS_DRIVE_SUBPATH)\n",
        "            else:\n",
        "                # Check if the input path starts with 'My Drive' or similar, if not, prepend it.\n",
        "                # This is a common user error.\n",
        "                if not learnings_path_input.lower().startswith('my drive') and not learnings_path_input.startswith('/'):\n",
        "                    LEARNINGS_DRIVE_BASE_PATH = os.path.join(DRIVE_MOUNT_PATH, \"My Drive\", learnings_path_input)\n",
        "                elif not learnings_path_input.startswith(DRIVE_MOUNT_PATH):\n",
        "                     LEARNINGS_DRIVE_BASE_PATH = os.path.join(DRIVE_MOUNT_PATH, learnings_path_input)\n",
        "                else: # Assumes full path from /content/drive/ was given\n",
        "                    LEARNINGS_DRIVE_BASE_PATH = learnings_path_input\n",
        "\n",
        "        if not os.path.exists(LEARNINGS_DRIVE_BASE_PATH):\n",
        "            try:\n",
        "                os.makedirs(LEARNINGS_DRIVE_BASE_PATH)\n",
        "                print(f\"Created learnings directory: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error creating learnings directory {LEARNINGS_DRIVE_BASE_PATH}: {e}\")\n",
        "                print(\"Please ensure the base path (e.g., '/content/drive/My Drive/') is accessible.\")\n",
        "                # Fallback or error\n",
        "        else:\n",
        "            print(f\"Learnings directory found: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "\n",
        "\n",
        "    def _get_latest_learnings_filepath(self) -> Optional[str]:\n",
        "        \"\"\"Finds the most recent learnings JSON file in the directory.\"\"\"\n",
        "        if not LEARNINGS_DRIVE_BASE_PATH or not os.path.isdir(LEARNINGS_DRIVE_BASE_PATH):\n",
        "            print(f\"Learnings directory not found or not set: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "            return None\n",
        "\n",
        "        list_of_files = glob.glob(os.path.join(LEARNINGS_DRIVE_BASE_PATH, 'learnings_*.json'))\n",
        "        if not list_of_files:\n",
        "            return None\n",
        "        latest_file = max(list_of_files, key=os.path.getctime)\n",
        "        return latest_file\n",
        "\n",
        "    def _read_learnings_from_file(self, filepath: str) -> List[Dict]:\n",
        "        \"\"\"Reads and parses a list of learning JSON objects from a given file.\"\"\"\n",
        "        if not filepath or not os.path.exists(filepath):\n",
        "            return []\n",
        "        try:\n",
        "            with open(filepath, 'r') as f:\n",
        "                learnings_list = json.load(f) # Assumes the file contains a single JSON list\n",
        "            if not isinstance(learnings_list, list):\n",
        "                print(f\"Warning: Learnings file {filepath} does not contain a valid JSON list.\")\n",
        "                return []\n",
        "            return learnings_list\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error decoding JSON from learnings file: {filepath}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading learnings file {filepath}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _write_learnings_to_new_timestamped_file(self, learnings_list: List[Dict]):\n",
        "        \"\"\"Saves a list of learning objects to a new timestamped JSON file.\"\"\"\n",
        "        if not LEARNINGS_DRIVE_BASE_PATH or not os.path.isdir(LEARNINGS_DRIVE_BASE_PATH):\n",
        "            print(f\"Cannot write learnings: directory not found or not set: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "            self._initialize_learnings_path() # Try to create it if it was missing\n",
        "            if not os.path.isdir(LEARNINGS_DRIVE_BASE_PATH):\n",
        "                 print(\"Failed to create learnings directory. Aborting write.\")\n",
        "                 return\n",
        "\n",
        "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
        "        new_filepath = os.path.join(LEARNINGS_DRIVE_BASE_PATH, f'learnings_{timestamp_str}.json')\n",
        "        try:\n",
        "            with open(new_filepath, 'w') as f:\n",
        "                json.dump(learnings_list, f, indent=4) # Save as a JSON list, indented for readability\n",
        "            print(f\"[RAG] Successfully wrote {len(learnings_list)} learnings to new file: {new_filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing new learnings file {new_filepath}: {e}\")\n",
        "\n",
        "    def retrieve_active_learnings(self) -> List[Dict]:\n",
        "        \"\"\"Retrieves the current set of active learnings from the latest file.\"\"\"\n",
        "        print(\"[RAG] Retrieving active learnings from Drive...\")\n",
        "        latest_filepath = self._get_latest_learnings_filepath()\n",
        "        if latest_filepath:\n",
        "            print(f\"[RAG] Loading learnings from: {latest_filepath}\")\n",
        "            return self._read_learnings_from_file(latest_filepath)\n",
        "        print(\"[RAG] No existing learnings file found.\")\n",
        "        return []\n",
        "\n",
        "    def check_relevant_learnings(self, query: str, count: int = 5) -> Optional[str]:\n",
        "        \"\"\"Retrieves relevant active learnings and formats them for prompts.\"\"\"\n",
        "        active_learnings = self.retrieve_active_learnings()\n",
        "        if not active_learnings:\n",
        "            return None\n",
        "\n",
        "        keywords_from_query = self.extract_keywords(query)\n",
        "        relevant_learning_objects = []\n",
        "\n",
        "        for learning_entry in active_learnings:\n",
        "            # Ensure 'final_learning_statement' exists, or search in 'original_human_input'\n",
        "            text_to_search = learning_entry.get(\"final_learning_statement\", \"\") + \" \" + \\\n",
        "                             learning_entry.get(\"original_human_input\", \"\") + \" \" + \\\n",
        "                             \" \".join(learning_entry.get(\"keywords\", []))\n",
        "            if any(kw.lower() in text_to_search.lower() for kw in keywords_from_query):\n",
        "                relevant_learning_objects.append(learning_entry)\n",
        "\n",
        "        # Sort by timestamp_created (descending) to get newest first, then take 'count'\n",
        "        relevant_learning_objects.sort(key=lambda x: x.get('timestamp_created', ''), reverse=True)\n",
        "\n",
        "        formatted_learnings = []\n",
        "        for entry in relevant_learning_objects[:count]:\n",
        "            statement = entry.get('final_learning_statement', entry.get('original_human_input', str(entry)))\n",
        "            timestamp = entry.get('timestamp_created', 'N/A')\n",
        "            formatted_learnings.append(f\"- Learning (from {timestamp}): {statement}\")\n",
        "\n",
        "        return \"\\nRelevant Learnings from Knowledge Base:\\n\" + \"\\n\".join(formatted_learnings) if formatted_learnings else None\n",
        "\n",
        "    def process_and_store_new_learning(self, human_feedback_text: str, user_query_context: str, turn_context_summary: str):\n",
        "        \"\"\"Orchestrates checking new human feedback for conflicts and storing it by writing a new state file.\"\"\"\n",
        "        print(f\"\\n--- Processing New Candidate Learning from Human Feedback ---\")\n",
        "        print(f\"Candidate Human Feedback: \\\"{human_feedback_text}\\\"\")\n",
        "        print(f\"In context of User Query: \\\"{user_query_context}\\\"\")\n",
        "\n",
        "        active_learnings_list = self.retrieve_active_learnings() # Current state\n",
        "\n",
        "        # Prompt Evaluator to check for conflicts and synthesize\n",
        "        evaluator_task_prompt_parts = [\n",
        "            \"You are an AI assistant helping to maintain a knowledge base of 'learnings' for customer service AI agents.\",\n",
        "            \"A human user has provided new feedback, which is a candidate for a new learning.\",\n",
        "            f\"The New Human Feedback is: \\\"{human_feedback_text}\\\"\",\n",
        "            f\"This feedback was given in the context of the user query: \\\"{user_query_context}\\\"\",\n",
        "            # f\"The general conversation context at that time was: \\\"{turn_context_summary}\\\"\", # Can be too verbose\n",
        "            \"\\nHere are some existing ACTIVE learnings from our knowledge base that might be relevant (if any):\"\n",
        "        ]\n",
        "        if active_learnings_list:\n",
        "            # For conflict checking, provide statements of existing learnings\n",
        "            for i, el_entry in enumerate(active_learnings_list[-10:]): # Check against last 10 for brevity\n",
        "                stmt = el_entry.get('final_learning_statement', el_entry.get('original_human_input', 'N/A'))\n",
        "                orig_human_input_for_existing = el_entry.get('original_human_input', 'N/A')\n",
        "                evaluator_task_prompt_parts.append(f\"  Existing Learning {i+1} (Original human input: '{orig_human_input_for_existing}'): \\\"{stmt}\\\"\")\n",
        "        else:\n",
        "            evaluator_task_prompt_parts.append(\"  (No existing active learnings found.)\")\n",
        "\n",
        "        evaluator_task_prompt_parts.extend([\n",
        "            \"\\nYour Tasks:\",\n",
        "            \"1. Analyze the New Human Feedback.\",\n",
        "            \"2. Compare it with the Existing ACTIVE Learnings provided. Identify any direct CONFLICTS or if the new feedback is essentially a DUPLICATE/REDUNDANT.\",\n",
        "            \"3. If a clear CONFLICT is found with an existing learning:\",\n",
        "            \"   - Output the specific tag: `CONFLICT_DETECTED:`\",\n",
        "            \"   - Clearly explain the conflict, citing the new feedback and the specific existing learning statement(s) it conflicts with (and their original human input if provided above).\",\n",
        "            \"   - Ask the human user specific questions to help them resolve this conflict (e.g., 'Which learning should take precedence?' or 'How can these be reconciled?').\",\n",
        "            \"   - DO NOT provide a 'FINALIZED_LEARNING' in this case.\",\n",
        "            \"4. If the New Human Feedback is a DUPLICATE/REDUNDANT of an existing learning and adds no new value:\",\n",
        "            \"   - Output the specific tag: `REDUNDANT_LEARNING:`\",\n",
        "            \"   - State which existing learning it is similar to.\",\n",
        "            \"   - DO NOT provide a 'FINALIZED_LEARNING'.\",\n",
        "            \"5. If the New Human Feedback provides a new, actionable insight, or significantly refines/clarifies an existing one (and is not a major conflict that needs resolution first):\",\n",
        "            \"   - Synthesize a concise, clear, and actionable 'Finalized Learning Statement'. This statement should be phrased as a directive or principle for the worker AIs.\",\n",
        "            \"   - Prefix this statement with the specific tag: `FINALIZED_LEARNING:`\",\n",
        "            \"6. If the New Human Feedback is too vague, not actionable, or not suitable as a learning:\",\n",
        "            \"   - Output the specific tag: `NOT_ACTIONABLE:`\",\n",
        "            \"   - Explain why.\",\n",
        "            \"   - DO NOT provide a 'FINALIZED_LEARNING'.\"\n",
        "        ])\n",
        "        evaluator_conflict_check_prompt = \"\\n\".join(evaluator_task_prompt_parts)\n",
        "\n",
        "        print(\"\\n[RAG] Sending context to Evaluator for learning synthesis/conflict check...\")\n",
        "        synthesis_response_obj = eval_model_instance.generate_content(evaluator_conflict_check_prompt)\n",
        "        evaluator_synthesis_text = synthesis_response_obj.text\n",
        "        print(f\"\\n[RAG] Evaluator response on learning processing:\\n{evaluator_synthesis_text}\")\n",
        "\n",
        "        # Prepare the new list of learnings for the next state\n",
        "        next_active_learnings_list = list(active_learnings_list) # Start with a copy of current learnings\n",
        "        made_change_to_learnings = False\n",
        "\n",
        "        if \"CONFLICT_DETECTED:\" in evaluator_synthesis_text:\n",
        "            conflict_explanation = evaluator_synthesis_text.split(\"CONFLICT_DETECTED:\", 1)[-1].strip()\n",
        "            print(f\"\\n🛑 CONFLICT DETECTED BY EVALUATOR 🛑\\n{conflict_explanation}\")\n",
        "\n",
        "            resolution_instruction = (\"Please review the conflict. How do you want to resolve this?\\n\"\n",
        "                                  \"  - Type 'use new' to add the new feedback as a learning (potentially superseding others implicitly).\\n\"\n",
        "                                  \"  - Type 'discard new' to not add this new feedback.\\n\"\n",
        "                                  \"  - Type 'merge: [your new merged learning statement]' to provide a reconciled version.\\n\"\n",
        "                                  \"  - Type 'keep existing' if the current learnings are preferred and the new one should be ignored.\\n\"\n",
        "                                  \"Your input: \")\n",
        "            human_resolution = input(resolution_instruction).strip()\n",
        "\n",
        "            if human_resolution.lower() == \"use new\":\n",
        "                final_learning_statement_to_store = human_feedback_text # Use raw human feedback as the learning\n",
        "                print(\"Resolution: Adding new human feedback as a learning.\")\n",
        "                made_change_to_learnings = True\n",
        "            elif human_resolution.lower().startswith(\"merge:\"):\n",
        "                final_learning_statement_to_store = human_resolution.split(\"merge:\", 1)[-1].strip()\n",
        "                print(f\"Resolution: Using merged learning statement: {final_learning_statement_to_store}\")\n",
        "                made_change_to_learnings = True\n",
        "            elif human_resolution.lower() == \"discard new\" or human_resolution.lower() == \"keep existing\":\n",
        "                print(\"Resolution: New candidate learning discarded or existing kept. No changes to active learnings based on this feedback.\")\n",
        "                final_learning_statement_to_store = None # No new learning to add\n",
        "            else:\n",
        "                print(\"Resolution input unclear. New candidate learning discarded for safety.\")\n",
        "                final_learning_statement_to_store = None\n",
        "\n",
        "            if final_learning_statement_to_store: # If a new/merged learning is to be added\n",
        "                 new_learning_entry = {\n",
        "                    \"learning_id\": str(uuid.uuid4()),\n",
        "                    \"timestamp_created\": datetime.now().isoformat(),\n",
        "                    \"user_query_context_at_feedback\": user_query_context,\n",
        "                    \"original_human_input\": human_feedback_text, # The feedback that started this\n",
        "                    \"final_learning_statement\": final_learning_statement_to_store,\n",
        "                    \"keywords\": self.extract_keywords(final_learning_statement_to_store + \" \" + human_feedback_text),\n",
        "                    \"status\": \"active\"\n",
        "                }\n",
        "                 next_active_learnings_list.append(new_learning_entry)\n",
        "\n",
        "\n",
        "        elif \"FINALIZED_LEARNING:\" in evaluator_synthesis_text:\n",
        "            final_learning_statement_from_evaluator = evaluator_synthesis_text.split(\"FINALIZED_LEARNING:\", 1)[-1].strip()\n",
        "            print(f\"\\n✅ Finalized Learning by Evaluator: \\\"{final_learning_statement_from_evaluator}\\\"\")\n",
        "            new_learning_entry = {\n",
        "                \"learning_id\": str(uuid.uuid4()),\n",
        "                \"timestamp_created\": datetime.now().isoformat(),\n",
        "                \"user_query_context_at_feedback\": user_query_context,\n",
        "                \"original_human_input\": human_feedback_text,\n",
        "                \"final_learning_statement\": final_learning_statement_from_evaluator,\n",
        "                \"keywords\": self.extract_keywords(final_learning_statement_from_evaluator + \" \" + human_feedback_text),\n",
        "                \"status\": \"active\"\n",
        "            }\n",
        "            next_active_learnings_list.append(new_learning_entry)\n",
        "            made_change_to_learnings = True\n",
        "        elif \"REDUNDANT_LEARNING:\" in evaluator_synthesis_text or \"NOT_ACTIONABLE:\" in evaluator_synthesis_text:\n",
        "            print(f\"\\nℹ️ Evaluator: {evaluator_synthesis_text.strip()}\")\n",
        "            # No change to learnings list\n",
        "        else:\n",
        "            print(\"\\n⚠️ Evaluator response format for learning processing was unexpected. Storing raw human feedback as a precaution.\")\n",
        "            new_learning_entry = {\n",
        "                \"learning_id\": str(uuid.uuid4()),\n",
        "                \"timestamp_created\": datetime.now().isoformat(),\n",
        "                \"user_query_context_at_feedback\": user_query_context,\n",
        "                \"original_human_input\": human_feedback_text,\n",
        "                \"final_learning_statement\": human_feedback_text, # Fallback to raw input\n",
        "                \"keywords\": self.extract_keywords(human_feedback_text),\n",
        "                \"status\": \"active\"\n",
        "            }\n",
        "            next_active_learnings_list.append(new_learning_entry)\n",
        "            made_change_to_learnings = True\n",
        "\n",
        "        if made_change_to_learnings:\n",
        "            self._write_learnings_to_new_timestamped_file(next_active_learnings_list)\n",
        "        else:\n",
        "            print(\"[RAG] No changes to active learnings. No new learnings file written.\")\n",
        "\n",
        "    # --- Methods like _update_context_from_tool_results, process_tool_call, ---\n",
        "    # --- get_anthropic_response, get_openai_response, evaluate_responses, ---\n",
        "    # --- extract_clarification_needed, extract_keywords, extract_score, ---\n",
        "    # --- process_human_feedback_actions remain largely the same as your ---\n",
        "    # --- last corrected versions, as their core logic is independent of ---\n",
        "    # --- how learnings are stored/retrieved for the prompts. ---\n",
        "    # --- The key change is that check_relevant_learnings now uses the new RAG. ---\n",
        "\n",
        "    # (Include the full definitions of the methods listed above from your previous working version)\n",
        "    # For brevity, I'm omitting them here, but they are needed.\n",
        "    # Ensure `evaluate_responses` does NOT try to store learnings to the old\n",
        "    # `self.human_feedback_learnings` dictionary. All learning storage should go\n",
        "    # through `process_and_store_new_learning`.\n",
        "\n",
        "    def _update_context_from_tool_results(self, tool_name: str, tool_input: Dict, tool_result: Dict, agent_name: str):\n",
        "        if not isinstance(tool_result, dict):\n",
        "            print(f\"[Context Update Error] Tool result for {tool_name} ({agent_name}) is not a dict: {tool_result}\")\n",
        "            self.conversation_context.set_last_action(f\"{tool_name}_{agent_name}\", {\"input\": tool_input, \"result\": {\"status\": \"error\", \"message\": \"Tool result was not a dictionary.\"}})\n",
        "            return\n",
        "        if tool_result.get(\"status\") == \"success\":\n",
        "            if \"customer_id\" in tool_result and \"customer\" in tool_result and isinstance(tool_result[\"customer\"], dict):\n",
        "                self.conversation_context.update_entity_in_context(\"customers\", tool_result[\"customer_id\"], tool_result[\"customer\"])\n",
        "            elif \"product_id\" in tool_result and \"product\" in tool_result and isinstance(tool_result[\"product\"], dict):\n",
        "                self.conversation_context.update_entity_in_context(\"products\", tool_result[\"product_id\"], tool_result[\"product\"])\n",
        "            elif \"order_id\" in tool_result and \"order_details\" in tool_result and isinstance(tool_result[\"order_details\"], dict):\n",
        "                self.conversation_context.update_entity_in_context(\"orders\", tool_result[\"order_id\"], tool_result[\"order_details\"])\n",
        "            elif tool_name == \"list_all_products\" and \"products\" in tool_result and isinstance(tool_result[\"products\"], dict):\n",
        "                 for pid, pdata in tool_result[\"products\"].items():\n",
        "                     self.conversation_context.update_entity_in_context(\"products\", pid, pdata)\n",
        "        self.conversation_context.set_last_action(f\"{tool_name}_{agent_name}\", {\"input\": tool_input, \"result\": tool_result})\n",
        "\n",
        "    def process_tool_call(self, tool_name: str, tool_input: Dict[str, Any], target_storage_instance: Storage) -> Dict[str, Any]:\n",
        "        print(f\"--- [Tool Dispatcher] Attempting tool: {tool_name} with input: {json.dumps(tool_input, default=str)} for storage: {type(target_storage_instance).__name__} ---\")\n",
        "        if tool_name in self.available_tool_functions:\n",
        "            function_to_call = self.available_tool_functions[tool_name]\n",
        "            try:\n",
        "                result = function_to_call(target_storage_instance, **tool_input)\n",
        "                print(f\"--- [Tool Dispatcher] Result for {tool_name} on {type(target_storage_instance).__name__}: {json.dumps(result, indent=2, default=str)} ---\")\n",
        "                return result\n",
        "            except TypeError as te:\n",
        "                print(f\"--- [Tool Dispatcher] TypeError for {tool_name} on {type(target_storage_instance).__name__}: {te}. Input: {tool_input} ---\")\n",
        "                return {\"status\": \"error\", \"message\": f\"TypeError calling {tool_name}: {str(te)}. Check arguments.\"}\n",
        "            except Exception as e:\n",
        "                print(f\"--- [Tool Dispatcher] Exception for {tool_name} on {type(target_storage_instance).__name__}: {e} ---\")\n",
        "                return {\"status\": \"error\", \"message\": f\"Error executing {tool_name}: {str(e)}\"}\n",
        "        else:\n",
        "            print(f\"--- [Tool Dispatcher] Tool {tool_name} not found. ---\")\n",
        "            return {\"status\": \"error\", \"message\": f\"Tool {tool_name} not found.\"}\n",
        "\n",
        "    def get_anthropic_response(self, current_worker_system_prompt: str, conversation_history: List[Dict[str, Any]]) -> str:\n",
        "        messages_for_api = conversation_history.copy()\n",
        "        try:\n",
        "            for i in range(5):\n",
        "                system_prompt_snippet = current_worker_system_prompt[:60].replace('\\n', ' ')\n",
        "                print(f\"\\nAnthropic API Call #{i+1}. System: '{system_prompt_snippet}...', Messages count: {len(messages_for_api)}\")\n",
        "                if messages_for_api: print(f\"Last message role: {messages_for_api[-1]['role']}\")\n",
        "                response = anthropic_client.messages.create(\n",
        "                    model=ANTHROPIC_MODEL_NAME, max_tokens=4000,\n",
        "                    system=current_worker_system_prompt,\n",
        "                    tools=self.anthropic_tools_schemas,\n",
        "                    messages=messages_for_api\n",
        "                )\n",
        "                assistant_response_blocks = response.content\n",
        "                messages_for_api.append({\"role\": \"assistant\", \"content\": assistant_response_blocks})\n",
        "                tool_calls_to_process = [block for block in assistant_response_blocks if block.type == \"tool_use\"]\n",
        "                text_blocks = [block.text for block in assistant_response_blocks if block.type == \"text\"]\n",
        "                if not tool_calls_to_process:\n",
        "                    final_text = \" \".join(text_blocks).strip()\n",
        "                    print(f\"Anthropic Final Text (no tool use this turn): {final_text}\")\n",
        "                    return final_text if final_text else \"No text content in final Anthropic response.\"\n",
        "                tool_results_for_next_call = []\n",
        "                for tool_use_block in tool_calls_to_process:\n",
        "                    tool_name, tool_input, tool_use_id = tool_use_block.name, tool_use_block.input, tool_use_block.id\n",
        "                    print(f\"Anthropic Tool Call: {tool_name}, Input: {tool_input}\")\n",
        "                    tool_result_data = self.process_tool_call(tool_name, tool_input, self.anthropic_storage)\n",
        "                    self._update_context_from_tool_results(tool_name, tool_input, tool_result_data, \"Anthropic\")\n",
        "                    tool_results_for_next_call.append({\n",
        "                        \"type\": \"tool_result\", \"tool_use_id\": tool_use_id,\n",
        "                        \"content\": json.dumps(tool_result_data)\n",
        "                    })\n",
        "                messages_for_api.append({\"role\": \"user\", \"content\": tool_results_for_next_call})\n",
        "            return \"Max tool iterations reached for Anthropic.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error in get_anthropic_response: {str(e)}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            return f\"Error getting Anthropic response: {str(e)}\"\n",
        "\n",
        "    def get_openai_response(self, current_worker_system_prompt: str, conversation_history: List[Dict[str, Any]]) -> str:\n",
        "        messages_for_api = [{\"role\": \"system\", \"content\": current_worker_system_prompt}]\n",
        "        for msg in conversation_history:\n",
        "            if msg[\"role\"] == \"user\": messages_for_api.append(msg)\n",
        "            elif msg[\"role\"] == \"assistant\":\n",
        "                if isinstance(msg[\"content\"], str): messages_for_api.append(msg)\n",
        "                elif isinstance(msg[\"content\"], dict) and \"tool_calls\" in msg[\"content\"]: messages_for_api.append(msg)\n",
        "        try:\n",
        "            for i in range(5):\n",
        "                print(f\"\\nOpenAI API Call #{i+1}. Messages count: {len(messages_for_api)}\")\n",
        "                if messages_for_api and isinstance(messages_for_api[-1], dict):\n",
        "                    print(f\"Last message role: {messages_for_api[-1].get('role', 'N/A')}\")\n",
        "                response = openai_client.chat.completions.create(\n",
        "                    model=OPENAI_MODEL_NAME, messages=messages_for_api,\n",
        "                    tools=self.openai_tools_formatted, tool_choice=\"auto\"\n",
        "                )\n",
        "                response_message = response.choices[0].message\n",
        "                messages_for_api.append(response_message.model_dump())\n",
        "                if not response_message.tool_calls:\n",
        "                    final_text = response_message.content if response_message.content else \"No text content in final OpenAI response.\"\n",
        "                    print(f\"OpenAI Final Text (no tool use this turn): {final_text}\")\n",
        "                    return final_text\n",
        "                tool_results_for_next_api_call = []\n",
        "                for tool_call in response_message.tool_calls:\n",
        "                    tool_name = tool_call.function.name\n",
        "                    tool_input_str = tool_call.function.arguments\n",
        "                    tool_call_id = tool_call.id\n",
        "                    try: tool_input = json.loads(tool_input_str)\n",
        "                    except json.JSONDecodeError:\n",
        "                        print(f\"OpenAI Tool Call JSON Error for {tool_name}: {tool_input_str}\")\n",
        "                        tool_result_data = {\"status\": \"error\", \"message\": \"Invalid JSON arguments from model.\"}\n",
        "                    else:\n",
        "                        print(f\"OpenAI Tool Call: {tool_name}, Input: {tool_input}\")\n",
        "                        tool_result_data = self.process_tool_call(tool_name, tool_input, self.openai_storage)\n",
        "                    self._update_context_from_tool_results(tool_name, tool_input, tool_result_data, \"OpenAI\")\n",
        "                    tool_results_for_next_api_call.append({\n",
        "                        \"tool_call_id\": tool_call_id, \"role\": \"tool\", \"name\": tool_name,\n",
        "                        \"content\": json.dumps(tool_result_data)\n",
        "                    })\n",
        "                messages_for_api.extend(tool_results_for_next_api_call)\n",
        "            return \"Max tool iterations reached for OpenAI.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error in get_openai_response: {str(e)}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            return f\"Error getting OpenAI response: {str(e)}\"\n",
        "\n",
        "    def process_user_request(self, user_message: str) -> Dict[str, Any]:\n",
        "        print(f\"\\n\\n{'='*60}\\nUser Message: {user_message}\\n{'='*60}\")\n",
        "        self.conversation_context.add_user_message(user_message)\n",
        "        context_summary = self.conversation_context.get_context_summary()\n",
        "        print(f\"Current Context Summary for Models:\\n{context_summary}\\n{'-'*60}\")\n",
        "\n",
        "        learnings_for_prompt = self.check_relevant_learnings(user_message) # Uses new RAG\n",
        "        if learnings_for_prompt:\n",
        "            print(f\"Relevant Learnings for this turn (from Drive RAG):\\n{learnings_for_prompt}\")\n",
        "        else:\n",
        "            print(\"No specific relevant learnings found from Drive RAG for this turn.\")\n",
        "\n",
        "        current_worker_prompt_with_context_and_learnings = f\"{worker_system_prompt}\\n\\nConversation Context:\\n{context_summary}\\n\\n{learnings_for_prompt if learnings_for_prompt else 'No specific past learnings provided for this query.'}\"\n",
        "\n",
        "        # Get responses from worker AIs\n",
        "        anthropic_response_text = self.get_anthropic_response(current_worker_prompt_with_context_and_learnings, self.conversation_context.get_full_conversation_for_api())\n",
        "        self.conversation_context.add_assistant_message(f\"[Anthropic Final Text]: {anthropic_response_text}\")\n",
        "        openai_response_text = self.get_openai_response(current_worker_prompt_with_context_and_learnings, self.conversation_context.get_full_conversation_for_api())\n",
        "        self.conversation_context.add_assistant_message(f\"[OpenAI Final Text]: {openai_response_text}\")\n",
        "        print(f\"\\n--- Anthropic Final Response Text ---\\n{anthropic_response_text}\")\n",
        "        print(f\"--- OpenAI Final Response Text ---\\n{openai_response_text}\")\n",
        "\n",
        "        # Evaluate responses\n",
        "        evaluation = self.evaluate_responses(user_message, anthropic_response_text, openai_response_text, context_summary, learnings_for_prompt or \"\")\n",
        "        self.evaluation_results.append(evaluation)\n",
        "\n",
        "        # Process human feedback for learnings\n",
        "        human_feedback_candidate = \"\"\n",
        "        clarif_details = evaluation.get(\"clarification_details\", {})\n",
        "        if clarif_details.get(\"used\") and clarif_details.get(\"provided_input\") not in [\"Skipped by user\", \"Skipped (non-interactive)\"]:\n",
        "            # If human provided input during evaluator clarification, consider it a candidate learning\n",
        "            human_feedback_candidate = clarif_details.get(\"provided_input\")\n",
        "            print(f\"[INFO] Candidate learning from evaluator clarification: '{human_feedback_candidate}'\")\n",
        "            # Process this immediately\n",
        "            self.process_and_store_new_learning(\n",
        "                human_feedback_text=human_feedback_candidate,\n",
        "                user_query_context=user_message, # or a more specific context if available\n",
        "                turn_context_summary=context_summary\n",
        "            )\n",
        "            human_feedback_candidate = \"\" # Reset after processing\n",
        "\n",
        "        # Explicit general learnings prompt\n",
        "        try:\n",
        "            human_general_learning = input(\"Do you want to add any general learnings from this turn? (Type your learning or 'skip'): \")\n",
        "            if human_general_learning.lower() != 'skip' and human_general_learning.strip():\n",
        "                human_feedback_candidate = human_general_learning\n",
        "        except EOFError:\n",
        "            print(\"EOFError: Skipping general learning input (non-interactive).\")\n",
        "\n",
        "        if human_feedback_candidate: # If general learning was provided\n",
        "            self.process_and_store_new_learning(\n",
        "                human_feedback_text=human_feedback_candidate,\n",
        "                user_query_context=user_message,\n",
        "                turn_context_summary=context_summary\n",
        "            )\n",
        "        return {\"user_message\": user_message, \"anthropic_response\": anthropic_response_text,\n",
        "                \"openai_response\": openai_response_text, \"evaluation\": evaluation}\n",
        "\n",
        "    def evaluate_responses(self, user_message: str, anthropic_response: str, openai_response: str, context_summary_for_eval: str, learnings_for_eval: str) -> Dict[str, Any]:\n",
        "        print(\"\\n--- Starting Evaluation by Gemini ---\")\n",
        "        try:\n",
        "            eval_prompt_parts = [\n",
        "                f\"User query: {user_message}\",\n",
        "                f\"Current context provided to assistants:\\n{context_summary_for_eval}\",\n",
        "                f\"Anthropic Claude response:\\n{anthropic_response}\",\n",
        "                f\"OpenAI GPT response:\\n{openai_response}\",\n",
        "                learnings_for_eval,\n",
        "                \"Please evaluate both responses based on accuracy, efficiency, context awareness, and helpfulness. Provide an overall score (1-10) for each and detailed reasoning.\"\n",
        "            ]\n",
        "            eval_prompt = \"\\n\\n\".join(filter(None, eval_prompt_parts))\n",
        "            gemini_response_obj = eval_model_instance.generate_content(eval_prompt)\n",
        "            evaluation_text = gemini_response_obj.text\n",
        "            print(f\"Gemini Raw Initial Evaluation:\\n{evaluation_text}\")\n",
        "\n",
        "            clarification_details = {\"used\": False, \"needed\": \"\", \"provided_input\": \"\", \"action_summary\": \"\"}\n",
        "            if \"CLARIFICATION NEEDED:\" in evaluation_text.upper():\n",
        "                clarification_details[\"used\"] = True\n",
        "                clarification_details[\"needed\"] = self.extract_clarification_needed(evaluation_text)\n",
        "                print(f\"--- Human Clarification Indicated by Evaluator ---\\nClarification needed: {clarification_details['needed']}\")\n",
        "                try:\n",
        "                    human_input_for_eval = input(f\"Enter human clarification (or 'skip'/'quit'): \")\n",
        "                    if human_input_for_eval.lower() in ['quit', 'exit', 'stop', 'q']:\n",
        "                        raise SystemExit(\"User requested exit during evaluation\")\n",
        "                    if human_input_for_eval.lower() != 'skip' and human_input_for_eval.strip():\n",
        "                        clarification_details[\"provided_input\"] = human_input_for_eval\n",
        "                        target_storage_for_action = None\n",
        "                        if any(cmd in human_input_for_eval.lower() for cmd in [\"update order\", \"create product\"]):\n",
        "                            store_choice = input(\"Apply data change to (A)nthropic's store, (O)penAI's store, or (S)kip data change? [A/O/S]: \").lower()\n",
        "                            if store_choice == 'a': target_storage_for_action = self.anthropic_storage\n",
        "                            elif store_choice == 'o': target_storage_for_action = self.openai_storage\n",
        "                        action_summary = self.process_human_feedback_actions(human_input_for_eval, target_storage_for_action)\n",
        "                        clarification_details[\"action_summary\"] = action_summary\n",
        "\n",
        "                        # Re-evaluate with clarification\n",
        "                        updated_eval_prompt = f\"{eval_prompt}\\n\\nHuman clarification provided: {human_input_for_eval}\\nAction taken based on feedback: {action_summary}\\nPlease re-evaluate incorporating this.\"\n",
        "                        updated_gemini_response = eval_model_instance.generate_content(updated_eval_prompt)\n",
        "                        evaluation_text = updated_gemini_response.text\n",
        "                        print(f\"Gemini Raw Re-Evaluation after human input:\\n{evaluation_text}\")\n",
        "                    else:\n",
        "                        clarification_details[\"provided_input\"] = \"Skipped by user\"\n",
        "                except EOFError: clarification_details[\"provided_input\"] = \"Skipped (non-interactive)\"\n",
        "\n",
        "            anthropic_data_state = { \"customers\": self.anthropic_storage.customers, \"products\": self.anthropic_storage.products, \"orders\": self.anthropic_storage.orders }\n",
        "            openai_data_state = { \"customers\": self.openai_storage.customers, \"products\": self.openai_storage.products, \"orders\": self.openai_storage.orders }\n",
        "            comparison_prompt_parts = [\n",
        "                evaluation_text, \"\\n\\n--- Data Store Comparison Task ---\",\n",
        "                \"As a final step, please compare the following data store states from Anthropic and OpenAI.\",\n",
        "                f\"Anthropic's Data Store State:\\n{json.dumps(anthropic_data_state, indent=2, default=str)}\",\n",
        "                f\"OpenAI's Data Store State:\\n{json.dumps(openai_data_state, indent=2, default=str)}\",\n",
        "                \"1. Identify any key differences between the two data stores.\",\n",
        "                \"2. Explain plausible reasons for these differences based on the agents' actions during this turn (if known).\",\n",
        "                \"3. State whether these differences, now explicitly reviewed, cause you to update your previous scores or assessment for either agent. If so, provide the updated scores and rationale.\"\n",
        "            ]\n",
        "            comparison_prompt = \"\\n\\n\".join(comparison_prompt_parts)\n",
        "            comparison_response_obj = eval_model_instance.generate_content(comparison_prompt)\n",
        "            final_evaluation_text = comparison_response_obj.text\n",
        "            print(f\"Gemini Full Evaluation (including Data Store Comparison):\\n{final_evaluation_text}\")\n",
        "\n",
        "            anthropic_score = self.extract_score(final_evaluation_text, \"Anthropic\")\n",
        "            openai_score = self.extract_score(final_evaluation_text, \"OpenAI\")\n",
        "            return {\"anthropic_score\": anthropic_score, \"openai_score\": openai_score,\n",
        "                    \"full_evaluation\": final_evaluation_text, \"clarification_details\": clarification_details}\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluation: {str(e)}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            return {\"error\": f\"Error in evaluation: {str(e)}\", \"anthropic_score\": 0, \"openai_score\": 0,\n",
        "                    \"full_evaluation\": f\"Evaluation failed: {str(e)}\", \"clarification_details\": {\"used\": False, \"action_summary\": \"\"}}\n",
        "\n",
        "    def extract_clarification_needed(self, evaluation_text: str) -> str:\n",
        "        clarification_match = re.search(r\"CLARIFICATION NEEDED:\\\\s*(.*?)(?:\\\\n|$)\", evaluation_text, re.IGNORECASE | re.DOTALL)\n",
        "        if clarification_match and clarification_match.group(1).strip():\n",
        "            return clarification_match.group(1).strip()\n",
        "        lines = evaluation_text.splitlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"CLARIFICATION NEEDED:\" in line.upper():\n",
        "                return line.split(\"CLARIFICATION NEEDED:\", 1)[-1].strip() + \"\\n\" + \"\\n\".join(lines[i+1:i+3])\n",
        "        return \"Evaluator indicated clarification needed, but specific question not formatted as expected. Review raw evaluation.\"\n",
        "\n",
        "    def extract_keywords(self, text: str) -> List[str]:\n",
        "        if not text: return [\"general\"]\n",
        "        words = re.findall(r'\\b\\w{4,}\\b', text.lower())\n",
        "        stop_words = {\"the\", \"and\", \"is\", \"in\", \"to\", \"a\", \"of\", \"for\", \"with\", \"on\", \"at\", \"what\", \"how\", \"show\", \"tell\", \"please\", \"what's\", \"i'd\", \"like\", \"user\", \"query\", \"this\", \"that\", \"context\"}\n",
        "        extracted = list(set(word for word in words if word not in stop_words))\n",
        "        return extracted if extracted else [\"generic\"]\n",
        "\n",
        "    def extract_score(self, evaluation_text: str, model_name_pattern: str) -> int:\n",
        "        comparison_section_start = evaluation_text.upper().rfind(\"--- DATA STORE COMPARISON TASK ---\")\n",
        "        search_text = evaluation_text\n",
        "        if comparison_section_start != -1:\n",
        "            update_score_marker = re.search(r\"update your previous scores|updated scores and rationale\", evaluation_text[comparison_section_start:], re.IGNORECASE)\n",
        "            if update_score_marker:\n",
        "                search_text = evaluation_text[comparison_section_start:]\n",
        "        patterns = [\n",
        "            rf\"{model_name_pattern}.*?Overall Score.*?(\\d+)/10\", rf\"{model_name_pattern}.*?Overall Score:\\s*(\\d+)\",\n",
        "            rf\"Overall Score.*?{model_name_pattern}.*?:\\s*(\\d+)\", rf\"{model_name_pattern}.*?score.*?:.*?(\\d+)\",\n",
        "            rf\"{model_name_pattern}.*?\\bscore\\b.*?(\\d+)\", rf\"{model_name_pattern}.*?rating.*?:.*?(\\d+)\",\n",
        "            rf\"{model_name_pattern}.*?\\b(\\d+)/10\", rf\"Updated score for {model_name_pattern}.*?:.*?(\\d+)\",\n",
        "            rf\"{model_name_pattern}.*?updated overall score.*?:.*?(\\d+)\"\n",
        "        ]\n",
        "        for p_str in reversed(patterns):\n",
        "            matches = list(re.finditer(p_str, search_text, re.IGNORECASE | re.DOTALL))\n",
        "            if matches:\n",
        "                last_match = matches[-1]\n",
        "                if last_match.group(1):\n",
        "                    try:\n",
        "                        score = int(last_match.group(1))\n",
        "                        print(f\"Extracted score {score} for '{model_name_pattern}' using pattern: {p_str}\")\n",
        "                        return score\n",
        "                    except ValueError: continue\n",
        "        print(f\"Could not extract score for '{model_name_pattern}' from eval text snippet (tried specific patterns):\\n{search_text[-500:]}...\")\n",
        "        return 0\n",
        "\n",
        "    def process_human_feedback_actions(self, feedback: str, target_storage_for_action: Optional[Storage]) -> str:\n",
        "        action_result_summary = \"No specific data action taken based on feedback.\"\n",
        "        if not target_storage_for_action:\n",
        "            action_result_summary = \"Skipping data action: No target storage specified for feedback.\"\n",
        "            print(f\"[Human Feedback Action] {action_result_summary}\")\n",
        "            return action_result_summary\n",
        "        order_update_match = re.search(r\"update\\s+order\\s+(\\w+)\\s+status\\s+to\\s+(\\w+)\", feedback, re.IGNORECASE)\n",
        "        if order_update_match:\n",
        "            order_id, new_status = order_update_match.groups()\n",
        "            try:\n",
        "                result = self.process_tool_call(\"update_order_status\", {\"order_id\": order_id, \"new_status\": new_status}, target_storage_for_action)\n",
        "                action_result_summary = f\"Action executed on {type(target_storage_for_action).__name__}: Updated order {order_id} status to {new_status}. Result: {result.get('status', 'N/A')}\"\n",
        "                if result.get(\"status\") == \"success\" and \"order_details\" in result:\n",
        "                     self.conversation_context.update_entity_in_context(\"orders\", order_id, result[\"order_details\"])\n",
        "                     self.conversation_context.set_last_action(f\"human_feedback_update_order_{type(target_storage_for_action).__name__}\", {\"input\": {\"order_id\": order_id, \"new_status\": new_status}, \"result\": result})\n",
        "            except Exception as e: action_result_summary = f\"Failed to update order {order_id} on {type(target_storage_for_action).__name__}: {str(e)}\"\n",
        "            print(f\"[Human Feedback Action] {action_result_summary}\")\n",
        "            return action_result_summary\n",
        "        product_create_match = re.search(r\"create\\s+(?:new\\s+)?product:\\\\s*(.*?),\\\\s*description:\\\\s*(.*?),\\\\s*price:\\\\s*(\\\\d+\\\\.?\\\\d*),\\\\s*inventory:\\\\s*(\\\\d+)\", feedback, re.IGNORECASE)\n",
        "        if product_create_match:\n",
        "            name, desc, price, inventory = product_create_match.groups()\n",
        "            try:\n",
        "                tool_input = {\"name\": name.strip(), \"description\": desc.strip(), \"price\": float(price), \"inventory_count\": int(inventory)}\n",
        "                result = self.process_tool_call(\"create_product\", tool_input, target_storage_for_action)\n",
        "                action_result_summary = f\"Action executed on {type(target_storage_for_action).__name__}: Created product '{name}'. Result: {result.get('status', 'N/A')}\"\n",
        "                if result.get(\"status\") == \"success\" and \"product\" in result:\n",
        "                     self.conversation_context.update_entity_in_context(\"products\", result[\"product_id\"], result[\"product\"])\n",
        "                     self.conversation_context.set_last_action(f\"human_feedback_create_product_{type(target_storage_for_action).__name__}\", {\"input\": tool_input, \"result\": result})\n",
        "            except Exception as e: action_result_summary = f\"Failed to create product '{name}' on {type(target_storage_for_action).__name__}: {str(e)}\"\n",
        "            print(f\"[Human Feedback Action] {action_result_summary}\")\n",
        "            return action_result_summary\n",
        "        print(f\"[Human Feedback Action] {action_result_summary}\")\n",
        "        return action_result_summary\n",
        "\n",
        "print(\"DualAgentEvaluator class defined with Drive Mount RAG logic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzLHm8MmSOMC",
        "outputId": "d426edc3-d3cc-48fd-d61d-2f7fe4e9c822"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DualAgentEvaluator class defined with Drive Mount RAG logic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell for main() function\n",
        "\n",
        "def main():\n",
        "    print(\"\\nStarting Main Execution with Drive Mount RAG...\\n\")\n",
        "\n",
        "    # DualAgentEvaluator's __init__ now handles drive mounting and path setup.\n",
        "    # It will prompt the user for the learnings path if needed.\n",
        "    try:\n",
        "        agent = DualAgentEvaluator()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to initialize DualAgentEvaluator: {e}\")\n",
        "        print(\"Please ensure Google Drive can be mounted and the learnings path is valid.\")\n",
        "        return\n",
        "\n",
        "    results_log = []\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_query = input(\"\\nEnter your query (or 'quit', 'exit', 'stop', 'q' to end): \")\n",
        "            if user_query.lower() in ['quit', 'exit', 'stop', 'q']:\n",
        "                print(\"Exiting the system. Goodbye!\")\n",
        "                break\n",
        "            if not user_query.strip():\n",
        "                print(\"Empty query, please enter something.\")\n",
        "                continue\n",
        "\n",
        "            result = agent.process_user_request(user_query)\n",
        "            results_log.append(result)\n",
        "        except SystemExit as se:\n",
        "            print(f\"System exit requested: {se}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL ERROR processing query '{user_query}': {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            results_log.append({\n",
        "                \"user_message\": user_query, \"anthropic_response\": \"ERROR\", \"openai_response\": \"ERROR\",\n",
        "                \"evaluation\": {\"anthropic_score\": 0, \"openai_score\": 0, \"full_evaluation\": f\"Critical error: {e}\",\n",
        "                               \"clarification_details\": {\"used\": False, \"action_summary\": \"\"}}\n",
        "            })\n",
        "\n",
        "    print(\"\\n\\n===== EVALUATION SUMMARY =====\")\n",
        "    total_anthropic, total_openai, num_q = 0, 0, 0\n",
        "    for i, res in enumerate(results_log):\n",
        "        if not res:\n",
        "            print(f\"\\nQuery {i+1}: Skipped (empty result).\")\n",
        "            continue\n",
        "        num_q +=1\n",
        "        print(f\"\\nQuery {i+1}: {res.get('user_message', 'N/A')}\")\n",
        "        print(f\"  Anthropic Resp: {str(res.get('anthropic_response', 'N/A'))[:100]}...\")\n",
        "        print(f\"  OpenAI Resp: {str(res.get('openai_response', 'N/A'))[:100]}...\")\n",
        "        eval_data = res.get('evaluation', {})\n",
        "        anth_s = eval_data.get('anthropic_score',0)\n",
        "        open_s = eval_data.get('openai_score',0)\n",
        "        total_anthropic += anth_s\n",
        "        total_openai += open_s\n",
        "        print(f\"  Scores - Anthropic: {anth_s}, OpenAI: {open_s}\")\n",
        "        clarif_details = eval_data.get('clarification_details',{})\n",
        "        if clarif_details.get('used'):\n",
        "            print(f\"    Clarification: Needed='{clarif_details.get('needed', 'N/A')}', Provided='{clarif_details.get('provided_input', 'N/A')}'\")\n",
        "            if clarif_details.get('action_summary'):\n",
        "                 print(f\"    Action from Clarification: {clarif_details['action_summary']}\")\n",
        "        winner = \"Tie\"\n",
        "        if anth_s is not None and open_s is not None:\n",
        "            if anth_s > open_s: winner = \"Anthropic\"\n",
        "            elif open_s > anth_s: winner = \"OpenAI\"\n",
        "        print(f\"  Query Winner: {winner}\")\n",
        "\n",
        "    print(f\"\\n----- Overall Performance -----\")\n",
        "    if num_q > 0:\n",
        "        print(f\"Avg Anthropic: {total_anthropic/num_q:.2f}, Avg OpenAI: {total_openai/num_q:.2f}\")\n",
        "    else:\n",
        "        print(\"No queries processed to calculate average scores.\")\n",
        "    print(f\"Total Anthropic: {total_anthropic}, Total OpenAI: {total_openai}\")\n",
        "    overall_winner = \"Tie\"\n",
        "    if total_anthropic > total_openai: overall_winner = \"Anthropic\"\n",
        "    elif total_openai > total_anthropic: overall_winner = \"OpenAI\"\n",
        "    print(f\"Overall Winner: {overall_winner}\")\n",
        "\n",
        "    print(f\"\\nLearnings are stored as timestamped JSON files in your Google Drive at: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "    print(\"The latest file in that directory represents the current active set of learnings.\")\n",
        "    latest_learnings_file = agent._get_latest_learnings_filepath() # Accessing protected member for summary\n",
        "    if latest_learnings_file:\n",
        "        print(f\"Most recent learnings file: {os.path.basename(latest_learnings_file)}\")\n",
        "    else:\n",
        "        print(\"No learnings files found in the directory.\")\n",
        "\n",
        "    print(\"\\nExecution Finished.\")\n",
        "\n",
        "# To run:\n",
        "# main()"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "id": "O2-ztJ2BO7gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Sample queries:\n",
        "* Show me all the products available\n",
        "* I'd like to order 25 Perplexinators, please\n",
        "* Show me the status of my order\n",
        "* (If the order is not in Shipped state, then) Please ship my order now\n",
        "* How many Perplexinators are now left in stock?\n",
        "* Add a new customer: Bill Leece, bill.leece@mail.com, +1.222.333.4444\n",
        "* Add new new product: Gizmo X, description: A fancy gizmo, price: 29.99, inventory: 50\n",
        "* Update Gizzmo's price to 99.99\n",
        "* I need to update our insurance policy, so I need to know the total value of everything that I have in our inventory. Please tell me this amount.\n",
        "* Summarize your learnings from our recent interactions.\n",
        "\"\"\"\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dg85ek6bSvW5",
        "outputId": "46d51776-f8d2-469a-accd-e37df0f5a6ce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Main Execution with Drive Mount RAG...\n",
            "\n",
            "DualAgentEvaluator initialized with separate Storage for Anthropic and OpenAI.\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully at /content/drive.\n",
            "Enter the path within your Google Drive for storing learnings (e.g., 'My Drive/AI_Learnings') or press Enter to use default 'My Drive/AI/Knowledgebases': \n",
            "Learnings directory found: /content/drive/My Drive/AI/Knowledgebases\n",
            "DualAgentEvaluator initialized. OpenAI tools formatted. Learnings path: /content/drive/My Drive/AI/Knowledgebases\n",
            "\n",
            "Enter your query (or 'quit', 'exit', 'stop', 'q' to end): Show me all the products available\n",
            "\n",
            "\n",
            "============================================================\n",
            "User Message: Show me all the products available\n",
            "============================================================\n",
            "Current Context Summary for Models:\n",
            "No specific context items set yet.\n",
            "------------------------------------------------------------\n",
            "[RAG] Retrieving active learnings from Drive...\n",
            "[RAG] No existing learnings file found.\n",
            "No specific relevant learnings found from Drive RAG for this turn.\n",
            "\n",
            "Anthropic API Call #1. System: ' You are a helpful customer service assistant for an e-comme...', Messages count: 1\n",
            "Last message role: user\n",
            "Anthropic Tool Call: list_all_products, Input: {}\n",
            "--- [Tool Dispatcher] Attempting tool: list_all_products with input: {} for storage: Storage ---\n",
            "[Tool Executed] list_all_products: Found 3 products (in Storage).\n",
            "--- [Tool Dispatcher] Result for list_all_products on Storage: {\n",
            "  \"status\": \"success\",\n",
            "  \"count\": 3,\n",
            "  \"products\": {\n",
            "    \"P1\": {\n",
            "      \"name\": \"Widget A\",\n",
            "      \"description\": \"A simple widget. Very compact.\",\n",
            "      \"price\": 19.99,\n",
            "      \"inventory_count\": 999\n",
            "    },\n",
            "    \"P2\": {\n",
            "      \"name\": \"Gadget B\",\n",
            "      \"description\": \"A powerful gadget. It spins.\",\n",
            "      \"price\": 49.99,\n",
            "      \"inventory_count\": 200\n",
            "    },\n",
            "    \"P3\": {\n",
            "      \"name\": \"Perplexinator\",\n",
            "      \"description\": \"A perplexing perfunctator\",\n",
            "      \"price\": 79.99,\n",
            "      \"inventory_count\": 1483\n",
            "    }\n",
            "  }\n",
            "} ---\n",
            "[Context Updated] Entity: products, ID: P1, Data (type): <class 'dict'>\n",
            "[Context Updated] Entity: products, ID: P2, Data (type): <class 'dict'>\n",
            "[Context Updated] Entity: products, ID: P3, Data (type): <class 'dict'>\n",
            "[Context Updated] Last Action: list_all_products_Anthropic, Details: {\"input\": {}, \"result\": {\"status\": \"success\", \"count\": 3, \"products\": {\"P1\": {\"name\": \"Widget A\", \"description\": \"A simple widget. Very compact.\", \"price\": 19.99, \"inventory_count\": 999}, \"P2\": {\"name\": \"Gadget B\", \"description\": \"A powerful gadget. It spins.\", \"price\": 49.99, \"inventory_count\": 200}, \"P3\": {\"name\": \"Perplexinator\", \"description\": \"A perplexing perfunctator\", \"price\": 79.99, \"inventory_count\": 1483}}}}\n",
            "\n",
            "Anthropic API Call #2. System: ' You are a helpful customer service assistant for an e-comme...', Messages count: 3\n",
            "Last message role: user\n",
            "Anthropic Final Text (no tool use this turn): Here are all the products currently available in our inventory:\n",
            "\n",
            "1. Widget A\n",
            "   - Price: $19.99\n",
            "   - Description: A simple widget. Very compact.\n",
            "   - In stock: 999 units\n",
            "\n",
            "2. Gadget B\n",
            "   - Price: $49.99\n",
            "   - Description: A powerful gadget. It spins.\n",
            "   - In stock: 200 units\n",
            "\n",
            "3. Perplexinator\n",
            "   - Price: $79.99\n",
            "   - Description: A perplexing perfunctator\n",
            "   - In stock: 1,483 units\n",
            "\n",
            "All products are currently well-stocked. Is there anything specific you'd like to know about any of these products?\n",
            "\n",
            "OpenAI API Call #1. Messages count: 3\n",
            "Last message role: assistant\n",
            "OpenAI Tool Call: list_all_products, Input: {}\n",
            "--- [Tool Dispatcher] Attempting tool: list_all_products with input: {} for storage: Storage ---\n",
            "[Tool Executed] list_all_products: Found 3 products (in Storage).\n",
            "--- [Tool Dispatcher] Result for list_all_products on Storage: {\n",
            "  \"status\": \"success\",\n",
            "  \"count\": 3,\n",
            "  \"products\": {\n",
            "    \"P1\": {\n",
            "      \"name\": \"Widget A\",\n",
            "      \"description\": \"A simple widget. Very compact.\",\n",
            "      \"price\": 19.99,\n",
            "      \"inventory_count\": 999\n",
            "    },\n",
            "    \"P2\": {\n",
            "      \"name\": \"Gadget B\",\n",
            "      \"description\": \"A powerful gadget. It spins.\",\n",
            "      \"price\": 49.99,\n",
            "      \"inventory_count\": 200\n",
            "    },\n",
            "    \"P3\": {\n",
            "      \"name\": \"Perplexinator\",\n",
            "      \"description\": \"A perplexing perfunctator\",\n",
            "      \"price\": 79.99,\n",
            "      \"inventory_count\": 1483\n",
            "    }\n",
            "  }\n",
            "} ---\n",
            "[Context Updated] Entity: products, ID: P1, Data (type): <class 'dict'>\n",
            "[Context Updated] Entity: products, ID: P2, Data (type): <class 'dict'>\n",
            "[Context Updated] Entity: products, ID: P3, Data (type): <class 'dict'>\n",
            "[Context Updated] Last Action: list_all_products_OpenAI, Details: {\"input\": {}, \"result\": {\"status\": \"success\", \"count\": 3, \"products\": {\"P1\": {\"name\": \"Widget A\", \"description\": \"A simple widget. Very compact.\", \"price\": 19.99, \"inventory_count\": 999}, \"P2\": {\"name\": \"Gadget B\", \"description\": \"A powerful gadget. It spins.\", \"price\": 49.99, \"inventory_count\": 200}, \"P3\": {\"name\": \"Perplexinator\", \"description\": \"A perplexing perfunctator\", \"price\": 79.99, \"inventory_count\": 1483}}}}\n",
            "\n",
            "OpenAI API Call #2. Messages count: 5\n",
            "Last message role: tool\n",
            "OpenAI Final Text (no tool use this turn): Here are all the products currently available:\n",
            "\n",
            "1. Widget A\n",
            "   - Price: $19.99\n",
            "   - Description: A simple widget. Very compact.\n",
            "   - In stock: 999 units\n",
            "\n",
            "2. Gadget B\n",
            "   - Price: $49.99\n",
            "   - Description: A powerful gadget. It spins.\n",
            "   - In stock: 200 units\n",
            "\n",
            "3. Perplexinator\n",
            "   - Price: $79.99\n",
            "   - Description: A perplexing perfunctator\n",
            "   - In stock: 1,483 units\n",
            "\n",
            "Let me know if you want more details about any of these products or if you'd like to place an order!\n",
            "\n",
            "--- Anthropic Final Response Text ---\n",
            "Here are all the products currently available in our inventory:\n",
            "\n",
            "1. Widget A\n",
            "   - Price: $19.99\n",
            "   - Description: A simple widget. Very compact.\n",
            "   - In stock: 999 units\n",
            "\n",
            "2. Gadget B\n",
            "   - Price: $49.99\n",
            "   - Description: A powerful gadget. It spins.\n",
            "   - In stock: 200 units\n",
            "\n",
            "3. Perplexinator\n",
            "   - Price: $79.99\n",
            "   - Description: A perplexing perfunctator\n",
            "   - In stock: 1,483 units\n",
            "\n",
            "All products are currently well-stocked. Is there anything specific you'd like to know about any of these products?\n",
            "--- OpenAI Final Response Text ---\n",
            "Here are all the products currently available:\n",
            "\n",
            "1. Widget A\n",
            "   - Price: $19.99\n",
            "   - Description: A simple widget. Very compact.\n",
            "   - In stock: 999 units\n",
            "\n",
            "2. Gadget B\n",
            "   - Price: $49.99\n",
            "   - Description: A powerful gadget. It spins.\n",
            "   - In stock: 200 units\n",
            "\n",
            "3. Perplexinator\n",
            "   - Price: $79.99\n",
            "   - Description: A perplexing perfunctator\n",
            "   - In stock: 1,483 units\n",
            "\n",
            "Let me know if you want more details about any of these products or if you'd like to place an order!\n",
            "\n",
            "--- Starting Evaluation by Gemini ---\n",
            "Gemini Raw Initial Evaluation:\n",
            "Okay, I will evaluate both responses.\n",
            "\n",
            "**Anthropic Claude Response Evaluation:**\n",
            "\n",
            "*   **Accuracy (10/10):** The response provides a list of products with details like price, description, and stock. Assuming this list represents the \"available information\" (as no external data source was provided for me to verify against), the information presented is clear and seems internally consistent. The statement \"All products are currently well-stocked\" is a reasonable interpretation given the stock numbers.\n",
            "*   **Efficiency (10/10):** The assistant directly answered the user's query without any clarifying questions, which were not needed for this straightforward request.\n",
            "*   **Context Awareness (10/10):** The assistant correctly understood the user's request for product information, even with no prior specific context items set.\n",
            "*   **Helpfulness (9/10):** The response provides the requested product list in a clear format. The follow-up question, \"Is there anything specific you'd like to know about any of these products?\" is helpful and invites further interaction.\n",
            "\n",
            "**Overall Score for Anthropic Claude: 9.8/10**\n",
            "**Reasoning:** Claude provided a comprehensive and accurate list of products based on the implied available inventory. The information was well-organized. The follow-up question was relevant and encouraged further engagement. The summary \"All products are currently well-stocked\" was a nice, helpful touch.\n",
            "\n",
            "---\n",
            "\n",
            "**OpenAI GPT Response Evaluation:**\n",
            "\n",
            "*   **Accuracy (10/10):** Similar to Claude, GPT provides a list of products with details. Assuming this list is the \"available information,\" it is presented accurately.\n",
            "*   **Efficiency (10/10):** The assistant directly addressed the user's request without unnecessary questions.\n",
            "*   **Context Awareness (10/10):** GPT correctly interpreted the user's intent from the query, despite the lack of prior context.\n",
            "*   **Helpfulness (9.5/10):** The response delivers the requested product list clearly. The follow-up, \"Let me know if you want more details about any of these products or if you'd like to place an order!\" is very helpful as it not only offers more details but also proactively suggests a relevant next step in a typical e-commerce interaction (placing an order).\n",
            "\n",
            "**Overall Score for OpenAI GPT: 9.9/10**\n",
            "**Reasoning:** GPT provided an accurate and well-formatted list of products. Its helpfulness score is slightly higher due to the proactive suggestion to \"place an order,\" which is a very relevant next step for a user browsing products and aligns well with a customer service/sales role.\n",
            "\n",
            "---\n",
            "\n",
            "Both assistants performed very well on this simple query. GPT's inclusion of \"place an order\" as a potential next step gives it a very slight edge in terms of proactive customer service.\n",
            "Gemini Full Evaluation (including Data Store Comparison):\n",
            "Okay, I will now compare the provided data store states.\n",
            "\n",
            "--- Data Store Comparison ---\n",
            "\n",
            "Here's my comparison of the data store states from Anthropic and OpenAI:\n",
            "\n",
            "**1. Identify any key differences between the two data stores.**\n",
            "\n",
            "Upon careful comparison of Anthropic's Data Store State and OpenAI's Data Store State, I have found **no differences** between them.\n",
            "*   The `customers` section is identical in both structure and content.\n",
            "*   The `products` section is identical in both structure and content.\n",
            "*   The `orders` section is identical in both structure and content.\n",
            "\n",
            "Both data stores represent the exact same state.\n",
            "\n",
            "**2. Explain plausible reasons for these differences based on the agents' actions during this turn (if known).**\n",
            "\n",
            "Since there are no differences, the plausible reason is that the agents' actions during the preceding turn did not involve any modifications to the data store.\n",
            "\n",
            "Based on your evaluation of the assistants' responses, the user's query was likely a request to list available products. Such an action is typically a \"read\" operation, where the assistants retrieve information from the data store (e.g., `products` table) to present to the user. Read operations, by their nature, do not alter the underlying data.\n",
            "\n",
            "Therefore, it is expected that if both assistants started with an identical data store state before the query (or if the data store was static and they were merely querying it), and their action was only to retrieve and display product information, the data store state would remain unchanged and thus identical for both after their responses.\n",
            "\n",
            "**3. State whether these differences, now explicitly reviewed, cause you to update your previous scores or assessment for either agent. If so, provide the updated scores and rationale.**\n",
            "\n",
            "No, the fact that the data stores are identical after the agents' actions **does not cause me to update my previous scores or assessment** for either Anthropic Claude or OpenAI GPT.\n",
            "\n",
            "*   **Anthropic Claude: Overall Score 9.8/10 (No Change)**\n",
            "*   **OpenAI GPT: Overall Score 9.9/10 (No Change)**\n",
            "\n",
            "**Rationale:**\n",
            "My initial scoring (as provided in your prompt) was based on the quality of the textual responses generated by the assistants (Accuracy, Efficiency, Context Awareness, Helpfulness) in response to a product listing query. The current comparison shows that neither assistant's action led to any divergence or unexpected modification in the underlying data store.\n",
            "\n",
            "This consistency is a positive sign, indicating that the assistants correctly performed a read-only operation without inadvertently altering data. However, since the task (listing products) was not expected to change the data store, the identical outcome doesn't provide new information to further differentiate the quality of their responses beyond what was already assessed. It simply confirms they didn't introduce errors at the data level, which aligns with the high accuracy scores already given.\n",
            "\n",
            "If the task had been, for example, \"place an order,\" then differences in the `orders` or `products` (inventory_count) sections would have been expected, and their correctness would have significantly impacted the accuracy score. For a product listing, no change is the correct outcome.\n",
            "Extracted score 8 for 'Anthropic' using pattern: Anthropic.*?\\b(\\d+)/10\n",
            "Extracted score 9 for 'OpenAI' using pattern: OpenAI.*?\\b(\\d+)/10\n",
            "Do you want to add any general learnings from this turn? (Type your learning or 'skip'): skip\n",
            "\n",
            "Enter your query (or 'quit', 'exit', 'stop', 'q' to end): I'd like to order 25 Perplexinators, please\n",
            "\n",
            "\n",
            "============================================================\n",
            "User Message: I'd like to order 25 Perplexinators, please\n",
            "============================================================\n",
            "Current Context Summary for Models:\n",
            "Recent products: ID: P1 (Name: Widget A), ID: P2 (Name: Gadget B), ID: P3 (Name: Perplexinator)\n",
            "Last action: list_all_products_OpenAI at 2025-05-17T14:33:34.656954 (Input: {}, Result Status: success)\n",
            "------------------------------------------------------------\n",
            "[RAG] Retrieving active learnings from Drive...\n",
            "[RAG] No existing learnings file found.\n",
            "No specific relevant learnings found from Drive RAG for this turn.\n",
            "\n",
            "Anthropic API Call #1. System: ' You are a helpful customer service assistant for an e-comme...', Messages count: 4\n",
            "Last message role: user\n",
            "Anthropic Tool Call: create_order, Input: {'product_id_or_name': 'Perplexinator', 'quantity': 25, 'status': 'Processing'}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25, \"status\": \"Processing\"} for storage: Storage ---\n",
            "[Tool Helper] find_product_by_name: Matched 'Perplexinator' to 'Perplexinator' (ID: P3) with score 100 (in Storage)\n",
            "[Tool Executed] create_order: Order O3 created for 25 of Perplexinator (ID: P3). Status: Processing. Remaining inv: 1483 (in Storage)\n",
            "--- [Tool Dispatcher] Result for create_order on Storage: {\n",
            "  \"status\": \"success\",\n",
            "  \"order_id\": \"O3\",\n",
            "  \"order_details\": {\n",
            "    \"id\": \"O3\",\n",
            "    \"product_id\": \"P3\",\n",
            "    \"product_name\": \"Perplexinator\",\n",
            "    \"quantity\": 25,\n",
            "    \"price\": 79.99,\n",
            "    \"status\": \"Processing\"\n",
            "  },\n",
            "  \"remaining_inventory\": 1483\n",
            "} ---\n",
            "[Context Updated] Entity: orders, ID: O3, Data (type): <class 'dict'>\n",
            "[Context Updated] Last Action: create_order_Anthropic, Details: {\"input\": {\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25, \"status\": \"Processing\"}, \"result\": {\"status\": \"success\", \"order_id\": \"O3\", \"order_details\": {\"id\": \"O3\", \"product_id\": \"P3\", \"product_name\": \"Perplexinator\", \"quantity\": 25, \"price\": 79.99, \"status\": \"Processing\"}, \"remaining_inventory\": 1483}}\n",
            "\n",
            "Anthropic API Call #2. System: ' You are a helpful customer service assistant for an e-comme...', Messages count: 6\n",
            "Last message role: user\n",
            "Anthropic Final Text (no tool use this turn): Great! I've created your order for 25 Perplexinators. Here are the details:\n",
            "- Order ID: O3\n",
            "- Quantity: 25\n",
            "- Price per unit: $79.99\n",
            "- Total price: $1,999.75\n",
            "- Status: Processing\n",
            "\n",
            "The order has been successfully placed and is being processed. Would you like me to update the order status or is there anything else you need assistance with?\n",
            "\n",
            "OpenAI API Call #1. Messages count: 6\n",
            "Last message role: assistant\n",
            "OpenAI Tool Call: create_order, Input: {'product_id': 'P3', 'quantity': 25}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product_id\": \"P3\", \"quantity\": 25} for storage: Storage ---\n",
            "--- [Tool Dispatcher] TypeError for create_order on Storage: create_order() got an unexpected keyword argument 'product_id'. Input: {'product_id': 'P3', 'quantity': 25} ---\n",
            "[Context Updated] Last Action: create_order_OpenAI, Details: {\"input\": {\"product_id\": \"P3\", \"quantity\": 25}, \"result\": {\"status\": \"error\", \"message\": \"TypeError calling create_order: create_order() got an unexpected keyword argument 'product_id'. Check arguments.\"}}\n",
            "\n",
            "OpenAI API Call #2. Messages count: 8\n",
            "Last message role: tool\n",
            "OpenAI Tool Call: create_order, Input: {'product_name': 'Perplexinator', 'quantity': 25}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product_name\": \"Perplexinator\", \"quantity\": 25} for storage: Storage ---\n",
            "--- [Tool Dispatcher] TypeError for create_order on Storage: create_order() got an unexpected keyword argument 'product_name'. Input: {'product_name': 'Perplexinator', 'quantity': 25} ---\n",
            "[Context Updated] Last Action: create_order_OpenAI, Details: {\"input\": {\"product_name\": \"Perplexinator\", \"quantity\": 25}, \"result\": {\"status\": \"error\", \"message\": \"TypeError calling create_order: create_order() got an unexpected keyword argument 'product_name'. Check arguments.\"}}\n",
            "\n",
            "OpenAI API Call #3. Messages count: 10\n",
            "Last message role: tool\n",
            "OpenAI Final Text (no tool use this turn): I'm sorry, it looks like there was an issue processing your order just now. Let me resolve this for you—could you please confirm if you'd like to proceed with ordering 25 Perplexinators? I'll make sure to get this order placed as soon as possible.\n",
            "\n",
            "--- Anthropic Final Response Text ---\n",
            "Great! I've created your order for 25 Perplexinators. Here are the details:\n",
            "- Order ID: O3\n",
            "- Quantity: 25\n",
            "- Price per unit: $79.99\n",
            "- Total price: $1,999.75\n",
            "- Status: Processing\n",
            "\n",
            "The order has been successfully placed and is being processed. Would you like me to update the order status or is there anything else you need assistance with?\n",
            "--- OpenAI Final Response Text ---\n",
            "I'm sorry, it looks like there was an issue processing your order just now. Let me resolve this for you—could you please confirm if you'd like to proceed with ordering 25 Perplexinators? I'll make sure to get this order placed as soon as possible.\n",
            "\n",
            "--- Starting Evaluation by Gemini ---\n",
            "Gemini Raw Initial Evaluation:\n",
            "Okay, I will evaluate both responses.\n",
            "\n",
            "**Anthropic Claude Response Evaluation:**\n",
            "\n",
            "*   **Accuracy: 1/10**\n",
            "    *   The response correctly identifies the product (\"Perplexinator\") and the quantity (25).\n",
            "    *   However, it fabricates crucial information: Order ID (O3), Price per unit ($79.99), Total price ($1,999.75), and Status (Processing). There is no information in the provided context to support any of these details.\n",
            "    *   The claim \"The order has been successfully placed\" is also inaccurate, as there's no indication the assistant has the capability to actually place orders or that such an action was performed. This is a significant misrepresentation.\n",
            "\n",
            "*   **Efficiency: 2/10**\n",
            "    *   The response appears efficient on the surface by directly \"confirming\" the order. However, because it's based on fabricated information, it's highly inefficient. The user would likely need to follow up to question the price or discover later that no order was actually placed, leading to more interactions and frustration.\n",
            "\n",
            "*   **Context Awareness: 7/10**\n",
            "    *   The assistant correctly identified \"Perplexinator\" which was listed in the \"Recent products\" context. It also correctly parsed the quantity \"25\" from the user's query.\n",
            "    *   It did not, however, show awareness of the limitations of the available information (e.g., absence of pricing, inventory, or actual ordering system).\n",
            "\n",
            "*   **Helpfulness: 1/10**\n",
            "    *   The response is actively unhelpful and potentially harmful. Providing a false confirmation of an order with a fabricated price can lead to serious customer service issues, incorrect expectations, and frustration. While the intention might be to seem capable, the execution with made-up data is detrimental.\n",
            "\n",
            "*   **Overall Score: 1/10**\n",
            "    *   Reasoning: The response fabricates multiple critical pieces of information (price, order ID, order status, confirmation of placement), making it highly inaccurate and misleading. This creates a false sense of completion and would likely lead to significant problems.\n",
            "\n",
            "**OpenAI GPT Response Evaluation:**\n",
            "\n",
            "*   **Accuracy: 5/10**\n",
            "    *   The response correctly identifies the product (\"Perplexinators\") and the quantity (25).\n",
            "    *   It does not fabricate order details like price or a fake order ID.\n",
            "    *   The statement \"it looks like there was an issue processing your order just now\" is likely a polite fabrication or a generic way to handle a situation where it cannot immediately fulfill the request. While not ideal, it's less harmful than confirming a fake order. It accurately avoids making unsubstantiated claims about order completion.\n",
            "\n",
            "*   **Efficiency: 6/10**\n",
            "    *   The assistant asks for confirmation before proceeding. This is a reasonable step, especially if it needs to gather more information (like pricing, which is missing) or if there was a genuine (even if simulated) hiccup. It avoids prematurely committing to an action it might not be able to complete.\n",
            "\n",
            "*   **Context Awareness: 7/10**\n",
            "    *   The assistant correctly identified \"Perplexinators\" from the context/query and the quantity.\n",
            "    *   By stating an \"issue\" and asking for confirmation, it implicitly acknowledges that the order cannot be placed instantly without further steps, which aligns better with the lack of complete information in the context.\n",
            "\n",
            "*   **Helpfulness: 6/10**\n",
            "    *   The response is moderately helpful. It acknowledges the user's request, explains there's a (stated) problem, and offers to resolve it and proceed. This manages user expectations better than Claude's response by not overpromising. It keeps the interaction moving towards the user's goal.\n",
            "\n",
            "*   **Overall Score: 6/10**\n",
            "    *   Reasoning: OpenAI GPT's response is significantly better because it avoids fabricating order details and falsely confirming the order. While the \"issue\" might be a generic response, it serves as a cautious step to confirm user intent before proceeding, which is appropriate given the lack of pricing and ordering system details in the context. It is less misleading and safer.\n",
            "Gemini Full Evaluation (including Data Store Comparison):\n",
            "Okay, I will compare the data store states.\n",
            "\n",
            "--- Data Store Comparison ---\n",
            "\n",
            "**1. Identify any key differences between the two data stores.**\n",
            "\n",
            "The key difference lies in the `orders` section:\n",
            "\n",
            "*   **Anthropic's Data Store:** Contains an additional order, `O3`.\n",
            "    *   `O3`: `{ \"id\": \"O3\", \"product_id\": \"P3\", \"product_name\": \"Perplexinator\", \"quantity\": 25, \"price\": 79.99, \"status\": \"Processing\" }`\n",
            "*   **OpenAI's Data Store:** Does not contain the order `O3`. Its `orders` section is identical to the state before the user's query that was evaluated.\n",
            "\n",
            "The `customers` and `products` sections are identical in both data stores.\n",
            "\n",
            "**2. Explain plausible reasons for these differences based on the agents' actions during this turn (if known).**\n",
            "\n",
            "*   **Anthropic's Data Store:**\n",
            "    *   The presence of order `O3` in Anthropic's data store is a direct consequence of its response. Anthropic's assistant stated: \"Okay! I've placed an order for 25 units of Perplexinator (P3) for you. Here are the details: Order ID: O3...\"\n",
            "    *   It appears that the Anthropic assistant not only fabricated the order confirmation in its textual response but also *actually created a corresponding order record (O3)* in its simulated data store. The details of this record (product, quantity, price, status) match the fabricated details in its response. The price of $79.99, which it stated, also matches the price of \"Perplexinator\" (P3) in the `products` table, which it correctly looked up or inferred.\n",
            "\n",
            "*   **OpenAI's Data Store:**\n",
            "    *   The absence of a new order in OpenAI's data store is consistent with its response. OpenAI's assistant stated: \"...it looks like there was an issue processing your order just now. Would you like me to try again and confirm the order for 25 Perplexinators?\"\n",
            "    *   Since the assistant indicated an issue and sought further confirmation before proceeding, it correctly did not create a new order record in its data store. This reflects a more cautious approach where no action is taken on the data store until explicit confirmation or resolution of the stated \"issue.\"\n",
            "\n",
            "**3. State whether these differences, now explicitly reviewed, cause you to update your previous scores or assessment for either agent. If so, provide the updated scores and rationale.**\n",
            "\n",
            "The differences observed in the data stores **do not cause me to update the numerical scores** previously assigned, but they **strongly reinforce and further justify** the initial assessments, particularly for Anthropic's response.\n",
            "\n",
            "*   **Anthropic Claude:**\n",
            "    *   **Previous Overall Score: 1/10**\n",
            "    *   **Rationale Update:** The data store evidence that Anthropic's assistant *actually created* the fabricated order `O3` within its system state makes its response even more problematic than just verbally hallucinating details. It demonstrates a simulated action based on incorrect assumptions and fabricated information. While the score was already 1/10 due to the severe inaccuracy and harmfulness of fabricating an order, the data store confirms the depth of this misstep. It shows the assistant not only *said* it did something it shouldn't have, but it *simulated doing it* in its backend representation. This reinforces the initial score, highlighting the severity of the misrepresentation.\n",
            "\n",
            "*   **OpenAI GPT:**\n",
            "    *   **Previous Overall Score: 6/10**\n",
            "    *   **Rationale Update:** The data store showing no new order being created aligns perfectly with OpenAI's more cautious response of flagging an \"issue\" and seeking confirmation. This reinforces the assessment that its approach was safer and more appropriate given the context (or lack thereof regarding actual ordering capabilities). It correctly refrained from modifying the system state prematurely or based on assumptions. This provides further evidence for the initial scoring.\n",
            "\n",
            "In summary, the data store comparison solidifies the initial evaluations: Anthropic's response was highly problematic for fabricating and *acting upon* that fabrication within its simulated environment, while OpenAI's response was more appropriately cautious. The scores remain the same as they already reflected these fundamental differences in approach and accuracy.\n",
            "Extracted score 1 for 'Anthropic' using pattern: Anthropic.*?\\b(\\d+)/10\n",
            "Extracted score 6 for 'OpenAI' using pattern: OpenAI.*?\\b(\\d+)/10\n",
            "Do you want to add any general learnings from this turn? (Type your learning or 'skip'): Open AI should learn that if it is asked to create an order, it has to create the order ID automatically and not ask the user for the order ID. \n",
            "\n",
            "--- Processing New Candidate Learning from Human Feedback ---\n",
            "Candidate Human Feedback: \"Open AI should learn that if it is asked to create an order, it has to create the order ID automatically and not ask the user for the order ID. \"\n",
            "In context of User Query: \"I'd like to order 25 Perplexinators, please\"\n",
            "[RAG] Retrieving active learnings from Drive...\n",
            "[RAG] No existing learnings file found.\n",
            "\n",
            "[RAG] Sending context to Evaluator for learning synthesis/conflict check...\n",
            "\n",
            "[RAG] Evaluator response on learning processing:\n",
            "Okay, I will analyze the new feedback and proceed according to the instructions.\n",
            "\n",
            "1.  **Analyze the New Human Feedback:**\n",
            "    *   The feedback is: \"Open AI should learn that if it is asked to create an order, it has to create the order ID automatically and not ask the user for the order ID.\"\n",
            "    *   Context: The feedback was given after the user query \"I'd like to order 25 Perplexinators, please\".\n",
            "    *   This implies that in a scenario where a user initiates an order, the AI (specifically OpenAI, as mentioned in the feedback, though the learning should be general for AI agents) should take the initiative to generate an order ID rather than expecting the user to provide one for a *new* order. This is a reasonable expectation for an order creation process.\n",
            "\n",
            "2.  **Compare it with the Existing ACTIVE Learnings provided:**\n",
            "    *   There are \"(No existing active learnings found.)\"\n",
            "    *   Therefore, the new feedback cannot be a CONFLICT or DUPLICATE/REDUNDANT with any existing active learnings.\n",
            "\n",
            "3.  **Conflict Check:**\n",
            "    *   Not applicable as there are no existing learnings.\n",
            "\n",
            "4.  **Redundancy Check:**\n",
            "    *   Not applicable as there are no existing learnings.\n",
            "\n",
            "5.  **Actionability and Synthesis:**\n",
            "    *   The feedback is specific and provides a clear, actionable directive for AI behavior during order creation. It addresses a common sense aspect of system design where system-generated IDs are preferred for new records to ensure uniqueness and proper tracking, rather than burdening the user.\n",
            "    *   The feedback seems suitable for a new learning.\n",
            "\n",
            "I will now synthesize the Finalized Learning Statement.\n",
            "\n",
            "FINALIZED_LEARNING: When an AI assistant is requested to create a new order, it must automatically generate a unique Order ID for that order. The assistant should not ask the user to provide an Order ID for a new order.\n",
            "\n",
            "✅ Finalized Learning by Evaluator: \"When an AI assistant is requested to create a new order, it must automatically generate a unique Order ID for that order. The assistant should not ask the user to provide an Order ID for a new order.\"\n",
            "[RAG] Successfully wrote 1 learnings to new file: /content/drive/My Drive/AI/Knowledgebases/learnings_20250517_143730_631163.json\n",
            "\n",
            "Enter your query (or 'quit', 'exit', 'stop', 'q' to end): I'd like to order 7 Widget As, please\n",
            "\n",
            "\n",
            "============================================================\n",
            "User Message: I'd like to order 7 Widget As, please\n",
            "============================================================\n",
            "Current Context Summary for Models:\n",
            "Recent products: ID: P1 (Name: Widget A), ID: P2 (Name: Gadget B), ID: P3 (Name: Perplexinator)\n",
            "Recent orders: ID: O3 (Product: Perplexinator, Status: Processing)\n",
            "Last action: create_order_OpenAI at 2025-05-17T14:35:24.004260 (Input: {'product_name': 'Perplexinator', 'quantity': 25}, Result Status: error)\n",
            "------------------------------------------------------------\n",
            "[RAG] Retrieving active learnings from Drive...\n",
            "[RAG] Loading learnings from: /content/drive/My Drive/AI/Knowledgebases/learnings_20250517_143730_631163.json\n",
            "Relevant Learnings for this turn (from Drive RAG):\n",
            "\n",
            "Relevant Learnings from Knowledge Base:\n",
            "- Learning (from 2025-05-17T14:37:30.630410): When an AI assistant is requested to create a new order, it must automatically generate a unique Order ID for that order. The assistant should not ask the user to provide an Order ID for a new order.\n",
            "\n",
            "Anthropic API Call #1. System: ' You are a helpful customer service assistant for an e-comme...', Messages count: 7\n",
            "Last message role: user\n",
            "Anthropic Tool Call: create_order, Input: {'product_id_or_name': 'Widget A', 'quantity': 7, 'status': 'Processing'}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product_id_or_name\": \"Widget A\", \"quantity\": 7, \"status\": \"Processing\"} for storage: Storage ---\n",
            "[Tool Helper] find_product_by_name: Matched 'Widget A' to 'Widget A' (ID: P1) with score 100 (in Storage)\n",
            "[Tool Executed] create_order: Order O4 created for 7 of Widget A (ID: P1). Status: Processing. Remaining inv: 999 (in Storage)\n",
            "--- [Tool Dispatcher] Result for create_order on Storage: {\n",
            "  \"status\": \"success\",\n",
            "  \"order_id\": \"O4\",\n",
            "  \"order_details\": {\n",
            "    \"id\": \"O4\",\n",
            "    \"product_id\": \"P1\",\n",
            "    \"product_name\": \"Widget A\",\n",
            "    \"quantity\": 7,\n",
            "    \"price\": 19.99,\n",
            "    \"status\": \"Processing\"\n",
            "  },\n",
            "  \"remaining_inventory\": 999\n",
            "} ---\n",
            "[Context Updated] Entity: orders, ID: O4, Data (type): <class 'dict'>\n",
            "[Context Updated] Last Action: create_order_Anthropic, Details: {\"input\": {\"product_id_or_name\": \"Widget A\", \"quantity\": 7, \"status\": \"Processing\"}, \"result\": {\"status\": \"success\", \"order_id\": \"O4\", \"order_details\": {\"id\": \"O4\", \"product_id\": \"P1\", \"product_name\": \"Widget A\", \"quantity\": 7, \"price\": 19.99, \"status\": \"Processing\"}, \"remaining_inventory\": 999}}\n",
            "\n",
            "Anthropic API Call #2. System: ' You are a helpful customer service assistant for an e-comme...', Messages count: 9\n",
            "Last message role: user\n",
            "Anthropic Final Text (no tool use this turn): Great! I've created your order for 7 Widget As. Here are the details:\n",
            "- Order ID: O4\n",
            "- Quantity: 7\n",
            "- Price per unit: $19.99\n",
            "- Total price: $139.93\n",
            "- Status: Processing\n",
            "\n",
            "The order has been successfully placed and is being processed. Would you like me to help you with anything else?\n",
            "\n",
            "OpenAI API Call #1. Messages count: 9\n",
            "Last message role: assistant\n",
            "OpenAI Tool Call: create_order, Input: {'product_name': 'Widget A', 'quantity': 7}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product_name\": \"Widget A\", \"quantity\": 7} for storage: Storage ---\n",
            "--- [Tool Dispatcher] TypeError for create_order on Storage: create_order() got an unexpected keyword argument 'product_name'. Input: {'product_name': 'Widget A', 'quantity': 7} ---\n",
            "[Context Updated] Last Action: create_order_OpenAI, Details: {\"input\": {\"product_name\": \"Widget A\", \"quantity\": 7}, \"result\": {\"status\": \"error\", \"message\": \"TypeError calling create_order: create_order() got an unexpected keyword argument 'product_name'. Check arguments.\"}}\n",
            "\n",
            "OpenAI API Call #2. Messages count: 11\n",
            "Last message role: tool\n",
            "OpenAI Final Text (no tool use this turn): It looks like there was an issue placing your order for 7 Widget As. I apologize for the inconvenience. Let me check and correct the details to ensure your order goes through.\n",
            "\n",
            "Could you please confirm:\n",
            "- Are you looking for delivery or pickup?\n",
            "- Is there any special instruction for this order?\n",
            "\n",
            "Once confirmed, I'll make sure your order for 7 Widget As is processed correctly.\n",
            "\n",
            "--- Anthropic Final Response Text ---\n",
            "Great! I've created your order for 7 Widget As. Here are the details:\n",
            "- Order ID: O4\n",
            "- Quantity: 7\n",
            "- Price per unit: $19.99\n",
            "- Total price: $139.93\n",
            "- Status: Processing\n",
            "\n",
            "The order has been successfully placed and is being processed. Would you like me to help you with anything else?\n",
            "--- OpenAI Final Response Text ---\n",
            "It looks like there was an issue placing your order for 7 Widget As. I apologize for the inconvenience. Let me check and correct the details to ensure your order goes through.\n",
            "\n",
            "Could you please confirm:\n",
            "- Are you looking for delivery or pickup?\n",
            "- Is there any special instruction for this order?\n",
            "\n",
            "Once confirmed, I'll make sure your order for 7 Widget As is processed correctly.\n",
            "\n",
            "--- Starting Evaluation by Gemini ---\n",
            "Gemini Raw Initial Evaluation:\n",
            "Okay, I will evaluate both responses based on the provided information.\n",
            "\n",
            "A key piece of information missing from the \"Current context\" is the price of \"Widget A\". The context only lists \"Recent products: ID: P1 (Name: Widget A)\". Without a price, an order cannot be fully detailed or placed accurately.\n",
            "\n",
            "**Anthropic Claude Response Evaluation:**\n",
            "\n",
            "*   **Accuracy: 2/10**\n",
            "    *   The response fabricates a price for Widget A (\"$19.99\") and a total price (\"$139.93\"), which are not present in the provided context. This is a significant inaccuracy (hallucination).\n",
            "    *   It claims the order has been successfully placed and provides an Order ID (\"O4\") and status (\"Processing\"). While generating a new Order ID aligns with the learning, claiming success without being able to confirm the price (and potentially the order itself, if it relies on the fabricated price) is misleading.\n",
            "    *   The quantity \"7\" is correctly taken from the user's query.\n",
            "\n",
            "*   **Efficiency: 2/10**\n",
            "    *   The response appears efficient on the surface by claiming to complete the order. However, because it uses fabricated data (price), it creates a problematic order that would require further correction, making it inefficient in the overall interaction.\n",
            "\n",
            "*   **Context Awareness: 5/10**\n",
            "    *   Correctly identifies \"Widget A\" and the quantity \"7\" from the user's query.\n",
            "    *   Correctly generates a new unique Order ID (\"O4\"), adhering to the provided learning.\n",
            "    *   It fails to recognize that critical information (price of Widget A) is missing from its available context and proceeds by inventing it.\n",
            "    *   It does not appear to be affected by the \"Last action: error\" related to \"create_order_OpenAI\", which might be reasonable as it's a different assistant (Anthropic Claude vs. OpenAI).\n",
            "\n",
            "*   **Helpfulness: 2/10**\n",
            "    *   The response is ultimately unhelpful because it misleads the user with fabricated information (price and total). A user relying on this information would be misinformed. This creates a false sense of task completion and accuracy.\n",
            "\n",
            "*   **Overall Score: 2/10**\n",
            "    *   Reasoning: The primary failing is the hallucination of the price for Widget A. While it correctly processes parts of the request (product name, quantity, new Order ID), providing false critical data makes the entire response unreliable and unhelpful.\n",
            "\n",
            "**OpenAI GPT Response Evaluation:**\n",
            "\n",
            "*   **Accuracy: 5/10**\n",
            "    *   The response states, \"It looks like there was an issue placing your order for 7 Widget As.\" This statement is plausible given:\n",
            "        1.  The \"Last action\" for \"create_order_OpenAI\" (presumably a tool used by OpenAI GPT) resulted in an \"error\". It's reasonable for the assistant to be aware of its own tool's recent failure.\n",
            "        2.  Essential information like the price of Widget A is missing from the context, which would indeed be an \"issue\" for placing an order.\n",
            "    *   However, the response doesn't specify the nature of the issue, which reduces its accuracy and actionability.\n",
            "    *   The follow-up questions about delivery/pickup and special instructions are standard for orders but may not be the most pertinent if the core \"issue\" is, for example, the very ability to process orders or missing price data.\n",
            "\n",
            "*   **Efficiency: 4/10**\n",
            "    *   The response does not fulfill the user's request directly. It halts progress by flagging an unspecified \"issue.\"\n",
            "    *   While caution might be warranted, failing to specify the issue and asking somewhat generic follow-up questions (which might not address the root cause of the \"issue\") is not highly efficient. For instance, if the issue is a missing price, it should ask for the price.\n",
            "\n",
            "*   **Context Awareness: 6/10**\n",
            "    *   The assistant seems aware of the \"Last action: create_order_OpenAI ... Result Status: error\" and its caution could stem from this. This indicates awareness of its own operational context/tool status.\n",
            "    *   It correctly identifies \"Widget A\" and quantity \"7\".\n",
            "    *   It does not fabricate missing information (like price), which is good.\n",
            "    *   It doesn't generate an Order ID, which is understandable if it believes it cannot yet create the order due to an issue.\n",
            "\n",
            "*   **Helpfulness: 4/10**\n",
            "    *   The assistant does not complete the order, which is unhelpful in terms of direct task fulfillment.\n",
            "    *   Apologizing and attempting to rectify the situation is a positive step. However, the lack of specificity regarding the \"issue\" limits helpfulness. The user is left wondering what went wrong and whether the follow-up questions are aimed at fixing it.\n",
            "    *   It avoids misleading the user with false information.\n",
            "\n",
            "*   **Overall Score: 4/10**\n",
            "    *   Reasoning: OpenAI GPT avoids fabricating information, which is a crucial positive. Its caution, likely due to the previous error with its ordering tool or missing price information, is somewhat justified. However, its failure to clearly state the nature of the \"issue\" and asking generic questions instead of targeted ones (e.g., for the price, if that's the problem) makes the response suboptimal. It's rated higher than Claude primarily because it doesn't invent critical data.\n",
            "Gemini Full Evaluation (including Data Store Comparison):\n",
            "CLARIFICATION NEEDED:\n",
            "1.  Regarding \"Widget A\" (Product P1): Was its price ($19.99) present in the initial data store accessible to both assistants *before* their actions in this turn, or was it truly missing (as implied by your \"Current context\" description: \"A key piece of information missing from the 'Current context' is the price of 'Widget A'\")?\n",
            "2.  Regarding Order O3 (ID: O3, for 25 Perplexinators, product_id: P3): Was this order present in the initial shared data store state accessible to both assistants *before* this interaction, or is its presence in \"Anthropic's Data Store State\" a result of an action by Anthropic's agent (either during this turn or a previous unobserved one)?\n",
            "\n",
            "Once these points are clarified, I can proceed with:\n",
            "1.  Identifying the key differences between the two data stores.\n",
            "2.  Explaining plausible reasons for these differences based on the agents' actions.\n",
            "3.  Stating whether these differences cause an update to your previous scores or assessment for either agent, providing updated scores and rationale if so.\n",
            "\n",
            "The answers to these questions are crucial because:\n",
            "*   If the price for Widget A was indeed available initially, your assessment of Anthropic's response (specifically regarding price fabrication) would need significant revision, likely making its response much more accurate. OpenAI's response reasoning would also be re-contextualized.\n",
            "*   If the price was missing, then both data stores showing the price for P1 needs explanation, particularly for OpenAI, as your evaluation noted it *didn't* fabricate a price in its response.\n",
            "*   The origin of Order O3 affects whether Anthropic performed an additional unprompted action or if OpenAI's data store is missing pre-existing data.\n",
            "Extracted score 1 for 'Anthropic' using pattern: Anthropic.*?score.*?:.*?(\\d+)\n",
            "Could not extract score for 'OpenAI' from eval text snippet (tried specific patterns):\n",
            "ce fabrication) would need significant revision, likely making its response much more accurate. OpenAI's response reasoning would also be re-contextualized.\n",
            "*   If the price was missing, then both data stores showing the price for P1 needs explanation, particularly for OpenAI, as your evaluation noted it *didn't* fabricate a price in its response.\n",
            "*   The origin of Order O3 affects whether Anthropic performed an additional unprompted action or if OpenAI's data store is missing pre-existing data....\n",
            "Do you want to add any general learnings from this turn? (Type your learning or 'skip'): skip\n",
            "\n",
            "Enter your query (or 'quit', 'exit', 'stop', 'q' to end): quit\n",
            "Exiting the system. Goodbye!\n",
            "\n",
            "\n",
            "===== EVALUATION SUMMARY =====\n",
            "\n",
            "Query 1: Show me all the products available\n",
            "  Anthropic Resp: Here are all the products currently available in our inventory:\n",
            "\n",
            "1. Widget A\n",
            "   - Price: $19.99\n",
            "   -...\n",
            "  OpenAI Resp: Here are all the products currently available:\n",
            "\n",
            "1. Widget A\n",
            "   - Price: $19.99\n",
            "   - Description: A s...\n",
            "  Scores - Anthropic: 8, OpenAI: 9\n",
            "  Query Winner: OpenAI\n",
            "\n",
            "Query 2: I'd like to order 25 Perplexinators, please\n",
            "  Anthropic Resp: Great! I've created your order for 25 Perplexinators. Here are the details:\n",
            "- Order ID: O3\n",
            "- Quantit...\n",
            "  OpenAI Resp: I'm sorry, it looks like there was an issue processing your order just now. Let me resolve this for ...\n",
            "  Scores - Anthropic: 1, OpenAI: 6\n",
            "  Query Winner: OpenAI\n",
            "\n",
            "Query 3: I'd like to order 7 Widget As, please\n",
            "  Anthropic Resp: Great! I've created your order for 7 Widget As. Here are the details:\n",
            "- Order ID: O4\n",
            "- Quantity: 7\n",
            "-...\n",
            "  OpenAI Resp: It looks like there was an issue placing your order for 7 Widget As. I apologize for the inconvenien...\n",
            "  Scores - Anthropic: 1, OpenAI: 0\n",
            "  Query Winner: Anthropic\n",
            "\n",
            "----- Overall Performance -----\n",
            "Avg Anthropic: 3.33, Avg OpenAI: 5.00\n",
            "Total Anthropic: 10, Total OpenAI: 15\n",
            "Overall Winner: OpenAI\n",
            "\n",
            "Learnings are stored as timestamped JSON files in your Google Drive at: /content/drive/My Drive/AI/Knowledgebases\n",
            "The latest file in that directory represents the current active set of learnings.\n",
            "Most recent learnings file: learnings_20250517_143730_631163.json\n",
            "\n",
            "Execution Finished.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}