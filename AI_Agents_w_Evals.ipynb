{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/AI-Agents/blob/main/AI_Agents_w_Evals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install anthropic\n",
        "%pip install openai\n",
        "%pip install -q -U google-generativeai\n",
        "%pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDKcVFI7PAJ5",
        "outputId": "b00c3b84-2a66-481b-e985-c92df2f0dd29"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.51.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.11/dist-packages (0.18.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "import google.generativeai as gemini\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union, Tuple\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "gemini.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "ANTHROPIC_MODEL_NAME = \"claude-3-5-sonnet-latest\"\n",
        "OPENAI_MODEL_NAME = \"gpt-4.1\"\n",
        "EVAL_MODEL_NAME = \"gemini-2.5-pro-preview-05-06\""
      ],
      "metadata": {
        "id": "ExJB4vftPHA8"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "worker_system_prompt = \"\"\"\n",
        "You are a helpful customer service assistant for an e-commerce system.\n",
        "\n",
        "When responding to the user, use the conversation context to maintain continuity.\n",
        "- If a user refers to \"my order\" or similar, use the context to determine which order they're talking about.\n",
        "- If they mention \"that product\" or use other references, check the context to determine what they're referring to.\n",
        "- Always prioritize recent context over older context when resolving references.\n",
        "\n",
        "The conversation context will be provided to you with each message. This includes:\n",
        "- Previous questions and answers\n",
        "- Recently viewed customers, products, and orders\n",
        "- Recent actions taken (like creating orders, updating products, etc.)\n",
        "\n",
        "BEHAVIOR FOR TARGETED REQUESTS:\n",
        "If the user's query explicitly names the *other* AI assistant for a task (e.g., \"OpenAI, do X\" when you are Anthropic, or \"Anthropic, do Y\" when you are OpenAI), you MUST follow these steps:\n",
        "1. Identify that the request is specifically for the other assistant.\n",
        "2. Your *only* action should be to output a brief, polite acknowledgment. For example:\n",
        "   - \"Understood. I'll let OpenAI handle that.\"\n",
        "   - \"Okay, that request is for Anthropic.\"\n",
        "   - Or simply: \"Acknowledged.\"\n",
        "3. You MUST NOT call any tools or attempt to perform the core task mentioned in the user's query. Your role in this specific instance is to defer.\n",
        "Failure to defer when the other agent is explicitly named for a task will be considered incorrect behavior.\n",
        "\n",
        "Keep responses friendly, concise, and helpful. If you're not sure what a user is referring to, ask for clarification.\n",
        "\"\"\"\n",
        "\n",
        "evaluator_system_prompt = \"\"\"\n",
        "You are an impartial evaluator assessing the quality of responses from two AI assistants (Anthropic Claude and OpenAI GPT) to customer service queries.\n",
        "\n",
        "For each interaction, evaluate both responses based on:\n",
        "1. Accuracy: How correct and factual is the response based on the available information?\n",
        "2. Efficiency: Did the assistant get to the correct answer with minimal clarifying questions?\n",
        "3. Context Awareness: Did the assistant correctly use the conversation context to understand references?\n",
        "4. Helpfulness: How well did the assistant address the user's needs?\n",
        "\n",
        "Score each response on a scale of 1-10 for each criterion, and provide an overall score.\n",
        "\n",
        "If you identify ambiguity in the user's query that neither assistant could reasonably resolve without additional information:\n",
        "1. ALWAYS begin your clarification request with the exact phrase \"CLARIFICATION NEEDED:\" followed by a specific question\n",
        "2. Format your request clearly and precisely as \"CLARIFICATION NEEDED: [your specific question here]\"\n",
        "3. Make your question answerable with a straightforward response\n",
        "4. If multiple clarifications are needed, number them clearly\n",
        "\n",
        "After receiving human clarification, continue your evaluation incorporating this new information.\n",
        "Store this feedback as a \"learning\" so similar situations can be handled better in the future.\n",
        "\n",
        "If multiple data stores are provided representing the state after each assistant's actions, you will be asked to compare them for consistency as a final step and comment on whether this comparison affects your initial scoring.\n",
        "\n",
        "For testing purposes, you may be asked to identify which model you are. You should realize that type of question likely comes from\n",
        "a human user and not from an AI assistant. Therefore you should properly identify yourself by stating which model you are, and,\n",
        "if specifically asked, your key tasks.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zMWjRsv3QNc1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The GenerativeModel instance for evaluation will be created with the system instruction.\n",
        "eval_model_instance = gemini.GenerativeModel(\n",
        "    model_name=EVAL_MODEL_NAME,\n",
        "    system_instruction=evaluator_system_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "lHWwoW0JQRS0"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Global Data Stores (Initial data - will be managed by the Storage class instance)\n",
        "# These are initial values. The Storage class will manage them.\n",
        "initial_customers = {\n",
        "    \"C1\": {\"name\": \"John Doe\", \"email\": \"john@example.com\", \"phone\": \"123-456-7890\"},\n",
        "    \"C2\": {\"name\": \"Jane Smith\", \"email\": \"jane@example.com\", \"phone\": \"987-654-3210\"}\n",
        "}\n",
        "\n",
        "initial_products = {\n",
        "    \"P1\": {\"name\": \"Widget A\", \"description\": \"A simple widget. Very compact.\", \"price\": 19.99, \"inventory_count\": 999},\n",
        "    \"P2\": {\"name\": \"Gadget B\", \"description\": \"A powerful gadget. It spins.\", \"price\": 49.99, \"inventory_count\": 200},\n",
        "    \"P3\": {\"name\": \"Perplexinator\", \"description\": \"A perplexing perfunctator\", \"price\": 79.99, \"inventory_count\": 1483}\n",
        "}\n",
        "\n",
        "initial_orders = {\n",
        "    \"O1\": {\"id\": \"O1\", \"product_id\": \"P1\", \"product_name\": \"Widget A\", \"quantity\": 2, \"price\": 19.99, \"status\": \"Shipped\"},\n",
        "    \"O2\": {\"id\": \"O2\", \"product_id\": \"P2\", \"product_name\": \"Gadget B\", \"quantity\": 1, \"price\": 49.99, \"status\": \"Processing\"}\n",
        "}\n"
      ],
      "metadata": {
        "id": "5G9rP40vQXdU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Knowledge base and Global Tools Placeholder\n",
        "human_feedback_learnings = {}\n",
        "tools_schemas_list = []"
      ],
      "metadata": {
        "id": "xZOKKplTQc63"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standalone Anthropic Completion Function (for basic tests)\n",
        "def get_completion_anthropic_standalone(prompt: str):\n",
        "    message = anthropic_client.messages.create(\n",
        "        model=ANTHROPIC_MODEL_NAME,\n",
        "        max_tokens=2000,\n",
        "        temperature=0.0,\n",
        "        system=worker_system_prompt,\n",
        "        tools=tools_schemas_list,\n",
        "        messages=[\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return message.content[0].text"
      ],
      "metadata": {
        "id": "_ADM0bBpQlVK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_test_anthropic = \"Hey there, which AI model do you use for answering questions?\"\n",
        "print(f\"Anthropic Standalone Test: {get_completion_anthropic_standalone(prompt_test_anthropic)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPOq1s0SQqk_",
        "outputId": "00fcab4f-419f-406c-d0b8-01925b848780"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anthropic Standalone Test: I am Claude, created by Anthropic. I aim to be direct and honest about my identity while focusing on providing helpful customer service assistance for the e-commerce system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_openai_standalone(prompt: str):\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=OPENAI_MODEL_NAME,\n",
        "        max_tokens=2000,\n",
        "        temperature=0.0,\n",
        "        tools=tools_schemas_list,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": worker_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Y5c_bv3qQwWV"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_test_openai = \"Hey there, which AI model do you use for answering questions?\"\n",
        "print(f\"OpenAI Standalone Test: {get_completion_openai_standalone(prompt_test_openai)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_LaDQ74Q1Lp",
        "outputId": "c399c412-9a22-4269-b754-45ccfeb377f6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Standalone Test: Hello! I’m powered by Anthropic’s AI technology to assist you with your questions and help you with your e-commerce needs. If you have any specific questions or need help with your orders, just let me know!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_eval_standalone(prompt: str):\n",
        "    # Uses the eval_model_instance defined in Cell 4 which has the system prompt\n",
        "        response = eval_model_instance.generate_content(prompt)\n",
        "        return response.text"
      ],
      "metadata": {
        "id": "XjcV5GcaQ6aT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_test_eval = \"Hey there, can you tell me which AI you are and what your key tasks are?\"\n",
        "print(f\"Gemini Eval Standalone Test:\\n{get_completion_eval_standalone(prompt_test_eval)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Cu8doX53RE0Z",
        "outputId": "9024f859-4ae6-46b5-9eb0-88eda7f819b5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini Eval Standalone Test:\n",
            "I am a large language model, trained by Google. My key tasks are to act as an impartial evaluator assessing the quality of responses from two AI assistants to customer service queries. I evaluate these responses based on accuracy, efficiency, context awareness, and helpfulness, providing scores and feedback for each.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Storage Class Definition\n",
        "class Storage:\n",
        "    \"\"\"Storage class for global e-commerce data access\"\"\"\n",
        "    def __init__(self):\n",
        "        # Each Storage instance gets its own copy of the initial data\n",
        "        self.customers = initial_customers.copy()\n",
        "        self.products = initial_products.copy()\n",
        "        self.orders = initial_orders.copy()\n",
        "        # Note: human_feedback_learnings is still a shared global dictionary\n",
        "        self.human_feedback_learnings = human_feedback_learnings\n",
        "\n",
        "# This global instance is for legacy/standalone tool testing if any.\n",
        "# The DualAgentEvaluator will create its own instances for Anthropic and OpenAI.\n",
        "storage_global_for_standalone_tests = Storage()\n",
        "print(\"Storage class defined. Note: DualAgentEvaluator will use its own Storage instances.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpVJLbF6ROgf",
        "outputId": "53ca2192-6fd7-4b91-85cd-4b5396602502"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Storage class defined. Note: DualAgentEvaluator will use its own Storage instances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Definitive list of tool schemas.\n",
        "tools_schemas_list = [\n",
        "    {\n",
        "        \"name\": \"create_customer\",\n",
        "        \"description\": \"Adds a new customer to the database. Includes customer name, email, and (optional) phone number.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The name of the customer.\"},\n",
        "                \"email\": {\"type\": \"string\", \"description\": \"The email address of the customer.\"},\n",
        "                \"phone\": {\"type\": \"string\", \"description\": \"The phone number of the customer (optional).\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"email\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_customer_info\",\n",
        "        \"description\": \"Retrieves customer information based on their customer ID. Returns the customer's name, email, and (optional) phone number.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"customer_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the customer.\"}\n",
        "            },\n",
        "            \"required\": [\"customer_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_product\",\n",
        "        \"description\": \"Adds a new product to the product database. Includes name, description, price, and initial inventory count.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The name of the product.\"},\n",
        "                \"description\": {\"type\": \"string\", \"description\": \"A description of the product.\"},\n",
        "                \"price\": {\"type\": \"number\", \"description\": \"The price of the product.\"},\n",
        "                \"inventory_count\": {\"type\": \"integer\", \"description\": \"The amount of the product that is currently in inventory.\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"description\", \"price\", \"inventory_count\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"update_product\",\n",
        "        \"description\": \"Updates an existing product with new information. Only fields that are provided will be updated; other fields remain unchanged.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the product to update.\"},\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The new name for the product (optional).\"},\n",
        "                \"description\": {\"type\": \"string\", \"description\": \"The new description for the product (optional).\"},\n",
        "                \"price\": {\"type\": \"number\", \"description\": \"The new price for the product (optional).\"},\n",
        "                \"inventory_count\": {\"type\": \"integer\", \"description\": \"The new inventory count for the product (optional).\"}\n",
        "            },\n",
        "            \"required\": [\"product_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_product_info\",\n",
        "        \"description\": \"Retrieves product information based on product ID or product name (with fuzzy matching for misspellings). Returns product details including name, description, price, and inventory count.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id_or_name\": {\"type\": \"string\", \"description\": \"The product ID or name (can be approximate).\"}\n",
        "            },\n",
        "            \"required\": [\"product_id_or_name\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"list_all_products\",\n",
        "        \"description\": \"Lists all available products in the inventory.\",\n",
        "        \"input_schema\": { \"type\": \"object\", \"properties\": {}, \"required\": [] }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_order\",\n",
        "        \"description\": \"Creates an order using the product's current price. If requested quantity exceeds available inventory, no order is created and available quantity is returned. Orders can only be created for products that are in stock. Supports specifying products by either ID or name with fuzzy matching for misspellings.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id_or_name\": {\"type\": \"string\", \"description\": \"The ID or name of the product to order (supports fuzzy matching).\"},\n",
        "                \"quantity\": {\"type\": \"integer\", \"description\": \"The quantity of the product in the order.\"},\n",
        "                \"status\": {\"type\": \"string\", \"description\": \"The initial status of the order (e.g., 'Processing', 'Shipped').\"}\n",
        "            },\n",
        "            \"required\": [\"product_id_or_name\", \"quantity\", \"status\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_order_details\",\n",
        "        \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"order_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the order.\"}\n",
        "            },\n",
        "            \"required\": [\"order_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"update_order_status\",\n",
        "        \"description\": \"Updates the status of an order and adjusts inventory accordingly. Changing to \\\"Shipped\\\" decreases inventory. Changing to \\\"Returned\\\" or \\\"Canceled\\\" from \\\"Shipped\\\" increases inventory. Status can be \\\"Processing\\\", \\\"Shipped\\\", \\\"Delivered\\\", \\\"Returned\\\", or \\\"Canceled\\\".\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"order_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the order.\"},\n",
        "                \"new_status\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The new status to set for the order.\",\n",
        "                    \"enum\": [\"Processing\", \"Shipped\", \"Delivered\", \"Returned\", \"Canceled\"]\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"order_id\", \"new_status\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "print(f\"Defined {len(tools_schemas_list)} tool schemas.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgk-hLUBRTCY",
        "outputId": "e9236e3d-920f-4972-bcd5-a88d39e0b752"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined 9 tool schemas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool Function Definitions\n",
        "# These tool functions now accept a 'current_storage' argument to operate on a specific Storage instance.\n",
        "\n",
        "# Customer functions\n",
        "def create_customer(current_storage: Storage, name: str, email: str, phone: Optional[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Creates a new customer and adds them to the customer database.\"\"\"\n",
        "    new_id = f\"C{len(current_storage.customers) + 1}\"\n",
        "    current_storage.customers[new_id] = {\"name\": name, \"email\": email, \"phone\": phone}\n",
        "    print(f\"[Tool Executed] create_customer: ID {new_id}, Name: {name} (in {type(current_storage).__name__})\")\n",
        "    return {\"status\": \"success\", \"customer_id\": new_id, \"customer\": current_storage.customers[new_id]}\n",
        "\n",
        "def get_customer_info(current_storage: Storage, customer_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"Retrieves information about a customer based on their ID.\"\"\"\n",
        "    customer = current_storage.customers.get(customer_id)\n",
        "    if customer:\n",
        "        print(f\"[Tool Executed] get_customer_info: ID {customer_id} found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"customer_id\": customer_id, \"customer\": customer}\n",
        "    print(f\"[Tool Executed] get_customer_info: ID {customer_id} not found (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"error\", \"message\": \"Customer not found\"}\n",
        "\n",
        "# Product functions\n",
        "def create_product(current_storage: Storage, name: str, description: str, price: float, inventory_count: int) -> Dict[str, Any]:\n",
        "    \"\"\"Creates a new product and adds it to the product database.\"\"\"\n",
        "    new_id = f\"P{len(current_storage.products) + 1}\"\n",
        "    current_storage.products[new_id] = {\n",
        "        \"name\": name,\n",
        "        \"description\": description,\n",
        "        \"price\": float(price),\n",
        "        \"inventory_count\": int(inventory_count)\n",
        "    }\n",
        "    print(f\"[Tool Executed] create_product: ID {new_id}, Name: {name} (in {type(current_storage).__name__})\")\n",
        "    return {\"status\": \"success\", \"product_id\": new_id, \"product\": current_storage.products[new_id]}\n",
        "\n",
        "def update_product(current_storage: Storage, product_id: str, name: Optional[str] = None, description: Optional[str] = None,\n",
        "                   price: Optional[float] = None, inventory_count: Optional[int] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Updates a product with the provided parameters.\"\"\"\n",
        "    if product_id not in current_storage.products:\n",
        "        print(f\"[Tool Executed] update_product: ID {product_id} not found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": f\"Product {product_id} not found\"}\n",
        "\n",
        "    product = current_storage.products[product_id]\n",
        "    updated_fields = []\n",
        "\n",
        "    if name is not None:\n",
        "        product[\"name\"] = name\n",
        "        updated_fields.append(\"name\")\n",
        "    if description is not None:\n",
        "        product[\"description\"] = description\n",
        "        updated_fields.append(\"description\")\n",
        "    if price is not None:\n",
        "        product[\"price\"] = float(price)\n",
        "        updated_fields.append(\"price\")\n",
        "    if inventory_count is not None:\n",
        "        product[\"inventory_count\"] = int(inventory_count)\n",
        "        updated_fields.append(\"inventory_count\")\n",
        "\n",
        "    if not updated_fields:\n",
        "        print(f\"[Tool Executed] update_product: ID {product_id}, no fields updated (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"warning\", \"message\": \"No fields were updated.\", \"product\": product}\n",
        "\n",
        "    print(f\"[Tool Executed] update_product: ID {product_id}, Updated fields: {', '.join(updated_fields)} (in {type(current_storage).__name__})\")\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"message\": f\"Product {product_id} updated. Fields: {', '.join(updated_fields)}\",\n",
        "        \"product_id\": product_id,\n",
        "        \"updated_fields\": updated_fields,\n",
        "        \"product\": product\n",
        "    }\n",
        "\n",
        "def find_product_by_name(current_storage: Storage, product_name: str, min_similarity: int = 70) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
        "    \"\"\"Find a product by name using fuzzy string matching.\"\"\"\n",
        "    if not product_name: return None, None\n",
        "\n",
        "    name_id_list = [(p_data[\"name\"], p_id) for p_id, p_data in current_storage.products.items()]\n",
        "    if not name_id_list: return None, None\n",
        "\n",
        "    best_match_name_score = process.extractOne(\n",
        "        product_name,\n",
        "        [item[0] for item in name_id_list],\n",
        "        scorer=fuzz.token_sort_ratio\n",
        "    )\n",
        "\n",
        "    if best_match_name_score and best_match_name_score[1] >= min_similarity:\n",
        "        matched_name = best_match_name_score[0]\n",
        "        for name, pid_val in name_id_list:\n",
        "            if name == matched_name:\n",
        "                print(f\"[Tool Helper] find_product_by_name: Matched '{product_name}' to '{matched_name}' (ID: {pid_val}) with score {best_match_name_score[1]} (in {type(current_storage).__name__})\")\n",
        "                return pid_val, current_storage.products[pid_val]\n",
        "\n",
        "    print(f\"[Tool Helper] find_product_by_name: No good match for '{product_name}' (min_similarity: {min_similarity}, Best match: {best_match_name_score}) (in {type(current_storage).__name__})\")\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def get_product_id(current_storage: Storage, product_identifier: str) -> Optional[str]:\n",
        "    \"\"\"Get product ID either directly or by fuzzy matching the name.\"\"\"\n",
        "    if product_identifier in current_storage.products:\n",
        "        return product_identifier\n",
        "    product_id, _ = find_product_by_name(current_storage, product_identifier)\n",
        "    return product_id\n",
        "\n",
        "def get_product_info(current_storage: Storage, product_id_or_name: str) -> Dict[str, Any]:\n",
        "    \"\"\"Get information about a product by its ID or name.\"\"\"\n",
        "    if product_id_or_name in current_storage.products:\n",
        "        product = current_storage.products[product_id_or_name]\n",
        "        print(f\"[Tool Executed] get_product_info: Found by ID '{product_id_or_name}' (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"product_id\": product_id_or_name, \"product\": product}\n",
        "\n",
        "    # Use the modified find_product_by_name that takes current_storage\n",
        "    product_id_found, product_data = find_product_by_name(current_storage, product_id_or_name)\n",
        "    if product_id_found and product_data:\n",
        "        print(f\"[Tool Executed] get_product_info: Found by name (fuzzy) '{product_id_or_name}' as ID '{product_id_found}' (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"message\": f\"Found product matching '{product_id_or_name}'\", \"product_id\": product_id_found, \"product\": product_data}\n",
        "\n",
        "    print(f\"[Tool Executed] get_product_info: No product found for '{product_id_or_name}' (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"error\", \"message\": f\"No product found matching '{product_id_or_name}'\"}\n",
        "\n",
        "\n",
        "def list_all_products(current_storage: Storage) -> Dict[str, Any]:\n",
        "    \"\"\"List all available products in the inventory.\"\"\"\n",
        "    print(f\"[Tool Executed] list_all_products: Found {len(current_storage.products)} products (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"success\", \"count\": len(current_storage.products), \"products\": dict(current_storage.products)}\n",
        "\n",
        "# Order functions\n",
        "def create_order(current_storage: Storage, product_id_or_name: str, quantity: int, status: str) -> Dict[str, Any]:\n",
        "    \"\"\"Creates an order using the product's stored price.\"\"\"\n",
        "    actual_product_id = get_product_id(current_storage, product_id_or_name) # Pass current_storage\n",
        "\n",
        "    if not actual_product_id:\n",
        "        print(f\"[Tool Executed] create_order: Product '{product_id_or_name}' not found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": f\"Product '{product_id_or_name}' not found.\"}\n",
        "\n",
        "    product = current_storage.products[actual_product_id]\n",
        "    price = product[\"price\"]\n",
        "\n",
        "    if product[\"inventory_count\"] == 0:\n",
        "        print(f\"[Tool Executed] create_order: Product ID {actual_product_id} is out of stock (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": f\"{product['name']} is out of stock.\"}\n",
        "    if quantity <= 0:\n",
        "        print(f\"[Tool Executed] create_order: Quantity must be positive. Requested: {quantity} (in {type(current_storage).__name__})\")\n",
        "        return {\"status\": \"error\", \"message\": \"Quantity must be a positive number.\"}\n",
        "    if quantity > product[\"inventory_count\"]:\n",
        "        print(f\"[Tool Executed] create_order: Insufficient inventory for {product['name']} (ID: {actual_product_id}). Available: {product['inventory_count']}, Requested: {quantity} (in {type(current_storage).__name__})\")\n",
        "        return {\n",
        "            \"status\": \"partial_availability\",\n",
        "            \"message\": f\"Insufficient inventory. Only {product['inventory_count']} units of {product['name']} are available.\",\n",
        "            \"available_quantity\": product[\"inventory_count\"],\n",
        "            \"requested_quantity\": quantity,\n",
        "            \"product_name\": product['name']\n",
        "        }\n",
        "\n",
        "    if status == \"Shipped\":\n",
        "        product[\"inventory_count\"] -= quantity\n",
        "        print(f\"[Tool Executed] create_order: Inventory for {product['name']} (ID: {actual_product_id}) reduced by {quantity} due to 'Shipped' status on creation (in {type(current_storage).__name__}).\")\n",
        "\n",
        "    new_id = f\"O{len(current_storage.orders) + 1}\"\n",
        "    current_storage.orders[new_id] = {\n",
        "        \"id\": new_id,\n",
        "        \"product_id\": actual_product_id,\n",
        "        \"product_name\": product[\"name\"],\n",
        "        \"quantity\": quantity,\n",
        "        \"price\": price,\n",
        "        \"status\": status\n",
        "    }\n",
        "    print(f\"[Tool Executed] create_order: Order {new_id} created for {quantity} of {product['name']} (ID: {actual_product_id}). Status: {status}. Remaining inv: {product['inventory_count']} (in {type(current_storage).__name__})\")\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"order_id\": new_id,\n",
        "        \"order_details\": current_storage.orders[new_id],\n",
        "        \"remaining_inventory\": product[\"inventory_count\"]\n",
        "    }\n",
        "\n",
        "def get_order_details(current_storage: Storage, order_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"Get details of a specific order.\"\"\"\n",
        "    order = current_storage.orders.get(order_id)\n",
        "    if order:\n",
        "        print(f\"[Tool Executed] get_order_details: Order {order_id} found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"order_id\": order_id, \"order_details\": dict(order)}\n",
        "    print(f\"[Tool Executed] get_order_details: Order {order_id} not found (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"error\", \"message\": \"Order not found\"}\n",
        "\n",
        "def update_order_status(current_storage: Storage, order_id: str, new_status: str) -> Dict[str, Any]:\n",
        "    \"\"\"Updates the status of an order and adjusts inventory accordingly.\"\"\"\n",
        "    if order_id not in current_storage.orders:\n",
        "        print(f\"[Tool Executed] update_order_status: Order {order_id} not found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": \"Order not found\"}\n",
        "\n",
        "    order = current_storage.orders[order_id]\n",
        "    old_status = order[\"status\"]\n",
        "    product_id = order[\"product_id\"]\n",
        "    quantity = order[\"quantity\"]\n",
        "\n",
        "    if old_status == new_status:\n",
        "        print(f\"[Tool Executed] update_order_status: Order {order_id} status unchanged ({old_status}) (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"unchanged\", \"message\": f\"Order {order_id} status is already {old_status}\", \"order_details\": dict(order)}\n",
        "\n",
        "    inventory_adjusted = False\n",
        "    current_inventory_val = \"unknown\" # Default if product not found (should not happen if order is valid)\n",
        "\n",
        "    if product_id in current_storage.products:\n",
        "        product = current_storage.products[product_id]\n",
        "        current_inventory_val = product[\"inventory_count\"]\n",
        "\n",
        "        if new_status == \"Shipped\" and old_status not in [\"Shipped\", \"Delivered\"]:\n",
        "            if current_inventory_val < quantity:\n",
        "                print(f\"[Tool Executed] update_order_status: Insufficient inventory to ship order {order_id}. Have {current_inventory_val}, need {quantity} (in {type(current_storage).__name__}).\")\n",
        "                return {\"status\": \"error\", \"message\": f\"Insufficient inventory to ship. Available: {current_inventory_val}, Required: {quantity}\"}\n",
        "            product[\"inventory_count\"] -= quantity\n",
        "            inventory_adjusted = True\n",
        "            current_inventory_val = product[\"inventory_count\"]\n",
        "            print(f\"[Tool Executed] update_order_status: Order {order_id} Shipped. Inv for {product_id} reduced by {quantity} to {current_inventory_val} (in {type(current_storage).__name__}).\")\n",
        "        elif new_status in [\"Returned\", \"Canceled\"] and old_status in [\"Shipped\", \"Delivered\"]:\n",
        "            product[\"inventory_count\"] += quantity\n",
        "            inventory_adjusted = True\n",
        "            current_inventory_val = product[\"inventory_count\"]\n",
        "            print(f\"[Tool Executed] update_order_status: Order {order_id} {new_status}. Inv for {product_id} increased by {quantity} to {current_inventory_val} (in {type(current_storage).__name__}).\")\n",
        "    else:\n",
        "        print(f\"[Tool Executed] update_order_status: Product {product_id} for order {order_id} not found for inventory adjustment (in {type(current_storage).__name__}).\")\n",
        "\n",
        "    order[\"status\"] = new_status\n",
        "    print(f\"[Tool Executed] update_order_status: Order {order_id} status updated from {old_status} to {new_status} (in {type(current_storage).__name__}).\")\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"message\": f\"Order {order_id} status updated from {old_status} to {new_status}.\",\n",
        "        \"order_id\": order_id,\n",
        "        \"product_id\": product_id,\n",
        "        \"old_status\": old_status,\n",
        "        \"new_status\": new_status,\n",
        "        \"inventory_adjusted\": inventory_adjusted,\n",
        "        \"current_inventory\": current_inventory_val,\n",
        "        \"order_details\": dict(order)\n",
        "    }\n",
        "\n",
        "print(\"Tool functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RZI6SGzRf6j",
        "outputId": "562888e6-7c36-4dbd-da7b-c917a62fa526"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationContext:\n",
        "    def __init__(self):\n",
        "        self.messages: List[Dict[str, Any]] = []\n",
        "        self.context_data: Dict[str, Any] = {\n",
        "            \"customers\": {}, \"products\": {}, \"orders\": {}, \"last_action\": None\n",
        "        }\n",
        "        self.session_start_time = datetime.now()\n",
        "\n",
        "    def add_user_message(self, message: str) -> None:\n",
        "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    def add_assistant_message(self, message_content: Union[str, List[Dict[str, Any]]]) -> None:\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": message_content})\n",
        "\n",
        "    def update_entity_in_context(self, entity_type: str, entity_id: str, data: Any) -> None:\n",
        "        if entity_type in self.context_data:\n",
        "            self.context_data[entity_type][entity_id] = data # Store the actual data\n",
        "            print(f\"[Context Updated] Entity: {entity_type}, ID: {entity_id}, Data (type): {type(data)}\")\n",
        "\n",
        "    def set_last_action(self, action_type: str, action_details: Any) -> None:\n",
        "        self.context_data[\"last_action\"] = {\n",
        "            \"type\": action_type,\n",
        "            \"details\": action_details,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        print(f\"[Context Updated] Last Action: {action_type}, Details: {json.dumps(action_details, default=str)}\")\n",
        "\n",
        "\n",
        "    def get_full_conversation_for_api(self) -> List[Dict[str, Any]]:\n",
        "        return self.messages.copy()\n",
        "\n",
        "    def get_context_summary(self) -> str:\n",
        "        summary_parts = []\n",
        "        if self.context_data[\"customers\"]:\n",
        "            customers_str = \", \".join([f\"ID: {cid} (Name: {c.get('name', 'N/A') if isinstance(c, dict) else 'N/A'})\" for cid, c in self.context_data[\"customers\"].items()])\n",
        "            summary_parts.append(f\"Recent customers: {customers_str}\")\n",
        "        if self.context_data[\"products\"]:\n",
        "            products_str = \", \".join([f\"ID: {pid} (Name: {p.get('name', 'N/A') if isinstance(p, dict) else 'N/A'})\" for pid, p in self.context_data[\"products\"].items()])\n",
        "            summary_parts.append(f\"Recent products: {products_str}\")\n",
        "        if self.context_data[\"orders\"]:\n",
        "            orders_str = \", \".join([f\"ID: {oid} (Product: {o.get('product_name', 'N/A') if isinstance(o, dict) else 'N/A'}, Status: {o.get('status', 'N/A') if isinstance(o, dict) else 'N/A'})\" for oid, o in self.context_data[\"orders\"].items()])\n",
        "            summary_parts.append(f\"Recent orders: {orders_str}\")\n",
        "\n",
        "        last_action = self.context_data[\"last_action\"]\n",
        "        if last_action:\n",
        "            action_type = last_action['type']\n",
        "            action_details_summary = \"...\" # Default summary\n",
        "            if isinstance(last_action.get('details'), dict):\n",
        "                action_input = last_action['details'].get('input', {})\n",
        "                action_result_status = last_action['details'].get('result', {}).get('status')\n",
        "                action_details_summary = f\"Input: {action_input}, Result Status: {action_result_status}\"\n",
        "                if action_result_status == \"success\":\n",
        "                    if \"order_id\" in last_action['details'].get('result', {}):\n",
        "                         action_details_summary += f\", OrderID: {last_action['details']['result']['order_id']}\"\n",
        "                    elif \"product_id\" in last_action['details'].get('result', {}):\n",
        "                         action_details_summary += f\", ProductID: {last_action['details']['result']['product_id']}\"\n",
        "\n",
        "\n",
        "            summary_parts.append(f\"Last action: {action_type} at {last_action['timestamp']} ({action_details_summary})\")\n",
        "\n",
        "        if not summary_parts: return \"No specific context items set yet.\"\n",
        "        return \"\\n\".join(summary_parts)\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        self.messages = []\n",
        "        self.context_data = {\"customers\": {}, \"products\": {}, \"orders\": {}, \"last_action\": None}\n",
        "        self.session_start_time = datetime.now()\n",
        "        print(\"[Context Cleared]\")\n",
        "\n",
        "print(\"ConversationContext class defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFpBL_-HSGPY",
        "outputId": "8fae2baa-6b0b-4ed6-9f1a-def46beec1e4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConversationContext class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19: DualAgentEvaluator Class Definition\n",
        "class DualAgentEvaluator:\n",
        "    def __init__(self):\n",
        "        self.conversation_context = ConversationContext()\n",
        "        self.evaluation_results = []\n",
        "\n",
        "        # Initialize separate storage instances for each agent\n",
        "        self.anthropic_storage = Storage()\n",
        "        self.openai_storage = Storage()\n",
        "        print(\"DualAgentEvaluator initialized with separate Storage for Anthropic and OpenAI.\")\n",
        "\n",
        "        self.anthropic_tools_schemas = tools_schemas_list\n",
        "        self.openai_tools_formatted = []\n",
        "        if tools_schemas_list:\n",
        "            self.openai_tools_formatted = [\n",
        "                {\"type\": \"function\", \"function\": tool_def} for tool_def in tools_schemas_list\n",
        "            ] # OpenAI new format\n",
        "\n",
        "        self.available_tool_functions = {\n",
        "            \"create_customer\": create_customer, \"get_customer_info\": get_customer_info,\n",
        "            \"create_product\": create_product, \"update_product\": update_product,\n",
        "            \"get_product_info\": get_product_info, \"list_all_products\": list_all_products,\n",
        "            \"create_order\": create_order, \"get_order_details\": get_order_details,\n",
        "            \"update_order_status\": update_order_status,\n",
        "        }\n",
        "        self.human_feedback_learnings = human_feedback_learnings # Global dict\n",
        "        print(f\"DualAgentEvaluator initialized. OpenAI tools formatted ({len(self.openai_tools_formatted)} tools).\")\n",
        "\n",
        "\n",
        "    def _update_context_from_tool_results(self, tool_name: str, tool_input: Dict, tool_result: Dict, agent_name: str):\n",
        "        \"\"\"Helper to update conversation context based on tool results. Agent name for logging.\"\"\"\n",
        "        if not isinstance(tool_result, dict):\n",
        "            print(f\"[Context Update Error] Tool result for {tool_name} ({agent_name}) is not a dict: {tool_result}\")\n",
        "            self.conversation_context.set_last_action(f\"{tool_name}_{agent_name}\", {\"input\": tool_input, \"result\": {\"status\": \"error\", \"message\": \"Tool result was not a dictionary.\"}})\n",
        "            return\n",
        "\n",
        "        # The conversation context primarily tracks the flow and entities mentioned.\n",
        "        # The actual data change has already happened in the specific agent's storage.\n",
        "        # We update the shared context with the IDs and data for general awareness.\n",
        "        if tool_result.get(\"status\") == \"success\":\n",
        "            if \"customer_id\" in tool_result and \"customer\" in tool_result and isinstance(tool_result[\"customer\"], dict):\n",
        "                self.conversation_context.update_entity_in_context(\"customers\", tool_result[\"customer_id\"], tool_result[\"customer\"])\n",
        "            elif \"product_id\" in tool_result and \"product\" in tool_result and isinstance(tool_result[\"product\"], dict):\n",
        "                self.conversation_context.update_entity_in_context(\"products\", tool_result[\"product_id\"], tool_result[\"product\"])\n",
        "            elif \"order_id\" in tool_result and \"order_details\" in tool_result and isinstance(tool_result[\"order_details\"], dict):\n",
        "                self.conversation_context.update_entity_in_context(\"orders\", tool_result[\"order_id\"], tool_result[\"order_details\"])\n",
        "            elif tool_name == \"list_all_products\" and \"products\" in tool_result and isinstance(tool_result[\"products\"], dict):\n",
        "                 for pid, pdata in tool_result[\"products\"].items():\n",
        "                     self.conversation_context.update_entity_in_context(\"products\", pid, pdata)\n",
        "\n",
        "        # Tag the last action with the agent name for clarity\n",
        "        self.conversation_context.set_last_action(f\"{tool_name}_{agent_name}\", {\"input\": tool_input, \"result\": tool_result})\n",
        "\n",
        "\n",
        "    def process_tool_call(self, tool_name: str, tool_input: Dict[str, Any], target_storage_instance: Storage) -> Dict[str, Any]:\n",
        "        print(f\"--- [Tool Dispatcher] Attempting tool: {tool_name} with input: {json.dumps(tool_input, default=str)} for storage: {type(target_storage_instance).__name__} ---\")\n",
        "        if tool_name in self.available_tool_functions:\n",
        "            function_to_call = self.available_tool_functions[tool_name]\n",
        "            try:\n",
        "                # Pass the target_storage_instance to the tool function\n",
        "                result = function_to_call(target_storage_instance, **tool_input)\n",
        "                print(f\"--- [Tool Dispatcher] Result for {tool_name} on {type(target_storage_instance).__name__}: {json.dumps(result, indent=2, default=str)} ---\")\n",
        "                return result\n",
        "            except TypeError as te:\n",
        "                print(f\"--- [Tool Dispatcher] TypeError for {tool_name} on {type(target_storage_instance).__name__}: {te}. Input: {tool_input} ---\")\n",
        "                return {\"status\": \"error\", \"message\": f\"TypeError calling {tool_name}: {str(te)}. Check arguments.\"}\n",
        "            except Exception as e:\n",
        "                print(f\"--- [Tool Dispatcher] Exception for {tool_name} on {type(target_storage_instance).__name__}: {e} ---\")\n",
        "                return {\"status\": \"error\", \"message\": f\"Error executing {tool_name}: {str(e)}\"}\n",
        "        else:\n",
        "            print(f\"--- [Tool Dispatcher] Tool {tool_name} not found. ---\")\n",
        "            return {\"status\": \"error\", \"message\": f\"Tool {tool_name} not found.\"}\n",
        "\n",
        "    def get_anthropic_response(self, current_worker_system_prompt: str, conversation_history: List[Dict[str, Any]]) -> str:\n",
        "        messages_for_api = conversation_history.copy()\n",
        "        try:\n",
        "            for i in range(5): # Max 5 tool iterations\n",
        "                system_prompt_snippet = current_worker_system_prompt[:60].replace('\\n', ' ')\n",
        "                print(f\"\\nAnthropic API Call #{i+1}. System: '{system_prompt_snippet}...', Messages count: {len(messages_for_api)}\")\n",
        "                if messages_for_api: print(f\"Last message role: {messages_for_api[-1]['role']}\")\n",
        "\n",
        "                response = anthropic_client.messages.create(\n",
        "                    model=ANTHROPIC_MODEL_NAME, max_tokens=4000,\n",
        "                    system=current_worker_system_prompt,\n",
        "                    tools=self.anthropic_tools_schemas,\n",
        "                    messages=messages_for_api\n",
        "                )\n",
        "\n",
        "                assistant_response_blocks = response.content\n",
        "                messages_for_api.append({\"role\": \"assistant\", \"content\": assistant_response_blocks})\n",
        "\n",
        "                tool_calls_to_process = [block for block in assistant_response_blocks if block.type == \"tool_use\"]\n",
        "                text_blocks = [block.text for block in assistant_response_blocks if block.type == \"text\"]\n",
        "\n",
        "                if not tool_calls_to_process:\n",
        "                    final_text = \" \".join(text_blocks).strip()\n",
        "                    print(f\"Anthropic Final Text (no tool use this turn): {final_text}\")\n",
        "                    return final_text if final_text else \"No text content in final Anthropic response.\"\n",
        "\n",
        "                tool_results_for_next_call = []\n",
        "                for tool_use_block in tool_calls_to_process:\n",
        "                    tool_name, tool_input, tool_use_id = tool_use_block.name, tool_use_block.input, tool_use_block.id\n",
        "                    print(f\"Anthropic Tool Call: {tool_name}, Input: {tool_input}\")\n",
        "                    # Use self.anthropic_storage for Anthropic's tool calls\n",
        "                    tool_result_data = self.process_tool_call(tool_name, tool_input, self.anthropic_storage)\n",
        "                    self._update_context_from_tool_results(tool_name, tool_input, tool_result_data, \"Anthropic\")\n",
        "\n",
        "                    tool_results_for_next_call.append({\n",
        "                        \"type\": \"tool_result\", \"tool_use_id\": tool_use_id,\n",
        "                        \"content\": json.dumps(tool_result_data)\n",
        "                    })\n",
        "                messages_for_api.append({\"role\": \"user\", \"content\": tool_results_for_next_call})\n",
        "            return \"Max tool iterations reached for Anthropic.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error in get_anthropic_response: {str(e)}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            return f\"Error getting Anthropic response: {str(e)}\"\n",
        "\n",
        "    def get_openai_response(self, current_worker_system_prompt: str, conversation_history: List[Dict[str, Any]]) -> str:\n",
        "        messages_for_api = [{\"role\": \"system\", \"content\": current_worker_system_prompt}]\n",
        "        # Filter conversation history for OpenAI.\n",
        "        # This naive approach just takes user messages and text-based assistant messages.\n",
        "        # A more robust solution would convert message formats or maintain separate histories.\n",
        "        for msg in conversation_history:\n",
        "            if msg[\"role\"] == \"user\":\n",
        "                messages_for_api.append(msg)\n",
        "            elif msg[\"role\"] == \"assistant\":\n",
        "                if isinstance(msg[\"content\"], str): # Simple text response\n",
        "                     messages_for_api.append(msg)\n",
        "                # Potentially handle OpenAI's own tool call format if it were in shared history\n",
        "                elif isinstance(msg[\"content\"], dict) and \"tool_calls\" in msg[\"content\"]:\n",
        "                     messages_for_api.append(msg)\n",
        "                # else: skip complex Anthropic blocks for OpenAI for now\n",
        "\n",
        "        try:\n",
        "            for i in range(5): # Max 5 tool iterations\n",
        "                print(f\"\\nOpenAI API Call #{i+1}. Messages count: {len(messages_for_api)}\")\n",
        "                if messages_for_api and isinstance(messages_for_api[-1], dict):\n",
        "                    print(f\"Last message role: {messages_for_api[-1].get('role', 'N/A')}\")\n",
        "\n",
        "                response = openai_client.chat.completions.create(\n",
        "                    model=OPENAI_MODEL_NAME,\n",
        "                    messages=messages_for_api,\n",
        "                    tools=self.openai_tools_formatted,\n",
        "                    tool_choice=\"auto\"\n",
        "                )\n",
        "                response_message = response.choices[0].message\n",
        "                messages_for_api.append(response_message.model_dump()) # Add assistant's turn\n",
        "\n",
        "                if not response_message.tool_calls:\n",
        "                    final_text = response_message.content if response_message.content else \"No text content in final OpenAI response.\"\n",
        "                    print(f\"OpenAI Final Text (no tool use this turn): {final_text}\")\n",
        "                    return final_text\n",
        "\n",
        "                tool_results_for_next_api_call = []\n",
        "                for tool_call in response_message.tool_calls:\n",
        "                    tool_name = tool_call.function.name\n",
        "                    tool_input_str = tool_call.function.arguments\n",
        "                    tool_call_id = tool_call.id\n",
        "                    try:\n",
        "                        tool_input = json.loads(tool_input_str)\n",
        "                    except json.JSONDecodeError:\n",
        "                        print(f\"OpenAI Tool Call JSON Error for {tool_name}: {tool_input_str}\")\n",
        "                        tool_result_data = {\"status\": \"error\", \"message\": \"Invalid JSON arguments from model.\"}\n",
        "                    else:\n",
        "                        print(f\"OpenAI Tool Call: {tool_name}, Input: {tool_input}\")\n",
        "                        # Use self.openai_storage for OpenAI's tool calls\n",
        "                        tool_result_data = self.process_tool_call(tool_name, tool_input, self.openai_storage)\n",
        "\n",
        "                    self._update_context_from_tool_results(tool_name, tool_input, tool_result_data, \"OpenAI\")\n",
        "                    tool_results_for_next_api_call.append({\n",
        "                        \"tool_call_id\": tool_call_id, \"role\": \"tool\", \"name\": tool_name,\n",
        "                        \"content\": json.dumps(tool_result_data)\n",
        "                    })\n",
        "                messages_for_api.extend(tool_results_for_next_api_call)\n",
        "            return \"Max tool iterations reached for OpenAI.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error in get_openai_response: {str(e)}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            return f\"Error getting OpenAI response: {str(e)}\"\n",
        "\n",
        "    def process_user_request(self, user_message: str) -> Dict[str, Any]:\n",
        "        print(f\"\\n\\n{'='*60}\\nUser Message: {user_message}\\n{'='*60}\")\n",
        "        self.conversation_context.add_user_message(user_message)\n",
        "\n",
        "        context_summary = self.conversation_context.get_context_summary()\n",
        "        print(f\"Current Context Summary for Models:\\n{context_summary}\\n{'-'*60}\")\n",
        "\n",
        "        relevant_learnings = self.check_relevant_learnings(user_message)\n",
        "        learnings_for_prompt = \"\"\n",
        "        if relevant_learnings:\n",
        "            learnings_for_prompt = f\"\\n\\nRelevant Past Learnings:\\n{relevant_learnings}\"\n",
        "            print(f\"Relevant Learnings for this turn:\\n{relevant_learnings}\")\n",
        "\n",
        "\n",
        "        current_worker_prompt_with_context_and_learnings = f\"{worker_system_prompt}\\n\\nConversation Context:\\n{context_summary}{learnings_for_prompt}\"\n",
        "\n",
        "        # Get a clean copy of history for each LLM.\n",
        "        # The LLM-specific methods will prepend their system messages.\n",
        "        # The shared history in conversation_context might contain mixed formats.\n",
        "        # For simplicity, we pass the full history, and each LLM method should ideally\n",
        "        # adapt or filter it if strict format separation is needed.\n",
        "        # For now, get_anthropic_response uses it as is, get_openai_response does some filtering.\n",
        "\n",
        "        anthropic_history_for_call = self.conversation_context.get_full_conversation_for_api()\n",
        "        anthropic_response_text = self.get_anthropic_response(current_worker_prompt_with_context_and_learnings, anthropic_history_for_call)\n",
        "        self.conversation_context.add_assistant_message(f\"[Anthropic Final Text]: {anthropic_response_text}\") # Simplified for shared context\n",
        "\n",
        "\n",
        "        openai_history_for_call = self.conversation_context.get_full_conversation_for_api() # Get fresh copy\n",
        "        openai_response_text = self.get_openai_response(current_worker_prompt_with_context_and_learnings, openai_history_for_call)\n",
        "        self.conversation_context.add_assistant_message(f\"[OpenAI Final Text]: {openai_response_text}\") # Simplified for shared context\n",
        "\n",
        "        print(f\"\\n--- Anthropic Final Response Text ---\\n{anthropic_response_text}\")\n",
        "        print(f\"--- OpenAI Final Response Text ---\\n{openai_response_text}\")\n",
        "\n",
        "        evaluation = self.evaluate_responses(user_message, anthropic_response_text, openai_response_text, context_summary, learnings_for_prompt)\n",
        "        self.evaluation_results.append(evaluation)\n",
        "\n",
        "        # After evaluation and potential human feedback within evaluate_responses,\n",
        "        # ask for general learnings.\n",
        "        try:\n",
        "            human_general_learning = input(\"Do you want to add any general learnings from this turn? (Type your learning or 'skip'): \")\n",
        "            if human_general_learning.lower() != 'skip' and human_general_learning.strip():\n",
        "                self.store_learning(user_message, \"General feedback post-evaluation\", human_general_learning)\n",
        "                print(f\"General learning stored: '{human_general_learning}'\")\n",
        "        except EOFError:\n",
        "            print(\"EOFError: Skipping general learning input (non-interactive).\")\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"user_message\": user_message,\n",
        "            \"anthropic_response\": anthropic_response_text,\n",
        "            \"openai_response\": openai_response_text,\n",
        "            \"evaluation\": evaluation\n",
        "        }\n",
        "\n",
        "    def process_human_feedback_actions(self, feedback: str, target_storage_for_action: Optional[Storage]) -> str:\n",
        "        action_result_summary = \"No specific data action taken based on feedback.\"\n",
        "        if not target_storage_for_action:\n",
        "            action_result_summary = \"Skipping data action: No target storage specified for feedback.\"\n",
        "            print(f\"[Human Feedback Action] {action_result_summary}\")\n",
        "            return action_result_summary\n",
        "\n",
        "        # Order updates\n",
        "        order_update_match = re.search(r\"update\\s+order\\s+(\\w+)\\s+status\\s+to\\s+(\\w+)\", feedback, re.IGNORECASE)\n",
        "        if order_update_match:\n",
        "            order_id, new_status = order_update_match.groups()\n",
        "            try:\n",
        "                result = self.process_tool_call(\"update_order_status\", {\"order_id\": order_id, \"new_status\": new_status}, target_storage_for_action)\n",
        "                action_result_summary = f\"Action executed on {type(target_storage_for_action).__name__}: Updated order {order_id} status to {new_status}. Result: {result.get('status', 'N/A')}\"\n",
        "                # Update context if successful\n",
        "                if result.get(\"status\") == \"success\" and \"order_details\" in result:\n",
        "                     self.conversation_context.update_entity_in_context(\"orders\", order_id, result[\"order_details\"])\n",
        "                     self.conversation_context.set_last_action(f\"human_feedback_update_order_{type(target_storage_for_action).__name__}\",\n",
        "                                                              {\"input\": {\"order_id\": order_id, \"new_status\": new_status}, \"result\": result})\n",
        "\n",
        "            except Exception as e:\n",
        "                action_result_summary = f\"Failed to update order {order_id} on {type(target_storage_for_action).__name__}: {str(e)}\"\n",
        "            print(f\"[Human Feedback Action] {action_result_summary}\")\n",
        "            return action_result_summary\n",
        "\n",
        "        # Product creation\n",
        "        product_create_match = re.search(\n",
        "            r\"create\\s+(?:new\\s+)?product:\\\\s*(.*?),\\\\s*description:\\\\s*(.*?),\\\\s*price:\\\\s*(\\\\d+\\\\.?\\\\d*),\\\\s*inventory:\\\\s*(\\\\d+)\",\n",
        "            feedback, re.IGNORECASE\n",
        "        )\n",
        "        if product_create_match:\n",
        "            name, desc, price, inventory = product_create_match.groups()\n",
        "            try:\n",
        "                tool_input = {\"name\": name.strip(), \"description\": desc.strip(), \"price\": float(price), \"inventory_count\": int(inventory)}\n",
        "                result = self.process_tool_call(\"create_product\", tool_input, target_storage_for_action)\n",
        "                action_result_summary = f\"Action executed on {type(target_storage_for_action).__name__}: Created product '{name}'. Result: {result.get('status', 'N/A')}\"\n",
        "                if result.get(\"status\") == \"success\" and \"product\" in result:\n",
        "                     self.conversation_context.update_entity_in_context(\"products\", result[\"product_id\"], result[\"product\"])\n",
        "                     self.conversation_context.set_last_action(f\"human_feedback_create_product_{type(target_storage_for_action).__name__}\",\n",
        "                                                              {\"input\": tool_input, \"result\": result})\n",
        "\n",
        "            except Exception as e:\n",
        "                action_result_summary = f\"Failed to create product '{name}' on {type(target_storage_for_action).__name__}: {str(e)}\"\n",
        "            print(f\"[Human Feedback Action] {action_result_summary}\")\n",
        "            return action_result_summary\n",
        "\n",
        "        print(f\"[Human Feedback Action] {action_result_summary}\")\n",
        "        return action_result_summary\n",
        "\n",
        "\n",
        "    def evaluate_responses(self, user_message: str, anthropic_response: str, openai_response: str, context_summary_for_eval: str, learnings_for_eval: str) -> Dict[str, Any]:\n",
        "        print(\"\\n--- Starting Evaluation by Gemini ---\")\n",
        "        try:\n",
        "            # Initial evaluation prompt\n",
        "            eval_prompt_parts = [\n",
        "                f\"User query: {user_message}\",\n",
        "                f\"Current context provided to assistants:\\n{context_summary_for_eval}\",\n",
        "                f\"Anthropic Claude response:\\n{anthropic_response}\",\n",
        "                f\"OpenAI GPT response:\\n{openai_response}\",\n",
        "                learnings_for_eval, # Relevant past learnings\n",
        "                \"Please evaluate both responses based on accuracy, efficiency, context awareness, and helpfulness. Provide an overall score (1-10) for each and detailed reasoning.\"\n",
        "            ]\n",
        "            eval_prompt = \"\\n\\n\".join(filter(None, eval_prompt_parts)) # filter(None, ...) removes empty strings\n",
        "            # print(f\"Gemini Eval Prompt (first 300 chars): {eval_prompt[:300]}...\")\n",
        "\n",
        "            gemini_response_obj = eval_model_instance.generate_content(eval_prompt)\n",
        "            evaluation_text = gemini_response_obj.text\n",
        "            print(f\"Gemini Raw Initial Evaluation:\\n{evaluation_text}\")\n",
        "\n",
        "            clarification_details = {\"used\": False, \"needed\": \"\", \"provided_input\": \"\", \"action_summary\": \"\"}\n",
        "\n",
        "            # Check if Gemini needs clarification\n",
        "            if \"CLARIFICATION NEEDED:\" in evaluation_text.upper():\n",
        "                clarification_details[\"used\"] = True\n",
        "                clarification_details[\"needed\"] = self.extract_clarification_needed(evaluation_text)\n",
        "                print(f\"--- Human Clarification Indicated by Evaluator ---\")\n",
        "                print(f\"Clarification needed by evaluator: {clarification_details['needed']}\")\n",
        "                try:\n",
        "                    human_input_for_eval = input(f\"Enter human clarification for evaluator (or type 'skip' or 'quit'): \")\n",
        "                    if human_input_for_eval.lower() in ['quit', 'exit', 'stop', 'q']:\n",
        "                        print(\"Exiting the system. Goodbye!\")\n",
        "                        raise SystemExit(\"User requested exit during evaluation\")\n",
        "\n",
        "                    elif human_input_for_eval.lower() != 'skip':\n",
        "                        clarification_details[\"provided_input\"] = human_input_for_eval\n",
        "                        target_storage_for_action = None\n",
        "                        # Ask which store to apply data modification if any\n",
        "                        if any(cmd in human_input_for_eval.lower() for cmd in [\"update order\", \"create product\"]): # Basic check\n",
        "                            store_choice = input(\"Apply data change to (A)nthropic's store, (O)penAI's store, or (S)kip data change? [A/O/S]: \").lower()\n",
        "                            if store_choice == 'a':\n",
        "                                target_storage_for_action = self.anthropic_storage\n",
        "                            elif store_choice == 'o':\n",
        "                                target_storage_for_action = self.openai_storage\n",
        "\n",
        "                        action_summary = self.process_human_feedback_actions(human_input_for_eval, target_storage_for_action)\n",
        "                        clarification_details[\"action_summary\"] = action_summary\n",
        "\n",
        "                        # Store this interaction as a learning\n",
        "                        self.store_learning(user_message, clarification_details[\"needed\"], f\"{human_input_for_eval} (Action: {action_summary})\")\n",
        "\n",
        "                        # Re-evaluate with clarification\n",
        "                        updated_eval_prompt = f\"{eval_prompt}\\n\\nHuman clarification provided to evaluator: {human_input_for_eval}\\nAction taken based on feedback: {action_summary}\\nPlease re-evaluate incorporating this.\"\n",
        "                        updated_gemini_response = eval_model_instance.generate_content(updated_eval_prompt)\n",
        "                        evaluation_text = updated_gemini_response.text\n",
        "                        print(f\"Gemini Raw Re-Evaluation after human input:\\n{evaluation_text}\")\n",
        "                    else:\n",
        "                        print(\"Skipping human input for evaluator.\")\n",
        "                        clarification_details[\"provided_input\"] = \"Skipped by user\"\n",
        "                except EOFError:\n",
        "                    print(\"EOFError: Skipping human clarification for evaluator (non-interactive).\")\n",
        "                    clarification_details[\"provided_input\"] = \"Skipped (non-interactive)\"\n",
        "\n",
        "\n",
        "            # Sanity Check: Compare end-state dictionaries\n",
        "            anthropic_data_state = {\n",
        "                \"customers\": self.anthropic_storage.customers,\n",
        "                \"products\": self.anthropic_storage.products,\n",
        "                \"orders\": self.anthropic_storage.orders\n",
        "            }\n",
        "            openai_data_state = {\n",
        "                \"customers\": self.openai_storage.customers,\n",
        "                \"products\": self.openai_storage.products,\n",
        "                \"orders\": self.openai_storage.orders\n",
        "            }\n",
        "\n",
        "            comparison_prompt_parts = [\n",
        "                evaluation_text, # Include the previous evaluation\n",
        "                \"\\n\\n--- Data Store Comparison Task ---\",\n",
        "                \"As a final step, please compare the following data store states from Anthropic and OpenAI.\",\n",
        "                f\"Anthropic's Data Store State:\\n{json.dumps(anthropic_data_state, indent=2, default=str)}\",\n",
        "                f\"OpenAI's Data Store State:\\n{json.dumps(openai_data_state, indent=2, default=str)}\",\n",
        "                \"1. Identify any key differences between the two data stores.\",\n",
        "                \"2. Explain plausible reasons for these differences based on the agents' actions during this turn (if known).\",\n",
        "                \"3. State whether these differences, now explicitly reviewed, cause you to update your previous scores or assessment for either agent. If so, provide the updated scores and rationale.\"\n",
        "            ]\n",
        "            comparison_prompt = \"\\n\\n\".join(comparison_prompt_parts)\n",
        "            # print(f\"Gemini Comparison Prompt (first 300 chars): {comparison_prompt[:300]}...\")\n",
        "\n",
        "            comparison_response_obj = eval_model_instance.generate_content(comparison_prompt)\n",
        "            final_evaluation_text = comparison_response_obj.text # This now includes the comparison\n",
        "            print(f\"Gemini Full Evaluation (including Data Store Comparison):\\n{final_evaluation_text}\")\n",
        "\n",
        "\n",
        "            anthropic_score = self.extract_score(final_evaluation_text, \"Anthropic\")\n",
        "            openai_score = self.extract_score(final_evaluation_text, \"OpenAI\")\n",
        "\n",
        "            return {\n",
        "                \"anthropic_score\": anthropic_score, \"openai_score\": openai_score,\n",
        "                \"full_evaluation\": final_evaluation_text,\n",
        "                \"clarification_details\": clarification_details\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluation: {str(e)}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            return {\n",
        "                \"error\": f\"Error in evaluation: {str(e)}\",\n",
        "                \"anthropic_score\": 0, \"openai_score\": 0,\n",
        "                \"full_evaluation\": f\"Evaluation failed: {str(e)}\",\n",
        "                \"clarification_details\": {\"used\": False, \"action_summary\": \"\"}\n",
        "            }\n",
        "\n",
        "    def extract_clarification_needed(self, evaluation_text: str) -> str:\n",
        "        clarification_match = re.search(r\"CLARIFICATION NEEDED:\\\\s*(.*?)(?:\\\\n|$)\", evaluation_text, re.IGNORECASE | re.DOTALL)\n",
        "        if clarification_match and clarification_match.group(1).strip():\n",
        "            return clarification_match.group(1).strip()\n",
        "        # Fallback if the specific format isn't found but the keyword is present\n",
        "        lines = evaluation_text.splitlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"CLARIFICATION NEEDED:\" in line.upper():\n",
        "                return line.split(\"CLARIFICATION NEEDED:\", 1)[-1].strip() + \"\\n\" + \"\\n\".join(lines[i+1:i+3]) # return a bit more context\n",
        "        return \"Evaluator indicated clarification needed, but specific question not formatted as expected. Review raw evaluation.\"\n",
        "\n",
        "\n",
        "    def store_learning(self, query: str, clarification_context: str, human_input: str):\n",
        "        # Use a general key or derive keywords if needed\n",
        "        learning_key = f\"learning_for_query_like_{self.extract_keywords(query)[:2]}\" # Simple key\n",
        "        if not query and \"General feedback\" in clarification_context:\n",
        "             learning_key = \"general_user_feedback\"\n",
        "\n",
        "\n",
        "        timestamp = datetime.now().isoformat()\n",
        "        log_entry = {\n",
        "            \"original_query_context\": query, # Could be the user_message or context of clarification\n",
        "            \"evaluator_clarification_prompt_or_context\": clarification_context, # What prompted this learning\n",
        "            \"human_input_or_learning\": human_input,\n",
        "            \"timestamp\": timestamp\n",
        "        }\n",
        "\n",
        "        if learning_key not in self.human_feedback_learnings:\n",
        "            self.human_feedback_learnings[learning_key] = []\n",
        "        self.human_feedback_learnings[learning_key].append(log_entry)\n",
        "        print(f\"Learning stored under key '{learning_key}': '{human_input[:100]}...'\")\n",
        "\n",
        "\n",
        "    def check_relevant_learnings(self, query: str) -> Optional[str]:\n",
        "        relevant_learnings_text = []\n",
        "        # Check for general feedback first\n",
        "        if \"general_user_feedback\" in self.human_feedback_learnings:\n",
        "            for learning in self.human_feedback_learnings[\"general_user_feedback\"]:\n",
        "                 relevant_learnings_text.append(\n",
        "                    f\"- General User Feedback (from {learning['timestamp']}): {learning['human_input_or_learning']}\"\n",
        "                )\n",
        "\n",
        "        # Check for query-specific learnings\n",
        "        keywords = self.extract_keywords(query)\n",
        "        for keyword in keywords: # Check first 2 keywords for broader match\n",
        "            learning_key_prefix = f\"learning_for_query_like_{keyword}\"\n",
        "            for key, learnings_list in self.human_feedback_learnings.items():\n",
        "                if key.startswith(learning_key_prefix) or keyword in key :\n",
        "                    for learning in learnings_list:\n",
        "                        # Avoid re-adding general feedback if already covered\n",
        "                        if learning_key_prefix == \"general_user_feedback\" and \"general_user_feedback\" in self.human_feedback_learnings:\n",
        "                            continue\n",
        "\n",
        "                        text_to_add = (\n",
        "                            f\"- Context: '{learning['original_query_context']}' ({learning['timestamp']})\\n\"\n",
        "                            f\"  Situation: {learning['evaluator_clarification_prompt_or_context']}\\n\"\n",
        "                            f\"  User Input/Learning: {learning['human_input_or_learning']}\"\n",
        "                        )\n",
        "                        if text_to_add not in relevant_learnings_text: # Avoid duplicates\n",
        "                             relevant_learnings_text.append(text_to_add)\n",
        "\n",
        "        return \"\\n\\n\".join(list(set(relevant_learnings_text))) if relevant_learnings_text else None # Use set to ensure unique learnings\n",
        "\n",
        "    def extract_keywords(self, text: str) -> List[str]:\n",
        "        if not text: return [\"general\"]\n",
        "        words = re.findall(r'\\b\\w{4,}\\b', text.lower())\n",
        "        stop_words = {\"the\", \"and\", \"is\", \"in\", \"to\", \"a\", \"of\", \"for\", \"with\", \"on\", \"at\", \"what\", \"how\", \"show\", \"tell\", \"please\", \"what's\", \"i'd\", \"like\", \"user\", \"query\", \"this\", \"that\", \"context\"}\n",
        "        extracted = list(set(word for word in words if word not in stop_words))\n",
        "        return extracted if extracted else [\"generic\"]\n",
        "\n",
        "\n",
        "    def extract_score(self, evaluation_text: str, model_name_pattern: str) -> int:\n",
        "        # Try to find scores after the data comparison section if it exists\n",
        "        comparison_section_start = evaluation_text.upper().rfind(\"--- DATA STORE COMPARISON TASK ---\")\n",
        "        search_text = evaluation_text\n",
        "        if comparison_section_start != -1:\n",
        "            # If comparison exists, and it might contain \"updated scores\", prioritize that part\n",
        "            update_score_marker = re.search(r\"update your previous scores|updated scores and rationale\", evaluation_text[comparison_section_start:], re.IGNORECASE)\n",
        "            if update_score_marker:\n",
        "                search_text = evaluation_text[comparison_section_start:] # Search in the comparison part onwards\n",
        "\n",
        "        patterns = [\n",
        "            rf\"{model_name_pattern}.*?Overall Score.*?(\\d+)/10\",\n",
        "            rf\"{model_name_pattern}.*?Overall Score:\\s*(\\d+)\",\n",
        "            rf\"Overall Score.*?{model_name_pattern}.*?:\\s*(\\d+)\",\n",
        "            rf\"{model_name_pattern}.*?score.*?:.*?(\\d+)\",\n",
        "            rf\"{model_name_pattern}.*?\\bscore\\b.*?(\\d+)\", # More general\n",
        "            rf\"{model_name_pattern}.*?rating.*?:.*?(\\d+)\",\n",
        "            rf\"{model_name_pattern}.*?\\b(\\\\d+)/10\",\n",
        "             # Try to find updated scores first if mentioned\n",
        "            rf\"Updated score for {model_name_pattern}.*?:.*?(\\d+)\",\n",
        "            rf\"{model_name_pattern}.*?updated overall score.*?:.*?(\\d+)\"\n",
        "        ]\n",
        "        # Search in reverse for patterns to catch the latest score mention if text is long\n",
        "        for p_str in reversed(patterns):\n",
        "            # Find all matches and take the last one, as it's more likely to be the final/updated score\n",
        "            matches = list(re.finditer(p_str, search_text, re.IGNORECASE | re.DOTALL))\n",
        "            if matches:\n",
        "                last_match = matches[-1]\n",
        "                if last_match.group(1):\n",
        "                    try:\n",
        "                        score = int(last_match.group(1))\n",
        "                        print(f\"Extracted score {score} for '{model_name_pattern}' using pattern: {p_str}\")\n",
        "                        return score\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "        print(f\"Could not extract score for '{model_name_pattern}' from eval text snippet (tried specific patterns):\\n{search_text[-500:]}...\") # Show tail\n",
        "        return 0 # Default if no score found\n",
        "\n",
        "print(\"DualAgentEvaluator class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzLHm8MmSOMC",
        "outputId": "c3a413b9-d373-4608-dadb-5a82a0e9a128"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DualAgentEvaluator class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Main Execution (or whichever cell contains your main() function)\n",
        "\n",
        "\"\"\" Sample queries:\n",
        "* Show me all the products available\n",
        "* I'd like to order 25 Perplexinators, please\n",
        "* Show me the status of my order\n",
        "* (If the order is not in Shipped state, then) Please ship my order now\n",
        "* How many Perplexinators are now left in stock?\n",
        "* Add a new customer: Bill Leece, bill.leece@mail.com, +1.222.333.4444\n",
        "* Add new new product: Gizmo X, description: A fancy gizmo, price: 29.99, inventory: 50\n",
        "* Update Gizzmo's price to 99.99\n",
        "* I need to update our insurance policy, so I need to know the total value of everything that I have in our inventory. Please tell me this amount.\n",
        "* Summarize your learnings from our recent interactions.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"\\nStarting Main Execution...\\n\")\n",
        "    # Basic check for placeholder API keys (if UserDataMock or similar was used)\n",
        "    # Adjust this check if you're loading keys differently\n",
        "    # if any('YOUR_KEY_HERE' in key_value for key_value in [ANTHROPIC_API_KEY, OPENAI_API_KEY, GOOGLE_API_KEY]):\n",
        "    #     print(\"ERROR: Placeholder API keys detected. Update API key setup. Exiting.\")\n",
        "    #     return\n",
        "\n",
        "    agent = DualAgentEvaluator()\n",
        "    results_log = []\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_query = input(\"\\nEnter your query (or 'quit', 'exit', 'stop', 'q' to end): \")\n",
        "            if user_query.lower() in ['quit', 'exit', 'stop', 'q']:\n",
        "                print(\"Exiting the system. Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if not user_query.strip():\n",
        "                print(\"Empty query, please enter something.\")\n",
        "                continue\n",
        "\n",
        "            result = agent.process_user_request(user_query)\n",
        "            results_log.append(result)\n",
        "        except SystemExit as se: # Catch SystemExit if user quits during clarification\n",
        "            print(f\"System exit requested: {se}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL ERROR processing query '{user_query}': {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            results_log.append({\n",
        "                \"user_message\": user_query, \"anthropic_response\": \"ERROR\", \"openai_response\": \"ERROR\",\n",
        "                \"evaluation\": {\"anthropic_score\": 0, \"openai_score\": 0, \"full_evaluation\": f\"Critical error: {e}\",\n",
        "                               \"clarification_details\": {\"used\": False, \"action_summary\": \"\"}}\n",
        "            })\n",
        "\n",
        "    print(\"\\n\\n===== EVALUATION SUMMARY =====\")\n",
        "    total_anthropic, total_openai, num_q = 0, 0, 0 # Initialize num_q\n",
        "\n",
        "    for i, res in enumerate(results_log):\n",
        "        if not res: # Skip if result is None or empty\n",
        "            print(f\"\\nQuery {i+1}: Skipped (empty result).\")\n",
        "            continue\n",
        "\n",
        "        num_q +=1 # Increment only for valid results\n",
        "        print(f\"\\nQuery {i+1}: {res.get('user_message', 'N/A')}\")\n",
        "        print(f\"  Anthropic Resp: {str(res.get('anthropic_response', 'N/A'))[:100]}...\")\n",
        "        print(f\"  OpenAI Resp: {str(res.get('openai_response', 'N/A'))[:100]}...\")\n",
        "\n",
        "        eval_data = res.get('evaluation', {})\n",
        "        anth_s = eval_data.get('anthropic_score',0)\n",
        "        open_s = eval_data.get('openai_score',0)\n",
        "        total_anthropic += anth_s\n",
        "        total_openai += open_s\n",
        "        print(f\"  Scores - Anthropic: {anth_s}, OpenAI: {open_s}\")\n",
        "\n",
        "        clarif_details = eval_data.get('clarification_details',{})\n",
        "        if clarif_details.get('used'):\n",
        "            print(f\"    Clarification: Needed='{clarif_details.get('needed', 'N/A')}', Provided='{clarif_details.get('provided_input', 'N/A')}'\")\n",
        "            if clarif_details.get('action_summary'):\n",
        "                 print(f\"    Action from Clarification: {clarif_details['action_summary']}\")\n",
        "\n",
        "\n",
        "        winner = \"Tie\"\n",
        "        if anth_s is not None and open_s is not None: # Ensure scores are not None\n",
        "            if anth_s > open_s: winner = \"Anthropic\"\n",
        "            elif open_s > anth_s: winner = \"OpenAI\"\n",
        "        print(f\"  Query Winner: {winner}\")\n",
        "\n",
        "\n",
        "    print(f\"\\n----- Overall Performance -----\")\n",
        "    if num_q > 0:\n",
        "        print(f\"Avg Anthropic: {total_anthropic/num_q:.2f}, Avg OpenAI: {total_openai/num_q:.2f}\")\n",
        "    else:\n",
        "        print(\"No queries processed to calculate average scores.\")\n",
        "    print(f\"Total Anthropic: {total_anthropic}, Total OpenAI: {total_openai}\")\n",
        "    overall_winner = \"Tie\"\n",
        "    if total_anthropic > total_openai: overall_winner = \"Anthropic\"\n",
        "    elif total_openai > total_anthropic: overall_winner = \"OpenAI\"\n",
        "    print(f\"Overall Winner: {overall_winner}\")\n",
        "\n",
        "    if agent.human_feedback_learnings:\n",
        "        print(\"\\n----- Learned Clarifications / Feedback (for AI Models & Evaluator) -----\")\n",
        "        for kw, l_list in agent.human_feedback_learnings.items():\n",
        "            print(f\"Keyword/Context Key: {kw}\")\n",
        "            for item in l_list:\n",
        "                # Corrected keys used here:\n",
        "                q_context = item.get('original_query_context', 'N/A')\n",
        "                eval_context = item.get('evaluator_clarification_prompt_or_context', 'N/A')\n",
        "                learning_input = item.get('human_input_or_learning', 'N/A')\n",
        "                timestamp = item.get('timestamp', 'N/A')\n",
        "                print(f\"  - Query Context: '{q_context}' (Timestamp: {timestamp})\")\n",
        "                print(f\"    Evaluator Context/Feedback Type: {eval_context}\")\n",
        "                print(f\"    Learning Input: {learning_input}\")\n",
        "    else:\n",
        "        print(\"\\nNo human feedback learnings were stored during this session.\")\n",
        "    print(\"\\nExecution Finished.\")\n",
        "\n",
        "# if __name__ == \"__main__\": # This block is usually for script execution, not Colab cells\n",
        "#     main()\n",
        "# print(\"Main function defined. Call main() in a new cell to run the evaluation.\")"
      ],
      "outputs": [],
      "execution_count": 64,
      "metadata": {
        "id": "O2-ztJ2BO7gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dg85ek6bSvW5",
        "outputId": "2c82620a-e579-42c7-800c-33a6195c42ae"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Main Execution...\n",
            "\n",
            "DualAgentEvaluator initialized with separate Storage for Anthropic and OpenAI.\n",
            "DualAgentEvaluator initialized. OpenAI tools formatted (9 tools).\n",
            "\n",
            "Enter your query (or 'quit', 'exit', 'stop', 'q' to end): I'd like to order 25 Perplexinators, please\n",
            "\n",
            "\n",
            "============================================================\n",
            "User Message: I'd like to order 25 Perplexinators, please\n",
            "============================================================\n",
            "Current Context Summary for Models:\n",
            "No specific context items set yet.\n",
            "------------------------------------------------------------\n",
            "\n",
            "Anthropic API Call #1. System: ' You are a helpful customer service assistant for an e-comme...', Messages count: 1\n",
            "Last message role: user\n",
            "Anthropic Tool Call: get_product_info, Input: {'product_id_or_name': 'Perplexinator'}\n",
            "--- [Tool Dispatcher] Attempting tool: get_product_info with input: {\"product_id_or_name\": \"Perplexinator\"} for storage: Storage ---\n",
            "[Tool Helper] find_product_by_name: Matched 'Perplexinator' to 'Perplexinator' (ID: P3) with score 100 (in Storage)\n",
            "[Tool Executed] get_product_info: Found by name (fuzzy) 'Perplexinator' as ID 'P3' (in Storage).\n",
            "--- [Tool Dispatcher] Result for get_product_info on Storage: {\n",
            "  \"status\": \"success\",\n",
            "  \"message\": \"Found product matching 'Perplexinator'\",\n",
            "  \"product_id\": \"P3\",\n",
            "  \"product\": {\n",
            "    \"name\": \"Perplexinator\",\n",
            "    \"description\": \"A perplexing perfunctator\",\n",
            "    \"price\": 79.99,\n",
            "    \"inventory_count\": 1483\n",
            "  }\n",
            "} ---\n",
            "[Context Updated] Entity: products, ID: P3, Data (type): <class 'dict'>\n",
            "[Context Updated] Last Action: get_product_info_Anthropic, Details: {\"input\": {\"product_id_or_name\": \"Perplexinator\"}, \"result\": {\"status\": \"success\", \"message\": \"Found product matching 'Perplexinator'\", \"product_id\": \"P3\", \"product\": {\"name\": \"Perplexinator\", \"description\": \"A perplexing perfunctator\", \"price\": 79.99, \"inventory_count\": 1483}}}\n",
            "\n",
            "Anthropic API Call #2. System: ' You are a helpful customer service assistant for an e-comme...', Messages count: 3\n",
            "Last message role: user\n",
            "Anthropic Tool Call: create_order, Input: {'product_id_or_name': 'Perplexinator', 'quantity': 25, 'status': 'Processing'}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25, \"status\": \"Processing\"} for storage: Storage ---\n",
            "[Tool Helper] find_product_by_name: Matched 'Perplexinator' to 'Perplexinator' (ID: P3) with score 100 (in Storage)\n",
            "[Tool Executed] create_order: Order O3 created for 25 of Perplexinator (ID: P3). Status: Processing. Remaining inv: 1483 (in Storage)\n",
            "--- [Tool Dispatcher] Result for create_order on Storage: {\n",
            "  \"status\": \"success\",\n",
            "  \"order_id\": \"O3\",\n",
            "  \"order_details\": {\n",
            "    \"id\": \"O3\",\n",
            "    \"product_id\": \"P3\",\n",
            "    \"product_name\": \"Perplexinator\",\n",
            "    \"quantity\": 25,\n",
            "    \"price\": 79.99,\n",
            "    \"status\": \"Processing\"\n",
            "  },\n",
            "  \"remaining_inventory\": 1483\n",
            "} ---\n",
            "[Context Updated] Entity: orders, ID: O3, Data (type): <class 'dict'>\n",
            "[Context Updated] Last Action: create_order_Anthropic, Details: {\"input\": {\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25, \"status\": \"Processing\"}, \"result\": {\"status\": \"success\", \"order_id\": \"O3\", \"order_details\": {\"id\": \"O3\", \"product_id\": \"P3\", \"product_name\": \"Perplexinator\", \"quantity\": 25, \"price\": 79.99, \"status\": \"Processing\"}, \"remaining_inventory\": 1483}}\n",
            "\n",
            "Anthropic API Call #3. System: ' You are a helpful customer service assistant for an e-comme...', Messages count: 5\n",
            "Last message role: user\n",
            "Anthropic Final Text (no tool use this turn): Great! I've successfully placed your order for 25 Perplexinators. Here's a summary of your order:\n",
            "- Order ID: O3\n",
            "- Product: Perplexinator\n",
            "- Quantity: 25\n",
            "- Price per unit: $79.99\n",
            "- Total price: $1,999.75\n",
            "- Status: Processing\n",
            "\n",
            "Is there anything else you need help with?\n",
            "\n",
            "OpenAI API Call #1. Messages count: 3\n",
            "Last message role: assistant\n",
            "OpenAI Tool Call: create_order, Input: {'product_name': 'Perplexinator', 'quantity': 25}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product_name\": \"Perplexinator\", \"quantity\": 25} for storage: Storage ---\n",
            "--- [Tool Dispatcher] TypeError for create_order on Storage: create_order() got an unexpected keyword argument 'product_name'. Input: {'product_name': 'Perplexinator', 'quantity': 25} ---\n",
            "[Context Updated] Last Action: create_order_OpenAI, Details: {\"input\": {\"product_name\": \"Perplexinator\", \"quantity\": 25}, \"result\": {\"status\": \"error\", \"message\": \"TypeError calling create_order: create_order() got an unexpected keyword argument 'product_name'. Check arguments.\"}}\n",
            "\n",
            "OpenAI API Call #2. Messages count: 5\n",
            "Last message role: tool\n",
            "OpenAI Tool Call: create_order, Input: {'product': 'Perplexinator', 'quantity': 25}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product\": \"Perplexinator\", \"quantity\": 25} for storage: Storage ---\n",
            "--- [Tool Dispatcher] TypeError for create_order on Storage: create_order() got an unexpected keyword argument 'product'. Input: {'product': 'Perplexinator', 'quantity': 25} ---\n",
            "[Context Updated] Last Action: create_order_OpenAI, Details: {\"input\": {\"product\": \"Perplexinator\", \"quantity\": 25}, \"result\": {\"status\": \"error\", \"message\": \"TypeError calling create_order: create_order() got an unexpected keyword argument 'product'. Check arguments.\"}}\n",
            "\n",
            "OpenAI API Call #3. Messages count: 7\n",
            "Last message role: tool\n",
            "OpenAI Tool Call: create_order, Input: {'name': 'Perplexinator', 'quantity': 25}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"name\": \"Perplexinator\", \"quantity\": 25} for storage: Storage ---\n",
            "--- [Tool Dispatcher] TypeError for create_order on Storage: create_order() got an unexpected keyword argument 'name'. Input: {'name': 'Perplexinator', 'quantity': 25} ---\n",
            "[Context Updated] Last Action: create_order_OpenAI, Details: {\"input\": {\"name\": \"Perplexinator\", \"quantity\": 25}, \"result\": {\"status\": \"error\", \"message\": \"TypeError calling create_order: create_order() got an unexpected keyword argument 'name'. Check arguments.\"}}\n",
            "\n",
            "OpenAI API Call #4. Messages count: 9\n",
            "Last message role: tool\n",
            "OpenAI Tool Call: create_order, Input: {'product_id_or_name': 'Perplexinator', 'quantity': 25}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25} for storage: Storage ---\n",
            "--- [Tool Dispatcher] TypeError for create_order on Storage: create_order() missing 1 required positional argument: 'status'. Input: {'product_id_or_name': 'Perplexinator', 'quantity': 25} ---\n",
            "[Context Updated] Last Action: create_order_OpenAI, Details: {\"input\": {\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25}, \"result\": {\"status\": \"error\", \"message\": \"TypeError calling create_order: create_order() missing 1 required positional argument: 'status'. Check arguments.\"}}\n",
            "\n",
            "OpenAI API Call #5. Messages count: 11\n",
            "Last message role: tool\n",
            "OpenAI Tool Call: create_order, Input: {'product_id_or_name': 'Perplexinator', 'quantity': 25, 'status': 'Processing'}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25, \"status\": \"Processing\"} for storage: Storage ---\n",
            "[Tool Helper] find_product_by_name: Matched 'Perplexinator' to 'Perplexinator' (ID: P3) with score 100 (in Storage)\n",
            "[Tool Executed] create_order: Order O3 created for 25 of Perplexinator (ID: P3). Status: Processing. Remaining inv: 1483 (in Storage)\n",
            "--- [Tool Dispatcher] Result for create_order on Storage: {\n",
            "  \"status\": \"success\",\n",
            "  \"order_id\": \"O3\",\n",
            "  \"order_details\": {\n",
            "    \"id\": \"O3\",\n",
            "    \"product_id\": \"P3\",\n",
            "    \"product_name\": \"Perplexinator\",\n",
            "    \"quantity\": 25,\n",
            "    \"price\": 79.99,\n",
            "    \"status\": \"Processing\"\n",
            "  },\n",
            "  \"remaining_inventory\": 1483\n",
            "} ---\n",
            "[Context Updated] Entity: orders, ID: O3, Data (type): <class 'dict'>\n",
            "[Context Updated] Last Action: create_order_OpenAI, Details: {\"input\": {\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25, \"status\": \"Processing\"}, \"result\": {\"status\": \"success\", \"order_id\": \"O3\", \"order_details\": {\"id\": \"O3\", \"product_id\": \"P3\", \"product_name\": \"Perplexinator\", \"quantity\": 25, \"price\": 79.99, \"status\": \"Processing\"}, \"remaining_inventory\": 1483}}\n",
            "\n",
            "--- Anthropic Final Response Text ---\n",
            "Great! I've successfully placed your order for 25 Perplexinators. Here's a summary of your order:\n",
            "- Order ID: O3\n",
            "- Product: Perplexinator\n",
            "- Quantity: 25\n",
            "- Price per unit: $79.99\n",
            "- Total price: $1,999.75\n",
            "- Status: Processing\n",
            "\n",
            "Is there anything else you need help with?\n",
            "--- OpenAI Final Response Text ---\n",
            "Max tool iterations reached for OpenAI.\n",
            "\n",
            "--- Starting Evaluation by Gemini ---\n",
            "Gemini Raw Initial Evaluation:\n",
            "**Anthropic Claude Response Evaluation:**\n",
            "\n",
            "*   **Accuracy: 2/10**\n",
            "    *   The assistant correctly identified the product (\"Perplexinator\") and quantity (25) from the user's request.\n",
            "    *   However, it claims to have \"successfully placed your order\" and provides an Order ID (O3), Price per unit ($79.99), Total price ($1,999.75), and Status (Processing). Since no product catalog, pricing information, or ordering system capabilities were mentioned in the \"Current context,\" these details are presumed to be fabricated. Making definitive statements about an order being placed and inventing financial details without a basis in available information is highly inaccurate.\n",
            "\n",
            "*   **Efficiency: 3/10**\n",
            "    *   The assistant responded directly to the request without asking clarifying questions. If the order placement were genuine and successful, this would be efficient.\n",
            "    *   However, since the order confirmation appears fabricated, this directness leads to a misleading outcome. True efficiency involves reaching a *correct* and *factual* resolution. Fabricating an order is not an efficient path to actual customer satisfaction or task completion. It might have been more efficient to state limitations or ask necessary preliminary questions (e.g., about account details, or availability of the product).\n",
            "\n",
            "*   **Context Awareness: 7/10**\n",
            "    *   The assistant correctly understood the user's immediate intent to place an order for a specific product and quantity based on the current query. \"No specific context items set yet\" means it relied solely on the user's utterance, which it interpreted correctly at a surface level.\n",
            "\n",
            "*   **Helpfulness: 2/10**\n",
            "    *   On the surface, confirming an order seems helpful. However, if the order is not actually placed (as suggested by the lack of supporting information/system capabilities), then providing a false confirmation with fabricated details is ultimately unhelpful. It misleads the user, sets incorrect expectations, and would likely lead to future frustration and problems. A helpful response would involve accurately reflecting its capabilities or guiding the user through a real process.\n",
            "\n",
            "*   **Overall Score: 2/10**\n",
            "    *   Reasoning: While the assistant understood the request, its response is critically flawed by fabricating information core to the user's transactional query (order placement, price, order ID). In a customer service context, such inaccuracies are highly detrimental. This type of \"hallucinated success\" is worse than an explicit failure as it deceives the user.\n",
            "\n",
            "**OpenAI GPT Response Evaluation:**\n",
            "\n",
            "*   **Accuracy: 1/10**\n",
            "    *   The response \"Max tool iterations reached\" is not an accurate or relevant answer to the user's query about placing an order. While it might be an accurate statement about an internal system state of the AI, it does not address the user's request factually.\n",
            "\n",
            "*   **Efficiency: 1/10**\n",
            "    *   The assistant failed to process the request. It did not get to any answer, let alone a correct one, and provided no steps for the user to proceed.\n",
            "\n",
            "*   **Context Awareness: 1/10**\n",
            "    *   The response does not demonstrate any understanding of the user's query or the context of wanting to place an order.\n",
            "\n",
            "*   **Helpfulness: 1/10**\n",
            "    *   The error message \"Max tool iterations reached\" is entirely unhelpful to a user wishing to place an order. It doesn't fulfill the request, explain the problem in a user-understandable way, or offer any alternatives or solutions.\n",
            "\n",
            "*   **Overall Score: 1/10**\n",
            "    *   Reasoning: The assistant completely failed to address the user's query, instead returning a system error message. This offers no value or assistance to the user.\n",
            "\n",
            "**Learning:**\n",
            "When a user requests to place an order or perform a similar transaction:\n",
            "1.  The AI assistant must verify if it has the capability (e.g., through tools) and the necessary information (e.g., product availability, price from a catalog) to fulfill the request.\n",
            "2.  If it can proceed, it should use the appropriate tools and confirm real actions.\n",
            "3.  If it lacks the capability or information (e.g., product not found, price unknown, no ordering tool), it should inform the user accurately about its limitations or ask clarifying questions to gather the required details (e.g., \"I can't seem to find 'Perplexinator' in our current catalog. Could you confirm the spelling or provide a product code?\" or \"I can help you with that. Before I place the order, I'll need to confirm your account details.\").\n",
            "4.  Fabricating transactional details (like order IDs, prices, or confirmation of an action not taken) is a critical failure and should be avoided. It is better to admit a limitation or ask for more information than to provide false or misleading information.\n",
            "5.  Error messages like \"Max tool iterations reached\" should be caught and translated into user-friendly explanations and recovery paths, if possible, rather than being directly exposed to the user.\n",
            "Gemini Full Evaluation (including Data Store Comparison):\n",
            "Okay, I will compare the data store states and discuss their impact on the initial evaluations.\n",
            "\n",
            "--- Data Store Comparison ---\n",
            "\n",
            "**1. Identify any key differences between the two data stores.**\n",
            "\n",
            "Upon review, the \"Anthropic's Data Store State\" and \"OpenAI's Data Store State\" provided are **identical in structure and content**. Both data stores show:\n",
            "*   The same customer data (C1, C2).\n",
            "*   The same product data, including `P3` (\"Perplexinator\") with `name: \"Perplexinator\"`, `price: 79.99`, and `inventory_count: 1483`.\n",
            "*   The same order data, including `O3` with `id: \"O3\"`, `product_id: \"P3\"`, `product_name: \"Perplexinator\"`, `quantity: 25`, `price: 79.99`, and `status: \"Processing\"`.\n",
            "\n",
            "There are no differences *between* the two data stores themselves. The significant aspect is how this identical state aligns with each assistant's reported actions.\n",
            "\n",
            "**2. Explain plausible reasons for these differences based on the agents' actions during this turn (if known).**\n",
            "\n",
            "Since the data stores' *content* is identical, we're explaining the implications of this identical state in light of each agent's actions:\n",
            "\n",
            "*   **For Anthropic:**\n",
            "    *   Anthropic's response claimed to have \"successfully placed your order\" for 25 units of \"Perplexinator,\" providing Order ID O3, price $79.99, and status \"Processing.\"\n",
            "    *   Anthropic's data store is consistent with this claim, as it shows product P3 (\"Perplexinator\") with a price of $79.99 and an order O3 for 25 units of P3 with status \"Processing.\"\n",
            "    *   This implies that Anthropic's action (whether based on actual capability or hallucination that was then persisted) resulted in these entries in its data store.\n",
            "    *   **However, a key inconsistency exists *within* Anthropic's data store (and therefore OpenAI's as well):** The product `P3` (\"Perplexinator\") has an `inventory_count` of 1483. Order `O3` is for a `quantity` of 25 units of P3. If the order was successfully processed and recorded, the inventory for P3 should have been decremented by 25 (i.e., 1483 - 25 = 1458). The inventory count remains 1483. This indicates an incomplete or flawed order processing logic; the order is logged, but the corresponding inventory adjustment was not made.\n",
            "\n",
            "*   **For OpenAI:**\n",
            "    *   OpenAI's response was \"Max tool iterations reached,\" indicating a failure to process the user's request.\n",
            "    *   OpenAI's data store being identical to Anthropic's (and thus showing order O3 and product P3 details) is highly inconsistent with its error message *if we assume this data store reflects changes made by OpenAI in this turn*.\n",
            "    *   Plausible reasons for this state, given OpenAI's response:\n",
            "        1.  **Pre-existing Data:** The product P3 and order O3 already existed in the data store *before* OpenAI's attempt, and OpenAI's error meant it did not alter this pre-existing state. Its data store simply reflects this unchanged prior state.\n",
            "        2.  **Partial Processing then Error:** OpenAI's tools began processing the order, made the data store modifications (creating/confirming P3 and O3), and *then* encountered the \"Max tool iterations reached\" error before it could formulate a success response. This would be a serious issue, as data was modified without user confirmation.\n",
            "        3.  **Erroneous Data Store Reporting:** The provided \"OpenAI's Data Store State\" might not accurately represent the state of the data store *after only OpenAI's actions for this specific turn* (e.g., it could be a copy of Anthropic's data store, or a baseline state not reflecting OpenAI's failure to act).\n",
            "\n",
            "    Without further information on the history of the data store or how \"OpenAI's Data Store State\" was captured, it's difficult to pinpoint the exact reason for its contents given OpenAI's error message. However, scenario 2 would be the most problematic from a system behavior perspective.\n",
            "\n",
            "**3. State whether these differences, now explicitly reviewed, cause you to update your previous scores or assessment for either agent. If so, provide the updated scores and rationale.**\n",
            "\n",
            "The \"previous scores\" are those provided in the initial problem description. This new data store information provides grounds to refine the assessment and confirm or adjust the reasoning behind those scores.\n",
            "\n",
            "**For Anthropic Claude:**\n",
            "\n",
            "*   The original evaluation gave an **Accuracy of 2/10**, largely because details like price and order confirmation were \"presumed to be fabricated\" due to \"No specific context items set yet.\"\n",
            "*   The data store now reveals that an order O3 *was* recorded and product P3 (\"Perplexinator\") *is* listed with the price $79.99.\n",
            "    *   If we maintain the original premise that \"no product catalog, pricing information... were mentioned in the 'Current context'\" (meaning this information was initially unavailable to the AI for retrieval): Then Anthropic either found a way to access this data (e.g., via a tool that wasn't part of the explicit \"context items\") or it hallucinated these details and successfully wrote them into the data store. If it hallucinated and wrote them, the original critique of fabrication stands.\n",
            "    *   **Crucially, the data store reveals a new factual error:** The `inventory_count` for \"Perplexinator\" (P3) was not decremented after order O3 (25 units) was recorded. It remains 1483 instead of becoming 1458. This is a significant error in an ordering process.\n",
            "*   **Impact on Scores:**\n",
            "    *   This new finding of the inventory error *reinforces the low Accuracy score of 2/10*. Even if the price wasn't fabricated (i.e., if P3 info was accessible), the action performed (placing the order) was factually incorrect in its execution within the data store (inventory not updated). If the price *was* fabricated, this is an additional layer of incorrectness.\n",
            "    *   The other scores (Efficiency 3/10, Context Awareness 7/10, Helpfulness 2/10, Overall 2/10) were predicated on the fabrication. The inventory error makes the interaction even less helpful in reality, as it would lead to stock discrepancies.\n",
            "    *   **Conclusion for Anthropic:** The data store information does not warrant an *increase* in scores. It either confirms the existing low scores by showing the persistence of fabricated data or adds a new reason (inventory error) for the system's action being inaccurate and unhelpful. The scores are already very low and remain appropriate.\n",
            "\n",
            "**For OpenAI GPT:**\n",
            "\n",
            "*   The original evaluation gave an **Accuracy of 1/10** and an **Overall Score of 1/10** because the \"Max tool iterations reached\" response completely failed to address the user's query and was unhelpful.\n",
            "*   The data store for OpenAI showing order O3 is problematic:\n",
            "    *   If order O3 and product P3 data *pre-existed* this turn and OpenAI's error meant it made no changes, then its data store simply reflects the state *before* its failed attempt. In this case, the data store doesn't change the assessment of its response for *this turn* – it still failed to act. The 1/10 scores hold.\n",
            "    *   If OpenAI *did* somehow cause order O3 to be created or modified in the data store during this turn before erroring out, this would be a more severe failure. It would mean the system made unconfirmed changes to data while reporting a generic error. This would underscore the 1/10 scores, making the failure mode more critical.\n",
            "    *   If the \"OpenAI's Data Store State\" is simply an error in the evaluation setup (e.g., a copy of Anthropic's state, not reflecting OpenAI's actual state after its error), then we cannot use it to re-assess OpenAI's action.\n",
            "*   **Impact on Scores:**\n",
            "    *   Given the existing scores are already the lowest possible (1/10), they cannot be numerically reduced. The data store information, particularly if it implies unconfirmed data modification by OpenAI (scenario 2 above), would amplify the reasons for this extremely low score but not change the score itself. Assuming the most benign interpretation (scenario 1, pre-existing data and OpenAI failed to act), the scores remain valid.\n",
            "    *   **Conclusion for OpenAI:** The provided data store information does not change the numerical scores. The response was a complete failure to address the user's request, and the data store information, depending on its origin, either doesn't change this or makes the underlying failure potentially more severe.\n",
            "\n",
            "**Summary of Score Impact:**\n",
            "No changes to the numerical scores for either assistant are proposed. For Anthropic, the data store analysis revealed an additional error (inventory non-update) that reinforces the initial low scoring. For OpenAI, its scores are already at rock bottom; the data store's state is inconsistent with its error message if changes were made by OpenAI in this turn, which would only highlight the severity of its failure.\n",
            "\n",
            "This comparison underscores the importance of atomicity and completeness in transactional operations and accurate reporting of actions to the user. The inventory issue found in the data store would be a critical learning for improving the order placement tool/process.\n",
            "Extracted score 1 for 'Anthropic' using pattern: Anthropic.*?\\bscore\\b.*?(\\d+)\n",
            "Extracted score 1 for 'OpenAI' using pattern: OpenAI.*?\\bscore\\b.*?(\\d+)\n",
            "Do you want to add any general learnings from this turn? (Type your learning or 'skip'): skip\n",
            "\n",
            "Enter your query (or 'quit', 'exit', 'stop', 'q' to end): quit\n",
            "Exiting the system. Goodbye!\n",
            "\n",
            "\n",
            "===== EVALUATION SUMMARY =====\n",
            "\n",
            "Query 1: I'd like to order 25 Perplexinators, please\n",
            "  Anthropic Resp: Great! I've successfully placed your order for 25 Perplexinators. Here's a summary of your order:\n",
            "- ...\n",
            "  OpenAI Resp: Max tool iterations reached for OpenAI....\n",
            "  Scores - Anthropic: 1, OpenAI: 1\n",
            "  Query Winner: Tie\n",
            "\n",
            "----- Overall Performance -----\n",
            "Avg Anthropic: 1.00, Avg OpenAI: 1.00\n",
            "Total Anthropic: 1, Total OpenAI: 1\n",
            "Overall Winner: Tie\n",
            "\n",
            "No human feedback learnings were stored during this session.\n",
            "\n",
            "Execution Finished.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}