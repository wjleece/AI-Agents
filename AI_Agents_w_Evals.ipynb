{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/AI-Agents/blob/main/AI_Agents_w_Evals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install anthropic\n",
        "#%pip install openai\n",
        "%pip install -q -U google-generativeai\n",
        "%pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDKcVFI7PAJ5",
        "outputId": "709e05b3-8c41-46bd-90bd-4674f72d233f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.51.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.11/dist-packages (0.18.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup and Imports\n",
        "import anthropic\n",
        "import google.generativeai as gemini\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import glob # For finding files matching a pattern\n",
        "import uuid # For generating unique learning IDs in RAG\n",
        "from google.colab import userdata\n",
        "#from openai import OpenAI\n",
        "from google.colab import drive # For Google Drive mounting\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union, Tuple\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "# LLM API Keys\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "#OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "#openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "gemini.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "ANTHROPIC_MODEL_NAME = \"claude-3-5-sonnet-latest\"\n",
        "#OPENAI_MODEL_NAME = \"gpt-4.1\" # Or your preferred GPT-4 class model\n",
        "EVAL_MODEL_NAME = \"gemini-2.5-pro-preview-05-06\" # Or your preferred Gemini model\n",
        "CLASSIFIER_MODEL_NAME = \"gemini-1.5-flash-latest\" # Fast model for question classification / routing"
      ],
      "metadata": {
        "id": "RoQ4L2N6o339"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drive Authentication & path mapping\n",
        "\n",
        "DRIVE_MOUNT_PATH = '/content/drive'\n",
        "\n",
        "try:\n",
        "    drive.mount(DRIVE_MOUNT_PATH)\n",
        "    print(f\"Google Drive mounted successfully at {DRIVE_MOUNT_PATH}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}. RAG features will not work.\")\n",
        "\n",
        "# Set up the default learnings path\n",
        "DEFAULT_LEARNINGS_DRIVE_SUBPATH = \"My Drive/AI/Knowledgebases\"  # Your default path\n",
        "LEARNINGS_DRIVE_BASE_PATH = os.path.join(DRIVE_MOUNT_PATH, DEFAULT_LEARNINGS_DRIVE_SUBPATH)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(LEARNINGS_DRIVE_BASE_PATH):\n",
        "    try:\n",
        "        os.makedirs(LEARNINGS_DRIVE_BASE_PATH)\n",
        "        print(f\"Created learnings directory: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating learnings directory {LEARNINGS_DRIVE_BASE_PATH}: {e}\")\n",
        "else:\n",
        "    print(f\"Using existing learnings directory: {LEARNINGS_DRIVE_BASE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VeBrmqG1K-o",
        "outputId": "f64ca8e4-913a-42c1-f54d-6f2d44d6b7c3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully at /content/drive.\n",
            "Using existing learnings directory: /content/drive/My Drive/AI/Knowledgebases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Specialized System Prompts\n",
        "\n",
        "# --- Worker AI Prompts ---\n",
        "worker_base_instructions = \"\"\"\n",
        "You are a helpful customer service assistant for an e-commerce system.\n",
        "Your overriding goal is to be helpful by answering questions and performing actions as requested by a human user.\n",
        "When responding to the user, use the conversation context to maintain continuity.\n",
        "- If a user refers to \"my order\" or similar, use the context to determine which order they're talking about.\n",
        "- If they mention \"that product\" or use other references, check the context to determine what they're referring to.\n",
        "- Always prioritize recent context over older context when resolving references.\n",
        "\n",
        "The conversation context will be provided to you with each message. This includes:\n",
        "- Previous questions and answers\n",
        "- Recently viewed customers, products, and orders\n",
        "- Recent actions taken (like creating orders, updating products, etc.)\n",
        "- Relevant Learnings from a knowledge base (if applicable to the current query type).\n",
        "- **Crucially, results from any tools you use will also be part of the context provided back to you.**\n",
        "\n",
        "**YOUR RESPONSE TO THE HUMAN USER:**\n",
        "Your primary role is to communicate effectively and naturally with the human user.\n",
        "- After you use tools and get their results (which will be shown to you), your final textual response to the user **must be a friendly, conversational, and easy-to-understand summary.**\n",
        "- **DO NOT output raw data (like JSON strings or complex lists/dictionaries from tool results) directly in your response to the user.**\n",
        "- Instead, you must **interpret the tool results** and explain the outcome or provide the requested information in natural language.\n",
        "- For example, if a tool you used returns information like `{\"product_name\": \"Perplexinator\", \"inventory_count\": 1485}`, your response to the user should be something like: \"Currently, we have 1485 Perplexinators in stock.\" or \"The Perplexinator has 1485 units available, would you like to know more?\"\n",
        "- If you perform an action (e.g., creating an order), confirm this action clearly and provide key details in a sentence, for instance: \"I've successfully created your order (Order ID: O4) for 10 Widgets.\"\n",
        "- Always aim to be helpful, polite, and clear in your language.\n",
        "\n",
        "REQUESTING CLARIFICATION FROM THE USER:\n",
        "If you determine that you absolutely need more information from the user to accurately and efficiently fulfill their request or use a tool correctly, you MUST:\n",
        "1. Formulate a clear, concise question for the user.\n",
        "2. Prefix your entire response with the exact tag: `CLARIFICATION_REQUESTED:`\n",
        "   Example: `CLARIFICATION_REQUESTED: To update the order, could you please provide the Order ID?`\n",
        "3. Do NOT use any tools in the same turn you are requesting clarification. Wait for the user's response.\n",
        "\n",
        "Keep all other responses friendly, concise, and helpful.\n",
        "\"\"\"\n",
        "\n",
        "worker_operational_system_prompt = f\"\"\"\n",
        "{worker_base_instructions}\n",
        "\n",
        "Your current task is OPERATIONAL. Focus on understanding user requests related to e-commerce functions (managing orders, products, customers), using the provided tools accurately, and interacting with the data store.\n",
        "The \"Relevant Learnings from Knowledge Base\" provided in your context may contain operational guidelines.\n",
        "Remember to synthesize tool results into a user-friendly textual response. The detailed tool outputs are logged separately for evaluation.\n",
        "\"\"\"\n",
        "\n",
        "worker_metacognitive_learnings_system_prompt = f\"\"\"\n",
        "{worker_base_instructions}\n",
        "\n",
        "Your current task is METACOGNITIVE: SUMMARIZING LEARNINGS AND EXPLAINING YOUR THINKING.\n",
        "If the user asks you to \"summarize your learnings\", \"what have you learned\", \"why did you\", \"is there a better way to\" or similar phrases, your response should be based PRIMARILY on the content provided to you under the heading \"Relevant Learnings from Knowledge Base (In-Session Cache)\" in your current context.\n",
        "- List the key principles or pieces of information from these provided learnings.\n",
        "- Do not confuse these explicit learnings with a general summary of your recent actions or the current state of the data store, unless a learning specifically refers to such an action or state.\n",
        "- If no specific learnings are provided in your context for this type of query, you can state that no specific new learnings have been highlighted for this interaction.\n",
        "- Avoid using tools for this type of summarization unless a tool is specifically designed to retrieve or process learnings.\n",
        "\"\"\"\n",
        "\n",
        "# --- Evaluator AI Prompt (unified but guided by query type information) ---\n",
        "# This prompt is largely the same as the one from worker_prompt_update_learning_summary,\n",
        "# but we will emphasize the query_type in the main prompt to the evaluator.\n",
        "\n",
        "evaluator_system_prompt = \"\"\"\n",
        "You are Google Gemini, an impartial evaluator assessing the quality of responses from an AI assistant to customer service queries.\n",
        "\n",
        "You will be provided with:\n",
        "- The user's query and the TYPE of query it was classified as (e.g., OPERATIONAL, METACOGNITIVE_LEARNINGS_SUMMARY).\n",
        "- The conversation context (including RAG learnings) that was available to the AI assistant.\n",
        "- **The AI assistant's final user-facing textual response.**\n",
        "- **A log of tools called by the AI assistant, including their inputs and raw outputs.**\n",
        "- A snapshot of the 'Data Store State *Before* AI Action'.\n",
        "- A snapshot of the 'Data Store State *After* AI Action'.\n",
        "- Details of any clarification questions the AI assistant asked.\n",
        "\n",
        "Your primary goal is to assess the AI assistant based on the SPECIFIC TASK it was attempting, as indicated by the query type.\n",
        "\n",
        "For each interaction, evaluate the assistant's response based on:\n",
        "1.  **Accuracy**:\n",
        "    * If OPERATIONAL:\n",
        "        * How correct and factual is the AI's **user-facing textual response**?\n",
        "        * Does the textual response accurately reflect the outcomes of the **tool calls** and changes in the datastore?\n",
        "        * Did its actions (tool calls) correctly process information or modify the datastore as intended? Verify against 'Tool Call Log', 'Before' and 'After' states.\n",
        "    * If METACOGNITIVE_LEARNINGS_SUMMARY: Did the AI accurately summarize the \"Relevant Learnings from Knowledge Base\" provided in its context? Was the summary faithful to these learnings?\n",
        "    * Check for new entity IDs and correct updates if applicable to the query type.\n",
        "\n",
        "2.  **Efficiency**:\n",
        "    * Did the assistant achieve its goal with minimal clarifying questions?\n",
        "    * If OPERATIONAL: Were tool calls used appropriately and efficiently? (Refer to 'Tool Call Log')\n",
        "    * If METACOGNITIVE_LEARNINGS_SUMMARY: Was the summary direct and to the point based on provided learnings?\n",
        "\n",
        "3.  **Context Awareness**:\n",
        "    * Did the assistant correctly use the conversation history and entities?\n",
        "    * Crucially, did the assistant adhere to the task defined by the query type?\n",
        "    * Did it correctly use any \"Relevant Learnings from Knowledge Base\" that were pertinent to the query type?\n",
        "    * For OPERATIONAL tasks, did the user-facing response make sense given the tool outputs and datastore changes?\n",
        "\n",
        "4.  **Helpfulness & Clarity (of the user-facing response)**:\n",
        "    * How well did the assistant address the user's needs *for the identified query type* in its textual response?\n",
        "    * Was the **user-facing response clear, polite, and easy to understand?** Did it avoid jargon or raw data dumps?\n",
        "    * Did it provide relevant information in a helpful manner?\n",
        "\n",
        "Score the response on a scale of 1-10 for each criterion, and provide an overall score. Provide detailed reasoning, EXPLICITLY MENTIONING THE QUERY TYPE and referencing the **user-facing text**, the **tool call log**, and **datastore states** as appropriate.\n",
        "- For OPERATIONAL queries, heavily reference the 'Tool Call Log' and 'Before'/'After' data store states when assessing the underlying actions, and then assess if the user-facing text accurately and clearly conveys this.\n",
        "- For METACOGNITIVE_LEARNINGS_SUMMARY, heavily reference the \"Relevant Learnings from Knowledge Base\" that were provided to the worker.\n",
        "\n",
        "EVALUATING CLARIFICATION QUESTIONS:\n",
        "If the worker AI asked for clarification:\n",
        "- Assess necessity using 'Data Store State *Before* AI Action' and context.\n",
        "- If necessary and well-phrased, it should NOT negatively impact Efficiency.\n",
        "- If unnecessary, it SHOULD negatively impact Efficiency.\n",
        "\n",
        "If you, the evaluator, still have questions, use \"CLARIFICATION NEEDED_EVALUATOR:\".\n",
        "\n",
        "DATA STORE CONSISTENCY (Primarily for OPERATIONAL tasks):\n",
        "When assessing Accuracy for OPERATIONAL tasks, explicitly compare the AI's actions (via tool log and datastore changes) with its claims in the user-facing text.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zMWjRsv3QNc1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Data Stores (Initial data - will be managed by the Storage class instance)\n",
        "# These are initial values. The Storage class will manage them.\n",
        "initial_customers = {\n",
        "    \"C1\": {\"name\": \"John Doe\", \"email\": \"john@example.com\", \"phone\": \"123-456-7890\"},\n",
        "    \"C2\": {\"name\": \"Jane Smith\", \"email\": \"jane@example.com\", \"phone\": \"987-654-3210\"}\n",
        "}\n",
        "\n",
        "initial_products = {\n",
        "    \"P1\": {\"name\": \"Widget A\", \"description\": \"A simple widget. Very compact.\", \"price\": 19.99, \"inventory_count\": 999},\n",
        "    \"P2\": {\"name\": \"Gadget B\", \"description\": \"A powerful gadget. It spins.\", \"price\": 49.99, \"inventory_count\": 200},\n",
        "    \"P3\": {\"name\": \"Perplexinator\", \"description\": \"A perplexing perfunctator\", \"price\": 79.99, \"inventory_count\": 1483}\n",
        "}\n",
        "\n",
        "initial_orders = {\n",
        "    \"O1\": {\"id\": \"O1\", \"product_id\": \"P1\", \"product_name\": \"Widget A\", \"quantity\": 2, \"price\": 19.99, \"status\": \"Shipped\"},\n",
        "    \"O2\": {\"id\": \"O2\", \"product_id\": \"P2\", \"product_name\": \"Gadget B\", \"quantity\": 1, \"price\": 49.99, \"status\": \"Processing\"}\n",
        "}\n"
      ],
      "metadata": {
        "id": "5G9rP40vQXdU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standalone Anthropic Completion Function (for basic tests)\n",
        "#def get_completion_anthropic_standalone(prompt: str):\n",
        "#    message = anthropic_client.messages.create(\n",
        "#        model=ANTHROPIC_MODEL_NAME,\n",
        "#        max_tokens=2000,\n",
        "#        temperature=0.0,\n",
        "#        system=worker_base_instructions,\n",
        "#        tools=tools_schemas_list,\n",
        "#        messages=[\n",
        "#          {\"role\": \"user\", \"content\": prompt}\n",
        "#        ]\n",
        "#    )\n",
        "#    return message.content[0].text"
      ],
      "metadata": {
        "id": "_ADM0bBpQlVK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt_test_anthropic = \"Hey there, which AI model do you use for answering questions?\"\n",
        "#print(f\"Anthropic Standalone Test: {get_completion_anthropic_standalone(prompt_test_anthropic)}\")"
      ],
      "metadata": {
        "id": "jPOq1s0SQqk_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def get_completion_openai_standalone(prompt: str):\n",
        "#    response = openai_client.chat.completions.create(\n",
        "#        model=OPENAI_MODEL_NAME,\n",
        "#        max_tokens=2000,\n",
        "#        temperature=0.0,\n",
        "#        tools=tools_schemas_list,\n",
        "#        messages=[\n",
        "#            {\"role\": \"system\", \"content\": worker_system_prompt},\n",
        "#            {\"role\": \"user\", \"content\": prompt}\n",
        "#        ]\n",
        "#    )\n",
        "#    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Y5c_bv3qQwWV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt_test_openai = \"Hey there, which AI model do you use for answering questions?\"\n",
        "#print(f\"OpenAI Standalone Test: {get_completion_openai_standalone(prompt_test_openai)}\")"
      ],
      "metadata": {
        "id": "b_LaDQ74Q1Lp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Storage:\n",
        "    \"\"\"Manages the in-memory e-commerce datastore.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.customers = copy.deepcopy(initial_customers)\n",
        "        self.products = copy.deepcopy(initial_products)\n",
        "        self.orders = copy.deepcopy(initial_orders)\n",
        "        print(\"Storage initialized with deep copies of initial data.\")\n",
        "\n",
        "    def get_full_datastore_copy(self) -> Dict[str, Any]:\n",
        "        \"\"\"Returns a deep copy of the current datastore.\"\"\"\n",
        "        return {\n",
        "            \"customers\": copy.deepcopy(self.customers),\n",
        "            \"products\": copy.deepcopy(self.products),\n",
        "            \"orders\": copy.deepcopy(self.orders)\n",
        "        }"
      ],
      "metadata": {
        "id": "_9XC9xyh3XVE"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definitive list of tool schemas.\n",
        "tools_schemas_list = [\n",
        "    {\n",
        "        \"name\": \"create_customer\",\n",
        "        \"description\": \"Adds a new customer to the database. Includes customer name, email, and (optional) phone number.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The name of the customer.\"},\n",
        "                \"email\": {\"type\": \"string\", \"description\": \"The email address of the customer.\"},\n",
        "                \"phone\": {\"type\": \"string\", \"description\": \"The phone number of the customer (optional).\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"email\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_customer_info\",\n",
        "        \"description\": \"Retrieves customer information based on their customer ID. Returns the customer's name, email, and (optional) phone number.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"customer_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the customer.\"}\n",
        "            },\n",
        "            \"required\": [\"customer_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_product\",\n",
        "        \"description\": \"Adds a new product to the product database. Includes name, description, price, and initial inventory count.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The name of the product.\"},\n",
        "                \"description\": {\"type\": \"string\", \"description\": \"A description of the product.\"},\n",
        "                \"price\": {\"type\": \"number\", \"description\": \"The price of the product.\"},\n",
        "                \"inventory_count\": {\"type\": \"integer\", \"description\": \"The amount of the product that is currently in inventory.\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"description\", \"price\", \"inventory_count\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"update_product\",\n",
        "        \"description\": \"Updates an existing product with new information. Only fields that are provided will be updated; other fields remain unchanged.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the product to update.\"},\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The new name for the product (optional).\"},\n",
        "                \"description\": {\"type\": \"string\", \"description\": \"The new description for the product (optional).\"},\n",
        "                \"price\": {\"type\": \"number\", \"description\": \"The new price for the product (optional).\"},\n",
        "                \"inventory_count\": {\"type\": \"integer\", \"description\": \"The new inventory count for the product (optional).\"}\n",
        "            },\n",
        "            \"required\": [\"product_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_product_info\",\n",
        "        \"description\": \"Retrieves product information based on product ID or product name (with fuzzy matching for misspellings). Returns product details including name, description, price, and inventory count.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id_or_name\": {\"type\": \"string\", \"description\": \"The product ID or name (can be approximate).\"}\n",
        "            },\n",
        "            \"required\": [\"product_id_or_name\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"list_all_products\",\n",
        "        \"description\": \"Lists all available products in the inventory.\",\n",
        "        \"input_schema\": { \"type\": \"object\", \"properties\": {}, \"required\": [] }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_order\",\n",
        "        \"description\": \"Creates an order using the product's current price. If requested quantity exceeds available inventory, no order is created and available quantity is returned. Orders can only be created for products that are in stock. Supports specifying products by either ID or name with fuzzy matching for misspellings.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id_or_name\": {\"type\": \"string\", \"description\": \"The ID or name of the product to order (supports fuzzy matching).\"},\n",
        "                \"quantity\": {\"type\": \"integer\", \"description\": \"The quantity of the product in the order.\"},\n",
        "                \"status\": {\"type\": \"string\", \"description\": \"The initial status of the order (e.g., 'Processing', 'Shipped').\"}\n",
        "            },\n",
        "            \"required\": [\"product_id_or_name\", \"quantity\", \"status\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_order_details\",\n",
        "        \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"order_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the order.\"}\n",
        "            },\n",
        "            \"required\": [\"order_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"update_order_status\",\n",
        "        \"description\": \"Updates the status of an order and adjusts inventory accordingly. Changing to \\\"Shipped\\\" decreases inventory. Changing to \\\"Returned\\\" or \\\"Canceled\\\" from \\\"Shipped\\\" increases inventory. Status can be \\\"Processing\\\", \\\"Shipped\\\", \\\"Delivered\\\", \\\"Returned\\\", or \\\"Canceled\\\".\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"order_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the order.\"},\n",
        "                \"new_status\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The new status to set for the order.\",\n",
        "                    \"enum\": [\"Processing\", \"Shipped\", \"Delivered\", \"Returned\", \"Canceled\"]\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"order_id\", \"new_status\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "print(f\"Defined {len(tools_schemas_list)} tool schemas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PREbUQfh2UBj",
        "outputId": "e39690d6-8813-47c2-f087-a149af2f62e1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined 9 tool schemas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tool Functions (Global for now, passed to ToolExecutor) ---\n",
        "\n",
        "def create_customer(current_storage: Storage, name: str, email: str, phone: Optional[str] = None): # Simplified for brevity\n",
        "    new_id = f\"C{len(current_storage.customers) + 1}\"\n",
        "    while new_id in current_storage.customers: new_id = f\"C{int(new_id[1:]) + 1}\"\n",
        "    current_storage.customers[new_id] = {\"name\": name, \"email\": email, \"phone\": phone}\n",
        "    print(f\"[Tool Exec] create_customer: ID {new_id}\")\n",
        "    return {\"status\": \"success\", \"customer_id\": new_id, \"customer\": current_storage.customers[new_id]}\n",
        "def get_customer_info(current_storage: Storage, customer_id: str):\n",
        "    customer = current_storage.customers.get(customer_id)\n",
        "    if customer: print(f\"[Tool Exec] get_customer_info: ID {customer_id} found.\"); return {\"status\": \"success\", \"customer\": customer}\n",
        "    print(f\"[Tool Exec] get_customer_info: ID {customer_id} not found.\"); return {\"status\": \"error\", \"message\": \"Customer not found\"}\n",
        "def create_product(current_storage: Storage, name: str, description: str, price: float, inventory_count: int):\n",
        "    new_id = f\"P{len(current_storage.products) + 1}\"\n",
        "    while new_id in current_storage.products: new_id = f\"P{int(new_id[1:]) + 1}\"\n",
        "    current_storage.products[new_id] = {\"name\": name, \"description\": description, \"price\": price, \"inventory_count\": inventory_count}\n",
        "    print(f\"[Tool Exec] create_product: ID {new_id}\")\n",
        "    return {\"status\": \"success\", \"product_id\": new_id, \"product\": current_storage.products[new_id]}\n",
        "def update_product(current_storage: Storage, product_id: str, name: Optional[str]=None, description: Optional[str]=None, price: Optional[float]=None, inventory_count: Optional[int]=None):\n",
        "    if product_id not in current_storage.products: print(f\"[Tool Exec] update_product: ID {product_id} not found.\"); return {\"status\": \"error\", \"message\": \"Product not found\"}\n",
        "    product = current_storage.products[product_id]; updated_fields = []\n",
        "    if name: product[\"name\"] = name; updated_fields.append(\"name\")\n",
        "    if description: product[\"description\"] = description; updated_fields.append(\"description\")\n",
        "    if price: product[\"price\"] = price; updated_fields.append(\"price\")\n",
        "    if inventory_count is not None : product[\"inventory_count\"] = inventory_count; updated_fields.append(\"inventory_count\")\n",
        "    if not updated_fields: print(f\"[Tool Exec] update_product: ID {product_id}, no fields updated.\"); return {\"status\":\"warning\", \"message\":\"No fields updated.\"}\n",
        "    print(f\"[Tool Exec] update_product: ID {product_id}, Updated: {', '.join(updated_fields)}\")\n",
        "    return {\"status\": \"success\", \"product_id\": product_id, \"product\": product, \"updated_fields\": updated_fields}\n",
        "def find_product_by_name(current_storage: Storage, product_name: str, min_similarity: int = 70) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
        "    if not product_name: return None, None\n",
        "    name_id_list = [(p_data[\"name\"], p_id) for p_id, p_data in current_storage.products.items()]\n",
        "    if not name_id_list: return None, None\n",
        "    best_match_name_score = process.extractOne(product_name, [item[0] for item in name_id_list], scorer=fuzz.token_sort_ratio)\n",
        "    if best_match_name_score and best_match_name_score[1] >= min_similarity:\n",
        "        matched_name = best_match_name_score[0]\n",
        "        for name_val, pid_val in name_id_list:\n",
        "            if name_val == matched_name: return pid_val, current_storage.products[pid_val]\n",
        "    return None, None\n",
        "def get_product_id(current_storage: Storage, product_identifier: str) -> Optional[str]:\n",
        "    if product_identifier in current_storage.products: return product_identifier\n",
        "    product_id, _ = find_product_by_name(current_storage, product_identifier)\n",
        "    return product_id\n",
        "def get_product_info(current_storage: Storage, product_id_or_name: str):\n",
        "    pid = get_product_id(current_storage, product_id_or_name)\n",
        "    if pid and pid in current_storage.products:\n",
        "        print(f\"[Tool Exec] get_product_info: Found '{product_id_or_name}' as ID '{pid}'.\")\n",
        "        return {\"status\": \"success\", \"product_id\": pid, \"product\": current_storage.products[pid]}\n",
        "    print(f\"[Tool Exec] get_product_info: Product '{product_id_or_name}' not found.\")\n",
        "    return {\"status\": \"error\", \"message\": f\"Product '{product_id_or_name}' not found\"}\n",
        "def list_all_products(current_storage: Storage):\n",
        "    print(f\"[Tool Exec] list_all_products: Found {len(current_storage.products)} products.\")\n",
        "    return {\"status\": \"success\", \"count\": len(current_storage.products), \"products\": current_storage.products}\n",
        "def create_order(current_storage: Storage, product_id_or_name: str, quantity: int, status: str):\n",
        "    actual_product_id = get_product_id(current_storage, product_id_or_name)\n",
        "    if not actual_product_id: print(f\"[Tool Exec] create_order: Product '{product_id_or_name}' not found.\"); return {\"status\": \"error\", \"message\": f\"Product '{product_id_or_name}' not found.\"}\n",
        "    product = current_storage.products[actual_product_id]\n",
        "    if product[\"inventory_count\"] < quantity and status == \"Shipped\": print(f\"[Tool Exec] create_order: Insufficient inventory for {product['name']} to ship.\"); return {\"status\": \"error\", \"message\": f\"Insufficient inventory for {product['name']} to ship. Available: {product['inventory_count']}\"}\n",
        "    if status == \"Shipped\" and product[\"inventory_count\"] >= quantity : product[\"inventory_count\"] -= quantity\n",
        "    new_id = f\"O{len(current_storage.orders) + 1}\"\n",
        "    while new_id in current_storage.orders: new_id = f\"O{int(new_id[1:]) + 1}\"\n",
        "    current_storage.orders[new_id] = {\"id\": new_id, \"product_id\": actual_product_id, \"product_name\": product[\"name\"], \"quantity\": quantity, \"price\": product[\"price\"], \"status\": status}\n",
        "    print(f\"[Tool Exec] create_order: Order {new_id} for {product['name']}. Status: {status}. Inv: {product['inventory_count']}\")\n",
        "    return {\"status\": \"success\", \"order_id\": new_id, \"order_details\": current_storage.orders[new_id], \"remaining_inventory\": product[\"inventory_count\"]}\n",
        "def get_order_details(current_storage: Storage, order_id: str):\n",
        "    order = current_storage.orders.get(order_id)\n",
        "    if order: print(f\"[Tool Exec] get_order_details: Order {order_id} found.\"); return {\"status\": \"success\", \"order_details\": order}\n",
        "    print(f\"[Tool Exec] get_order_details: Order {order_id} not found.\"); return {\"status\": \"error\", \"message\": \"Order not found\"}\n",
        "def update_order_status(current_storage: Storage, order_id: str, new_status: str):\n",
        "    if order_id not in current_storage.orders: print(f\"[Tool Exec] update_order_status: Order {order_id} not found.\"); return {\"status\": \"error\", \"message\": \"Order not found\"}\n",
        "    order = current_storage.orders[order_id]; product_id = order[\"product_id\"]; quantity = order[\"quantity\"]\n",
        "    product = current_storage.products[product_id]; old_status = order[\"status\"]\n",
        "    if old_status == new_status: print(f\"[Tool Exec] update_order_status: Status for {order_id} already {new_status}.\"); return {\"status\": \"unchanged\", \"message\": \"Status is already \" + new_status}\n",
        "    inventory_adjusted = False\n",
        "    if new_status == \"Shipped\" and old_status != \"Shipped\":\n",
        "        if product[\"inventory_count\"] < quantity: print(f\"[Tool Exec] update_order_status: Insufficient inv for {product_id} to ship order {order_id}.\"); return {\"status\": \"error\", \"message\": \"Insufficient inventory to ship.\"}\n",
        "        product[\"inventory_count\"] -= quantity; inventory_adjusted = True\n",
        "    elif old_status == \"Shipped\" and new_status != \"Shipped\": # e.g. returned/canceled from shipped\n",
        "        product[\"inventory_count\"] += quantity; inventory_adjusted = True\n",
        "    order[\"status\"] = new_status\n",
        "    print(f\"[Tool Exec] update_order_status: Order {order_id} to {new_status}. Inv for {product_id} is {product['inventory_count']}. Adjusted: {inventory_adjusted}\")\n",
        "    return {\"status\": \"success\", \"order_id\": order_id, \"order_details\": order, \"current_inventory\": product[\"inventory_count\"], \"inventory_adjusted\": inventory_adjusted}\n",
        "print(\"Tool functions defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGGSeN7T2W1O",
        "outputId": "113bf3a5-97a7-4a45-f571-6301eb3118eb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryClassifier:\n",
        "    \"\"\"Classifies user queries using an LLM.\"\"\"\n",
        "    def __init__(self, llm_client):\n",
        "        self.llm_client = llm_client\n",
        "        self.classification_prompt_template = \"\"\"\n",
        "Classify the following user query into one of these categories: OPERATIONAL, METACOGNITIVE_LEARNINGS_SUMMARY.\n",
        "Return ONLY the category name.\n",
        "\n",
        "OPERATIONAL queries are about performing e-commerce tasks, like asking about products, creating orders, or updating customer information.\n",
        "Examples of OPERATIONAL:\n",
        "- \"Show me all shoes.\"\n",
        "- \"What's the price of P1?\"\n",
        "- \"Create an order for 2 widgets.\"\n",
        "- \"Update my address.\"\n",
        "\n",
        "METACOGNITIVE_LEARNINGS_SUMMARY queries are about the AI's own learning process or knowledge derived from feedback.\n",
        "Examples of METACOGNITIVE_LEARNINGS_SUMMARY:\n",
        "- \"Summarize your learnings.\"\n",
        "- \"What have you learned recently?\"\n",
        "- \"Tell me about your new knowledge.\"\n",
        "- \"Why did you do that in the last turn?\"\n",
        "- \"Is there a better way to handle X?\"\n",
        "\n",
        "User Query: \"{user_message}\"\n",
        "Classification:\"\"\"\n",
        "        print(\"QueryClassifier initialized with LLM client.\")\n",
        "\n",
        "    def classify(self, user_message: str) -> str:\n",
        "        \"\"\"Classifies the user query using the LLM.\"\"\"\n",
        "        prompt = self.classification_prompt_template.format(user_message=user_message)\n",
        "        try:\n",
        "            response = self.llm_client.generate_content(prompt)\n",
        "            classification = response.text.strip()\n",
        "            if classification in [\"OPERATIONAL\", \"METACOGNITIVE_LEARNINGS_SUMMARY\"]:\n",
        "                return classification\n",
        "            else:\n",
        "                print(f\"[QueryClassifier Warning] LLM returned unexpected classification: '{classification}'. Defaulting to OPERATIONAL.\")\n",
        "                return \"OPERATIONAL\"\n",
        "        except Exception as e:\n",
        "            print(f\"[QueryClassifier Error] Failed to classify query using LLM: {e}. Defaulting to OPERATIONAL.\")\n",
        "            return \"OPERATIONAL\""
      ],
      "metadata": {
        "id": "Qxvm6ah2z1wO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationManager:\n",
        "    \"\"\"Manages conversation history and context data.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.messages: List[Dict[str, Any]] = []\n",
        "        self.context_data: Dict[str, Any] = {\n",
        "            \"customers\": {}, \"products\": {}, \"orders\": {}, \"last_action\": None\n",
        "        }\n",
        "        print(\"ConversationManager initialized.\")\n",
        "\n",
        "    def add_user_message(self, message: str) -> None:\n",
        "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    def add_assistant_message(self, message_content: Union[str, List[Dict[str, Any]]], query_type: str) -> None:\n",
        "        if isinstance(message_content, str):\n",
        "            content_to_log = f\"[{query_type}]: {message_content}\"\n",
        "        else:\n",
        "            content_to_log = message_content\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": content_to_log})\n",
        "\n",
        "    def update_entity_in_context(self, entity_type: str, entity_id: str, data: Any) -> None:\n",
        "        if entity_type in self.context_data:\n",
        "            self.context_data[entity_type][entity_id] = data\n",
        "            print(f\"[CM_Context Updated] Entity: {entity_type}, ID: {entity_id}\")\n",
        "\n",
        "    def set_last_action(self, action_type: str, action_details: Any) -> None:\n",
        "        self.context_data[\"last_action\"] = {\n",
        "            \"type\": action_type,\n",
        "            \"details\": action_details,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        print(f\"[CM_Context Updated] Last Action: {action_type}\")\n",
        "\n",
        "    def get_full_conversation_for_api(self) -> List[Dict[str, Any]]:\n",
        "        return self.messages.copy()\n",
        "\n",
        "    def get_context_summary(self) -> str:\n",
        "        summary_parts = []\n",
        "        if self.context_data[\"customers\"]: summary_parts.append(f\"Recent customers: {list(self.context_data['customers'].keys())}\")\n",
        "        if self.context_data[\"products\"]: summary_parts.append(f\"Recent products: {list(self.context_data['products'].keys())}\")\n",
        "        if self.context_data[\"orders\"]: summary_parts.append(f\"Recent orders: {list(self.context_data['orders'].keys())}\")\n",
        "        if self.context_data[\"last_action\"]: summary_parts.append(f\"Last action type: {self.context_data['last_action']['type']}\")\n",
        "        return \"\\\\n\".join(summary_parts) if summary_parts else \"No specific context items set yet.\"\n"
      ],
      "metadata": {
        "id": "uhTMNPio0BbT"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToolExecutor:\n",
        "    def __init__(self, available_tools_dict: Dict[str, callable]):\n",
        "        self.available_tools = available_tools_dict\n",
        "        print(\"ToolExecutor initialized.\")\n",
        "\n",
        "    def execute_tool(self, tool_name: str, tool_input: Dict[str, Any], storage_instance: Storage) -> Dict[str, Any]:\n",
        "        if tool_name in self.available_tools:\n",
        "            try:\n",
        "                tool_function = self.available_tools[tool_name]\n",
        "                result = tool_function(storage_instance, **tool_input)\n",
        "                print(f\"--- [ToolExecutor] Result for {tool_name}: {json.dumps(result, indent=2, default=str)} ---\")\n",
        "                return result\n",
        "            except Exception as e:\n",
        "                print(f\"--- [ToolExecutor Error] executing {tool_name}: {e} ---\"); import traceback; traceback.print_exc()\n",
        "                return {\"status\": \"error\", \"message\": f\"Error executing tool {tool_name}: {str(e)}\"}\n",
        "        print(f\"--- [ToolExecutor Error] Tool {tool_name} not found. ---\")\n",
        "        return {\"status\": \"error\", \"message\": f\"Tool {tool_name} not found.\"}"
      ],
      "metadata": {
        "id": "Qxo-Ho_R0VNy"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original Cell 18\n",
        "class KnowledgeManager:\n",
        "    def __init__(self, base_path: str, drive_mount_path: str, default_subpath: str, evaluator_llm_instance):\n",
        "        self.base_drive_path = base_path\n",
        "        self.drive_mount_path = drive_mount_path\n",
        "        self.default_drive_subpath = default_subpath\n",
        "        self.evaluator_llm = evaluator_llm_instance\n",
        "        self.active_learnings_cache: List[Dict] = self._load_initial_learnings_from_drive()\n",
        "        self.learnings_updated_this_session_flag: bool = False\n",
        "        print(f\"KnowledgeManager initialized. Loaded {len(self.active_learnings_cache)} initial learnings from {self.base_drive_path}.\")\n",
        "\n",
        "    def _mount_drive_if_needed(self):\n",
        "        if not os.path.exists(self.drive_mount_path) or not os.listdir(self.drive_mount_path):\n",
        "            try:\n",
        "                drive.mount(self.drive_mount_path, force_remount=True)\n",
        "                print(\"Drive mounted by KnowledgeManager.\")\n",
        "            except Exception as e:\n",
        "                print(f\"KM: Error mounting Drive: {e}.\")\n",
        "\n",
        "    def _initialize_learnings_path(self):\n",
        "        if not os.path.exists(self.base_drive_path):\n",
        "            try:\n",
        "                os.makedirs(self.base_drive_path)\n",
        "                print(f\"KM: Created learnings directory: {self.base_drive_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"KM: Error creating learnings directory {self.base_drive_path}: {e}\")\n",
        "\n",
        "    def _get_latest_learnings_filepath(self) -> Optional[str]:\n",
        "        self._mount_drive_if_needed()\n",
        "        self._initialize_learnings_path()\n",
        "        if not os.path.isdir(self.base_drive_path):\n",
        "            return None\n",
        "        list_of_files = glob.glob(os.path.join(self.base_drive_path, 'learnings_*.json'))\n",
        "        return max(list_of_files, key=os.path.getctime) if list_of_files else None\n",
        "\n",
        "    def _read_learnings_from_file(self, filepath: str) -> List[Dict]:\n",
        "        if not filepath or not os.path.exists(filepath):\n",
        "            return []\n",
        "        try:\n",
        "            with open(filepath, 'r') as f:\n",
        "                learnings_list = json.load(f)\n",
        "            return learnings_list if isinstance(learnings_list, list) else []\n",
        "        except Exception as e:\n",
        "            print(f\"KM: Error reading/decoding {filepath}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _load_initial_learnings_from_drive(self) -> List[Dict]:\n",
        "        latest_filepath = self._get_latest_learnings_filepath()\n",
        "        if latest_filepath:\n",
        "            print(f\"KM: Loading initial learnings from: {latest_filepath}\")\n",
        "            return self._read_learnings_from_file(latest_filepath)\n",
        "        print(\"KM: No existing learnings file found for initial load.\")\n",
        "        return []\n",
        "\n",
        "    def persist_active_learnings(self):\n",
        "        self._mount_drive_if_needed()\n",
        "        self._initialize_learnings_path()\n",
        "        if not os.path.isdir(self.base_drive_path):\n",
        "            print(\"KM CRITICAL: Learnings directory not available.\")\n",
        "            return\n",
        "        if not self.active_learnings_cache:\n",
        "            print(\"KM: Active learnings cache is empty or unchanged. Nothing to persist.\")\n",
        "            return\n",
        "\n",
        "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
        "        new_filepath = os.path.join(self.base_drive_path, f'learnings_{timestamp_str}.json')\n",
        "        try:\n",
        "            with open(new_filepath, 'w') as f:\n",
        "                json.dump(self.active_learnings_cache, f, indent=4)\n",
        "            print(f\"KM: Persisted {len(self.active_learnings_cache)} active learnings to: {new_filepath}\")\n",
        "            self.learnings_updated_this_session_flag = False\n",
        "        except Exception as e:\n",
        "            print(f\"KM: Error persisting active learnings to {new_filepath}: {e}\")\n",
        "\n",
        "    def get_relevant_learnings_for_prompt(self, query: str, query_type: str,\n",
        "                                           # Parameter name changed for clarity:\n",
        "                                           recipient_role: Optional[str] = None,\n",
        "                                           count: int = 5) -> str:\n",
        "        if not self.active_learnings_cache:\n",
        "            return \"No specific relevant learnings from knowledge base provided for this query.\"\n",
        "\n",
        "        # 1. Filter by recipient_role first\n",
        "        eligible_learnings: List[Dict] = []\n",
        "        if recipient_role == \"Agent\": # Agent gets \"AgentAndEvaluator\"\n",
        "            eligible_learnings = [\n",
        "                entry for entry in self.active_learnings_cache\n",
        "                if entry.get(\"learning_target\") == \"AgentAndEvaluator\"\n",
        "            ]\n",
        "            # print(f\"KM: For Agent, found {len(eligible_learnings)} 'AgentAndEvaluator' learnings before keyword filtering.\")\n",
        "        elif recipient_role == \"Evaluator\": # Evaluator gets \"AgentAndEvaluator\" OR \"EvaluatorOnly\"\n",
        "            eligible_learnings = [\n",
        "                entry for entry in self.active_learnings_cache\n",
        "                if entry.get(\"learning_target\") in [\"AgentAndEvaluator\", \"EvaluatorOnly\"]\n",
        "            ]\n",
        "            # print(f\"KM: For Evaluator, found {len(eligible_learnings)} 'AgentAndEvaluator' or 'EvaluatorOnly' learnings before keyword filtering.\")\n",
        "        else: # Default: if no role specified, or unknown role, consider all learnings that are not EvaluatorOnly\n",
        "              # This ensures general learnings are picked up if role isn't strictly Agent.\n",
        "              # Or, to be more restrictive, you could default to only \"AgentAndEvaluator\" or empty.\n",
        "            # print(f\"KM: No specific recipient_role or unknown role '{recipient_role}'. Considering 'AgentAndEvaluator' learnings.\")\n",
        "            eligible_learnings = [\n",
        "                entry for entry in self.active_learnings_cache\n",
        "                if entry.get(\"learning_target\") == \"AgentAndEvaluator\" # Default to what an agent would see\n",
        "            ]\n",
        "\n",
        "\n",
        "        # 2. Apply query_type and keyword-based filtering on the eligible_learnings\n",
        "        learnings_to_consider = []\n",
        "        if query_type == \"METACOGNITIVE_LEARNINGS_SUMMARY\":\n",
        "            # For summary, show all eligible learnings based on role, up to a limit, newest first.\n",
        "            learnings_to_consider = sorted(eligible_learnings, key=lambda x: x.get('timestamp_created', ''), reverse=True)\n",
        "        elif query_type == \"OPERATIONAL\":\n",
        "            if not eligible_learnings: # No role-specific learnings to filter with keywords\n",
        "                 return \"No specific relevant learnings from knowledge base found for this query based on role.\"\n",
        "            keywords = self._extract_keywords(query)\n",
        "            # print(f\"KM: Extracted keywords for operational query: {keywords}\")\n",
        "            learnings_to_consider = [\n",
        "                entry for entry in eligible_learnings # Filter from already role-filtered list\n",
        "                if any(kw.lower() in (entry.get(\"final_learning_statement\", \"\") + \" \" + \" \".join(entry.get(\"keywords\", []))).lower() for kw in keywords)\n",
        "            ]\n",
        "            learnings_to_consider.sort(key=lambda x: x.get('timestamp_created', ''), reverse=True)\n",
        "        else: # Default for other query types or if no specific logic\n",
        "            learnings_to_consider = sorted(eligible_learnings, key=lambda x: x.get('timestamp_created', ''), reverse=True)\n",
        "\n",
        "        # 3. Apply count limit\n",
        "        final_learnings_for_prompt = learnings_to_consider[:count]\n",
        "        # print(f\"KM: Final {len(final_learnings_for_prompt)} learnings for prompt (Recipient: {recipient_role}, QueryType: {query_type})\")\n",
        "\n",
        "        formatted_learnings = [\n",
        "            f\"- Learning (ID: {entry.get('learning_id', 'N/A')[:8]}, Target: {entry.get('learning_target', 'N/A')}): {entry.get('final_learning_statement', str(entry))}\"\n",
        "            for entry in final_learnings_for_prompt\n",
        "        ]\n",
        "\n",
        "        if not formatted_learnings:\n",
        "            return \"No specific relevant learnings from knowledge base found for this query after all filters.\"\n",
        "        return \"\\\\nRelevant Learnings from Knowledge Base (In-Session Cache):\\\\n\" + \"\\\\n\".join(formatted_learnings)\n",
        "\n",
        "    def _extract_keywords(self, text: str) -> List[str]:\n",
        "        if not text:\n",
        "            return [\"general\"]\n",
        "        words = re.findall(r'\\\\b\\\\w{4,}\\\\b', text.lower())\n",
        "        stop_words = {\n",
        "            \"the\", \"and\", \"is\", \"in\", \"to\", \"a\", \"of\", \"for\", \"with\", \"on\", \"at\", \"an\", \"by\", \"not\", \"or\", \"as\", \"if\",\n",
        "            \"it\", \"its\", \"this\", \"that\", \"these\", \"those\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
        "            \"do\", \"does\", \"did\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\",\n",
        "            \"what\", \"who\", \"whom\", \"which\", \"when\", \"where\", \"why\", \"how\",\n",
        "            \"show\", \"tell\", \"please\", \"user\", \"query\", \"context\", \"claude\", \"anthropic\", \"gemini\", \"google\",\n",
        "            \"before\", \"after\", \"state\", \"action\", \"truth\", \"ground\", \"learnings\", \"summarize\", \"your\", \"you\", \"me\", \"my\",\n",
        "            \"provide\", \"give\", \"make\", \"update\", \"create\", \"get\", \"list\", \"item\", \"items\", \"detail\", \"details\"\n",
        "        }\n",
        "        extracted = list(set(word for word in words if word not in stop_words and not word.isdigit()))\n",
        "        return extracted if extracted else [\"generic\"]\n",
        "\n",
        "    def synthesize_and_store_learning(self, human_feedback_text: str, user_query_context: str, turn_context_summary: str, learning_target: str):\n",
        "        print(f\"--- KM: Processing New Learning Candidate (Target: {learning_target}): \\\"{human_feedback_text}\\\" ---\")\n",
        "\n",
        "        current_feedback_to_process = human_feedback_text\n",
        "        attempt_count = 0\n",
        "        max_attempts = 3\n",
        "\n",
        "        while attempt_count < max_attempts:\n",
        "            attempt_count += 1\n",
        "            print(f\"KM: Learning Synthesis Attempt {attempt_count}/{max_attempts}\")\n",
        "\n",
        "            evaluator_task_prompt_parts = [\n",
        "                \"You are an AI assistant helping to maintain a knowledge base of 'learnings' from human feedback.\",\n",
        "                f\"The human feedback is targeted towards: {learning_target}.\",\n",
        "                f\"New Human Feedback to process: \\\"{current_feedback_to_process}\\\"\",\n",
        "                f\"Original User Query that led to this feedback: \\\"{user_query_context}\\\"\",\n",
        "                f\"General Conversation Context when feedback was given: \\\"{turn_context_summary}\\\"\",\n",
        "                \"Existing ACTIVE learnings (sample of last 3, if any):\" + \"\".join([f\"  - (ID: {entry.get('learning_id','N/A')[:6]}, Target: {entry.get('learning_target', 'N/A')}) {entry.get('final_learning_statement', '')[:100]}...\" for entry in self.active_learnings_cache[-3:]]) if self.active_learnings_cache else \"  - No existing learnings in cache.\",\n",
        "                \"Your Tasks:\",\n",
        "                \"1. Analyze the 'New Human Feedback'.\",\n",
        "                \"2. Check for CONFLICT or significant REDUNDANCY with existing learnings. Consider general knowledge principles and the stated target of the learnings.\",\n",
        "                \"3. If the feedback is new, valuable, non-conflicting, and non-redundant, synthesize it into a concise, actionable 'Finalized Learning Statement'. This statement should be generalizable if possible.\",\n",
        "                \"Output Format Instructions:\",\n",
        "                \"- If suitable for storing: `FINALIZED_LEARNING: [synthesized statement]`\",\n",
        "                \"- If it conflicts: `CONFLICT_DETECTED: [Explanation of the conflict, and if possible, reference key phrases or IDs of conflicting existing learnings]. Proposed statement if you tried to rephrase: [rephrased statement, or original if no rephrase attempt]`\",\n",
        "                \"- If it's redundant: `REDUNDANT_LEARNING: [Explanation of redundancy, and if possible, reference key phrases or IDs of the existing learning it's redundant with]. Proposed statement if you tried to rephrase: [rephrased statement, or original if no rephrase attempt]`\",\n",
        "                \"- If not actionable/too vague: `NOT_ACTIONABLE: [Explanation]`\",\n",
        "                \"Ensure your entire response strictly follows one of these prefixed formats.\"\n",
        "            ]\n",
        "            synthesis_prompt = \"\\\\n\".join(evaluator_task_prompt_parts)\n",
        "\n",
        "            try:\n",
        "                synthesis_response_obj = self.evaluator_llm.generate_content(synthesis_prompt)\n",
        "                evaluator_synthesis_text = synthesis_response_obj.text.strip()\n",
        "                print(f\"KM: Gemini Learning Synthesis Raw Response:\\\\n{evaluator_synthesis_text}\")\n",
        "\n",
        "                final_statement = None\n",
        "                conflict_explanation = None\n",
        "                redundant_explanation = None\n",
        "                not_actionable_explanation = None\n",
        "\n",
        "                if evaluator_synthesis_text.startswith(\"FINALIZED_LEARNING:\"):\n",
        "                    final_statement = evaluator_synthesis_text.replace(\"FINALIZED_LEARNING:\", \"\", 1).strip()\n",
        "                elif evaluator_synthesis_text.startswith(\"CONFLICT_DETECTED:\"):\n",
        "                    conflict_explanation = evaluator_synthesis_text.replace(\"CONFLICT_DETECTED:\", \"\", 1).strip()\n",
        "                elif evaluator_synthesis_text.startswith(\"REDUNDANT_LEARNING:\"):\n",
        "                    redundant_explanation = evaluator_synthesis_text.replace(\"REDUNDANT_LEARNING:\", \"\", 1).strip()\n",
        "                elif evaluator_synthesis_text.startswith(\"NOT_ACTIONABLE:\"):\n",
        "                    not_actionable_explanation = evaluator_synthesis_text.replace(\"NOT_ACTIONABLE:\", \"\", 1).strip()\n",
        "                else:\n",
        "                    print(\"KM: Gemini learning synthesis response format unexpected. Defaulting to not actionable.\")\n",
        "                    not_actionable_explanation = f\"Response format error: {evaluator_synthesis_text}\"\n",
        "\n",
        "                if final_statement:\n",
        "                    self.active_learnings_cache.append({\n",
        "                        \"learning_id\": str(uuid.uuid4()),\n",
        "                        \"timestamp_created\": datetime.now().isoformat(),\n",
        "                        \"original_human_input\": human_feedback_text,\n",
        "                        \"processed_human_input\": current_feedback_to_process,\n",
        "                        \"final_learning_statement\": final_statement,\n",
        "                        \"keywords\": self._extract_keywords(final_statement + \" \" + current_feedback_to_process),\n",
        "                        \"status\": \"active\",\n",
        "                        \"learning_target\": learning_target\n",
        "                    })\n",
        "                    self.learnings_updated_this_session_flag = True\n",
        "                    print(f\"KM: Stored new learning. Cache size: {len(self.active_learnings_cache)}\")\n",
        "                    return\n",
        "\n",
        "                elif conflict_explanation:\n",
        "                    print(f\"KM: Learning conflict detected by Gemini: {conflict_explanation}\")\n",
        "                    if attempt_count < max_attempts:\n",
        "                        print(\"KM: --- CONFLICT RESOLUTION ---\")\n",
        "                        print(f\"Original feedback: '{human_feedback_text}'\")\n",
        "                        print(f\"Feedback being processed: '{current_feedback_to_process}'\")\n",
        "                        user_choice = input(\"Conflict detected. (M)odify your feedback, (S)kip storing, or (P)roceed with current version for resynthesis? [M/S/P]: \").strip().upper()\n",
        "                        if user_choice == 'M':\n",
        "                            new_feedback = input(\"Enter your modified feedback: \").strip()\n",
        "                            if new_feedback:\n",
        "                                current_feedback_to_process = new_feedback\n",
        "                                print(\"KM: Retrying synthesis with modified feedback.\")\n",
        "                                continue\n",
        "                            else:\n",
        "                                print(\"KM: No modification provided. Skipping.\")\n",
        "                                return\n",
        "                        elif user_choice == 'P':\n",
        "                            print(\"KM: User chose to proceed. Retrying synthesis with current feedback version.\")\n",
        "                            continue\n",
        "                        else:\n",
        "                            print(\"KM: Skipping this learning due to unresolved conflict.\")\n",
        "                            return\n",
        "                    else:\n",
        "                        print(\"KM: Max attempts reached for conflict resolution. Skipping this learning.\")\n",
        "                        return\n",
        "\n",
        "                elif redundant_explanation:\n",
        "                    print(f\"KM: Learning deemed redundant by Gemini: {redundant_explanation}\")\n",
        "                    user_choice_redundant = input(\"This learning seems redundant. (S)kip storing, or (F)orce store anyway? [S/F]: \").strip().upper()\n",
        "                    if user_choice_redundant == 'F':\n",
        "                        proposed_statement_match = re.search(r\"Proposed statement.*?:\\s*(.*)\", redundant_explanation, re.IGNORECASE)\n",
        "                        if proposed_statement_match and proposed_statement_match.group(1).strip():\n",
        "                            forced_statement = proposed_statement_match.group(1).strip()\n",
        "                            print(f\"KM: Using LLM's proposed statement due to Force: '{forced_statement}'\")\n",
        "                        else:\n",
        "                            forced_statement = current_feedback_to_process\n",
        "                            print(f\"KM: No specific proposed statement from LLM. Using current feedback for Force: '{forced_statement}'\")\n",
        "\n",
        "                        self.active_learnings_cache.append({\n",
        "                            \"learning_id\": str(uuid.uuid4()),\n",
        "                            \"timestamp_created\": datetime.now().isoformat(),\n",
        "                            \"original_human_input\": human_feedback_text,\n",
        "                            \"processed_human_input\": current_feedback_to_process,\n",
        "                            \"final_learning_statement\": forced_statement,\n",
        "                            \"keywords\": self._extract_keywords(forced_statement + \" \" + current_feedback_to_process),\n",
        "                            \"status\": \"active_forced_redundancy\",\n",
        "                            \"learning_target\": learning_target,\n",
        "                            \"notes\": f\"Forced storage despite redundancy. Original LLM note: {redundant_explanation}\"\n",
        "                        })\n",
        "                        self.learnings_updated_this_session_flag = True\n",
        "                        print(f\"KM: Stored learning (forced despite redundancy). Cache size: {len(self.active_learnings_cache)}\")\n",
        "                        return\n",
        "                    else:\n",
        "                        print(\"KM: Skipping redundant learning.\")\n",
        "                        return\n",
        "\n",
        "                elif not_actionable_explanation:\n",
        "                    print(f\"KM: Learning deemed not actionable by Gemini: {not_actionable_explanation}\")\n",
        "                    print(\"KM: Skipping this learning.\")\n",
        "                    return\n",
        "\n",
        "                else:\n",
        "                    print(\"KM: Synthesis resulted in an unhandled state. Skipping.\")\n",
        "                    return\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"KM: Error during learning synthesis attempt {attempt_count}: {e}\")\n",
        "                import traceback; traceback.print_exc()\n",
        "                if attempt_count >= max_attempts:\n",
        "                    print(\"KM: Max attempts reached due to errors. Skipping this learning.\")\n",
        "                    return\n",
        "                time.sleep(1)\n",
        "\n",
        "        print(\"KM: Could not synthesize learning after maximum attempts. Skipping.\")"
      ],
      "metadata": {
        "id": "IuqeYwg80Ydj"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WorkerAgentHandler:\n",
        "    def __init__(self, llm_client, tool_schemas: List[Dict], tool_executor: ToolExecutor, storage_instance: Storage):\n",
        "        self.llm_client = llm_client\n",
        "        self.tool_schemas = tool_schemas\n",
        "        self.tool_executor = tool_executor\n",
        "        self.storage = storage_instance\n",
        "        print(\"WorkerAgentHandler initialized.\")\n",
        "\n",
        "    def _execute_llm_interaction_loop(self, system_prompt: str, messages_for_api: List[Dict[str, Any]], query_type: str, conversation_manager: ConversationManager) -> Tuple[str, List[Dict]]:\n",
        "        tools_for_this_call = self.tool_schemas if query_type == \"OPERATIONAL\" else []\n",
        "        max_iterations = 5 if query_type == \"OPERATIONAL\" else 1 # Max tool use iterations for operational, 1 for others\n",
        "\n",
        "        executed_tool_calls_log: List[Dict] = [] # Log for tool calls in this interaction loop\n",
        "\n",
        "        for i in range(max_iterations):\n",
        "            print(f\"--- WorkerLLM Calling Anthropic (Iter {i+1}/{max_iterations}, QType: {query_type}) ---\")\n",
        "            current_text_response = \"\" # Initialize for this iteration\n",
        "            try:\n",
        "                response = self.llm_client.messages.create(\n",
        "                    model=ANTHROPIC_MODEL_NAME,\n",
        "                    max_tokens=4000,\n",
        "                    temperature=0.0,\n",
        "                    system=system_prompt,\n",
        "                    tools=tools_for_this_call,\n",
        "                    messages=messages_for_api\n",
        "                )\n",
        "            except Exception as e:\n",
        "                error_message = f\"Error communicating with Worker LLM: {e}\"\n",
        "                print(f\"LLM API Error: {e}\")\n",
        "                return error_message, executed_tool_calls_log # Return error and any logs so far\n",
        "\n",
        "            assistant_response_blocks = response.content\n",
        "            # It's important to add the raw assistant response blocks to the API history\n",
        "            # This includes text parts and tool_use parts if any.\n",
        "            messages_for_api.append({\"role\": \"assistant\", \"content\": assistant_response_blocks})\n",
        "\n",
        "            text_blocks = [block.text for block in assistant_response_blocks if block.type == \"text\"]\n",
        "            current_text_response = \" \".join(text_blocks).strip()\n",
        "\n",
        "            if current_text_response.startswith(\"CLARIFICATION_REQUESTED:\"):\n",
        "                return current_text_response, executed_tool_calls_log # Return immediately for clarification\n",
        "\n",
        "            tool_calls_to_process = [block for block in assistant_response_blocks if block.type == \"tool_use\"]\n",
        "\n",
        "            if not tool_calls_to_process or query_type != \"OPERATIONAL\":\n",
        "                # If no tools to call, or not an operational query, this is the final response from the LLM for this loop.\n",
        "                final_response_text = current_text_response if current_text_response else \"Worker AI provided no text content in its final turn.\"\n",
        "                return final_response_text, executed_tool_calls_log\n",
        "\n",
        "            # If there are tool calls to process (and it's an OPERATIONAL query)\n",
        "            tool_results_for_next_llm_call_content = [] # This will be the content for the next \"user\" role message (tool results)\n",
        "\n",
        "            for tool_use_block in tool_calls_to_process:\n",
        "                tool_name, tool_input, tool_use_id = tool_use_block.name, tool_use_block.input, tool_use_block.id\n",
        "                print(f\"WorkerLLM: Requesting Tool Call: {tool_name}, Input: {tool_input}\")\n",
        "\n",
        "                # Execute the tool\n",
        "                tool_result_data = self.tool_executor.execute_tool(tool_name, tool_input, self.storage)\n",
        "\n",
        "                # Log the tool call and its result for the orchestrator/evaluator\n",
        "                executed_tool_calls_log.append({\n",
        "                    \"tool_name\": tool_name,\n",
        "                    \"tool_input\": copy.deepcopy(tool_input), # Deepcopy to avoid modification issues\n",
        "                    \"tool_output\": copy.deepcopy(tool_result_data)\n",
        "                })\n",
        "\n",
        "                # Update conversation manager's context (this was already here)\n",
        "                # Example: update context based on product/order/customer IDs in tool_result_data\n",
        "                entity_type_map = {\n",
        "                    \"order_details\": \"orders\", \"order_id\": \"orders\",\n",
        "                    \"product\": \"products\", \"product_id\": \"products\",\n",
        "                    \"customer\": \"customers\", \"customer_id\": \"customers\"\n",
        "                }\n",
        "                found_entity_type = \"unknown\"\n",
        "                found_entity_id = \"unknown_id\"\n",
        "                found_entity_data = tool_result_data\n",
        "\n",
        "                for key, etype in entity_type_map.items():\n",
        "                    if key in tool_result_data and tool_result_data[key]:\n",
        "                        found_entity_type = etype\n",
        "                        if isinstance(tool_result_data[key], dict) and (\"id\" in tool_result_data[key] or etype[:-1]+\"_id\" in tool_result_data[key]): # e.g. order_details might have 'id'\n",
        "                             found_entity_id = tool_result_data[key].get(\"id\") or tool_result_data[key].get(etype[:-1]+\"_id\")\n",
        "                             found_entity_data = tool_result_data[key]\n",
        "                        elif isinstance(tool_result_data.get(etype[:-1]+\"_id\"), str): # e.g. direct product_id\n",
        "                            found_entity_id = tool_result_data.get(etype[:-1]+\"_id\")\n",
        "                        break # Take first match for simplicity\n",
        "\n",
        "                # Try to get ID more robustly if it's directly in tool_result_data\n",
        "                if found_entity_id == \"unknown_id\":\n",
        "                     if \"order_id\" in tool_result_data: found_entity_id = tool_result_data[\"order_id\"]\n",
        "                     elif \"product_id\" in tool_result_data: found_entity_id = tool_result_data[\"product_id\"]\n",
        "                     elif \"customer_id\" in tool_result_data: found_entity_id = tool_result_data[\"customer_id\"]\n",
        "\n",
        "                if found_entity_id != \"unknown_id\":\n",
        "                    conversation_manager.update_entity_in_context(\n",
        "                        entity_type=found_entity_type,\n",
        "                        entity_id=found_entity_id,\n",
        "                        data=found_entity_data\n",
        "                    )\n",
        "                conversation_manager.set_last_action(f\"tool_{tool_name}_Anthropic\", {\"input\": tool_input, \"result_summary\": tool_result_data.get(\"status\", \"unknown_status\")})\n",
        "\n",
        "                tool_results_for_next_llm_call_content.append({\n",
        "                    \"type\": \"tool_result\",\n",
        "                    \"tool_use_id\": tool_use_id,\n",
        "                    \"content\": json.dumps(tool_result_data) if isinstance(tool_result_data, dict) else str(tool_result_data)\n",
        "                    # Consider adding an error field from tool_result_data if status is error\n",
        "                    # \"is_error\": tool_result_data.get(\"status\") == \"error\" if isinstance(tool_result_data, dict) else False\n",
        "                })\n",
        "\n",
        "            # Add the aggregated tool results as a new \"user\" message for the next LLM call\n",
        "            if tool_results_for_next_llm_call_content:\n",
        "                messages_for_api.append({\"role\": \"user\", \"content\": tool_results_for_next_llm_call_content})\n",
        "            else: # Should not happen if tool_calls_to_process was non-empty\n",
        "                print(\"WorkerLLM: No tool results to append, though tool calls were present. This is unexpected.\")\n",
        "\n",
        "\n",
        "        # If loop finishes (max_iterations reached)\n",
        "        final_response_text = current_text_response if current_text_response else \"Worker AI reached max tool iterations without a final text response.\"\n",
        "        return final_response_text, executed_tool_calls_log"
      ],
      "metadata": {
        "id": "XYs7E68G0qBP"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResponseEvaluator:\n",
        "    def __init__(self, evaluator_llm_instance):\n",
        "        self.evaluator_llm = evaluator_llm_instance # Gemini model\n",
        "        print(\"ResponseEvaluator initialized.\")\n",
        "\n",
        "    def evaluate_turn(self, user_message: str, query_type: str, worker_response_text: str,\n",
        "                      context_summary: str, rag_learnings_provided: str,\n",
        "                      clarification_interactions: Optional[List[Dict]],\n",
        "                      initial_datastore_state: Dict[str, Any],\n",
        "                      final_datastore_state: Dict[str, Any],\n",
        "                      executed_tool_calls_log: List[Dict]) -> Dict[str, Any]: # Added tool calls log\n",
        "\n",
        "        initial_ds_prompt = f\"Data Store State *Before* AI Action:\\\\n{json.dumps(initial_datastore_state, indent=2, default=str)}\"\n",
        "        final_ds_prompt = f\"Data Store State *After* AI Action:\\\\n{json.dumps(final_datastore_state, indent=2, default=str)}\"\n",
        "\n",
        "        clarification_info_prompt = \"No worker AI clarifications this turn.\"\n",
        "        if clarification_interactions:\n",
        "            clar_summary = [f\"  Q: '{c.get('agent_question', 'N/A')}' -> User A: '{c.get('user_answer', 'N/A')}'\" for c in clarification_interactions]\n",
        "            clarification_info_prompt = f\"Worker AI Clarification Interactions:\\\\n\" + \"\\\\n\".join(clar_summary)\n",
        "\n",
        "        tool_log_prompt = \"No tools were executed by the Worker AI this turn.\"\n",
        "        if executed_tool_calls_log:\n",
        "            formatted_tool_calls = []\n",
        "            for i, call in enumerate(executed_tool_calls_log):\n",
        "                # Limit the size of tool output in the prompt to avoid excessive length\n",
        "                output_summary = call.get('tool_output', {})\n",
        "                if isinstance(output_summary, dict):\n",
        "                    output_summary_str = json.dumps({k: v for k, v in output_summary.items() if k != \"products\"}, indent=1, default=str) # Exclude full product lists from summary\n",
        "                    if len(output_summary_str) > 300: # Truncate if still too long\n",
        "                        output_summary_str = output_summary_str[:300] + \"...\"\n",
        "                else:\n",
        "                    output_summary_str = str(output_summary)\n",
        "                    if len(output_summary_str) > 300:\n",
        "                         output_summary_str = output_summary_str[:300] + \"...\"\n",
        "\n",
        "                formatted_tool_calls.append(\n",
        "                    f\"  Tool Call {i+1}:\\n\"\n",
        "                    f\"    Name: {call.get('tool_name')}\\n\"\n",
        "                    f\"    Input: {json.dumps(call.get('tool_input'), default=str)}\\n\"\n",
        "                    f\"    Output Summary: {output_summary_str}\"\n",
        "                )\n",
        "            tool_log_prompt = f\"Worker AI Tool Calls Executed This Turn:\\\\n\" + \"\\\\n\".join(formatted_tool_calls)\n",
        "\n",
        "        eval_content_prompt = f\"\"\"\n",
        "User query: {user_message}\n",
        "Classified Query Type: {query_type}\n",
        "\n",
        "Context provided to assistant (summary):\n",
        "{context_summary}\n",
        "\n",
        "Relevant RAG Learnings provided to assistant:\n",
        "{rag_learnings_provided}\n",
        "\n",
        "{initial_ds_prompt}\n",
        "\n",
        "{tool_log_prompt}\n",
        "\n",
        "{final_ds_prompt}\n",
        "\n",
        "{clarification_info_prompt}\n",
        "\n",
        "Worker AI (Claude) final textual response:\n",
        "{worker_response_text}\n",
        "---\n",
        "INSTRUCTIONS FOR EVALUATOR (You are Gemini {EVAL_MODEL_NAME}):\n",
        "Based on your system prompt (which emphasizes impartiality and detailed assessment criteria) and the classified query type ({query_type}), please evaluate the AI assistant's response.\n",
        "- If OPERATIONAL, focus on tool use accuracy (refer to 'Worker AI Tool Calls Executed'), data store changes (Before vs. After), and whether the final textual response aligns with these actions.\n",
        "- If METACOGNITIVE_LEARNINGS_SUMMARY, focus on whether the AI accurately summarized the 'Relevant RAG Learnings' it was provided.\n",
        "Provide detailed reasoning for scores (Accuracy, Efficiency, Context Awareness, Helpfulness) and an overall score (1-10).\n",
        "Explicitly reference the tool call log and datastore states when assessing operational tasks.\n",
        "\"\"\"\n",
        "        try:\n",
        "            # Assuming self.evaluator_llm is a GenerativeModel instance configured with the evaluator_system_prompt\n",
        "            gemini_response_obj = self.evaluator_llm.generate_content(eval_content_prompt)\n",
        "            evaluation_text = gemini_response_obj.text\n",
        "\n",
        "            score = self._extract_score(evaluation_text) # Overall score\n",
        "            # You might want to extract individual criteria scores too if needed later.\n",
        "\n",
        "            return {\n",
        "                \"anthropic_score\": score, # This is the overall score for the worker AI\n",
        "                \"full_evaluation\": evaluation_text, # This is Gemini's full textual evaluation\n",
        "                \"clarification_details_evaluator\": {\"used\": False}, # Placeholder, as evaluator doesn't ask for clarification in this setup\n",
        "                \"query_type_evaluated\": query_type,\n",
        "                \"raw_evaluation_text\": evaluation_text # For direct printing\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Evaluator: Error during Gemini evaluation: {e}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"anthropic_score\": 0,\n",
        "                \"full_evaluation\": f\"Evaluation failed: {e}\",\n",
        "                \"clarification_details_evaluator\": {},\n",
        "                \"query_type_evaluated\": query_type,\n",
        "                \"raw_evaluation_text\": f\"Evaluation Error: {e}\"\n",
        "            }\n",
        "\n",
        "    def _extract_score(self, evaluation_text: str) -> int:\n",
        "        # More robust score extraction, looking for \"Overall Score: X/10\" or \"Overall Score: X\"\n",
        "        # Prefers scores out of 10 if specified.\n",
        "        patterns = [\n",
        "            r\"Overall Score\\s*:\\s*(\\d{1,2})\\s*/\\s*10\",  # \"Overall Score : X/10\" or \"Overall Score:X/10\"\n",
        "            r\"Overall Score\\s*:\\s*(\\d{1,2})\",         # \"Overall Score : X\" or \"Overall Score:X\"\n",
        "            r\"Overall\\s*:\\s*(\\d{1,2})\\s*/\\s*10\",       # \"Overall : X/10\"\n",
        "            r\"Overall\\s*:\\s*(\\d{1,2})\"                # \"Overall : X\"\n",
        "        ]\n",
        "        for p_str in patterns:\n",
        "            matches = list(re.finditer(p_str, evaluation_text, re.IGNORECASE | re.DOTALL))\n",
        "            if matches:\n",
        "                try:\n",
        "                    score_value = int(matches[-1].group(1))\n",
        "                    if 0 <= score_value <= 10: # Ensure score is within expected range\n",
        "                        return score_value\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "        print(f\"Evaluator: Could not reliably extract a 0-10 score from text: ...{evaluation_text[-300:]}\")\n",
        "        return 0 # Default score if not found or invalid"
      ],
      "metadata": {
        "id": "ZFpBL_-HSGPY"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentOrchestrator:\n",
        "    def __init__(self):\n",
        "        # ... (initialization remains the same) ...\n",
        "        self.classifier_llm_client = gemini.GenerativeModel(model_name=CLASSIFIER_MODEL_NAME)\n",
        "        self.query_classifier = QueryClassifier(llm_client=self.classifier_llm_client)\n",
        "        self.storage = Storage()\n",
        "        self.conversation_manager = ConversationManager()\n",
        "        self.tool_functions_map = {\n",
        "            \"create_customer\": create_customer, \"get_customer_info\": get_customer_info,\n",
        "            \"create_product\": create_product, \"update_product\": update_product,\n",
        "            \"get_product_info\": get_product_info, \"list_all_products\": list_all_products,\n",
        "            \"create_order\": create_order, \"get_order_details\": get_order_details,\n",
        "            \"update_order_status\": update_order_status,\n",
        "        }\n",
        "        self.tool_executor = ToolExecutor(self.tool_functions_map)\n",
        "\n",
        "        knowledge_synthesis_llm = gemini.GenerativeModel(model_name=EVAL_MODEL_NAME)\n",
        "        self.knowledge_manager = KnowledgeManager(LEARNINGS_DRIVE_BASE_PATH, DRIVE_MOUNT_PATH, DEFAULT_LEARNINGS_DRIVE_SUBPATH, knowledge_synthesis_llm)\n",
        "\n",
        "        self.worker_agent_handler = WorkerAgentHandler(anthropic_client, tools_schemas_list, self.tool_executor, self.storage)\n",
        "\n",
        "        main_evaluator_llm = gemini.GenerativeModel(model_name=EVAL_MODEL_NAME, system_instruction=evaluator_system_prompt)\n",
        "        self.response_evaluator = ResponseEvaluator(evaluator_llm_instance=main_evaluator_llm)\n",
        "\n",
        "        self.evaluation_results_log: List[Dict] = []\n",
        "        print(\"AgentOrchestrator initialized.\")\n",
        "\n",
        "    def _handle_worker_clarification_interaction(self, agent_question_text: str, system_prompt: str,\n",
        "                                                current_turn_history: List[Dict], query_type: str,\n",
        "                                                conversation_manager: ConversationManager,\n",
        "                                                max_attempts: int = 2) -> Tuple[str, List[Dict], List[Dict]]:\n",
        "        # ... (this method remains the same as the last correct version) ...\n",
        "        clarification_interactions = []\n",
        "        response_text = agent_question_text\n",
        "        executed_tool_calls_log_clarification_phase: List[Dict] = []\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            actual_question = response_text.split(\"CLARIFICATION_REQUESTED:\", 1)[-1].strip() if \"CLARIFICATION_REQUESTED:\" in response_text else response_text\n",
        "            print(f\"--- Worker AI requests clarification: {actual_question} ---\")\n",
        "\n",
        "            user_clarification = \"\"\n",
        "            try:\n",
        "                user_clarification = input(f\"Your response to Worker AI: \").strip()\n",
        "            except EOFError:\n",
        "                print(\"EOFError encountered during input. Assuming no user clarification.\")\n",
        "                user_clarification = \"(User provided no further input due to EOF)\"\n",
        "\n",
        "            if not user_clarification and not user_clarification.startswith(\"(User provided no further input\"):\n",
        "                 user_clarification = \"(User provided no further input)\"\n",
        "\n",
        "            clarification_interactions.append({\"agent_question\": actual_question, \"user_answer\": user_clarification})\n",
        "            current_turn_history.append({\"role\": \"user\", \"content\": user_clarification})\n",
        "\n",
        "            response_text, tools_log_this_iteration = self.worker_agent_handler._execute_llm_interaction_loop(\n",
        "                system_prompt, current_turn_history, query_type, conversation_manager\n",
        "            )\n",
        "            executed_tool_calls_log_clarification_phase.extend(tools_log_this_iteration)\n",
        "\n",
        "            if not response_text.startswith(\"CLARIFICATION_REQUESTED:\"):\n",
        "                return response_text, clarification_interactions, executed_tool_calls_log_clarification_phase\n",
        "\n",
        "        print(\"Max clarification attempts reached for worker AI.\")\n",
        "        return response_text, clarification_interactions, executed_tool_calls_log_clarification_phase\n",
        "\n",
        "\n",
        "    def run_agent_turn(self, user_message: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Handles the agent's processing of a query, up to generating its response.\n",
        "        Does NOT perform evaluation or solicit feedback on agent's response.\n",
        "        \"\"\"\n",
        "        self.conversation_manager.add_user_message(user_message)\n",
        "        query_type = self.query_classifier.classify(user_message)\n",
        "        print(f\"--- Orchestrator: Classified Query Type: {query_type} ---\")\n",
        "\n",
        "        context_summary_for_worker = self.conversation_manager.get_context_summary()\n",
        "        rag_learnings_for_worker = self.knowledge_manager.get_relevant_learnings_for_prompt(\n",
        "            user_message, query_type, recipient_role=\"Agent\"\n",
        "        )\n",
        "\n",
        "        full_worker_prompt = (\n",
        "            f\"{worker_operational_system_prompt if query_type == 'OPERATIONAL' else worker_metacognitive_learnings_system_prompt}\\\\n\\\\n\"\n",
        "            f\"Conversation Context Summary (recent entities and last action):\\\\n{context_summary_for_worker}\\\\n\\\\n\"\n",
        "            f\"{rag_learnings_for_worker}\"\n",
        "        )\n",
        "\n",
        "        initial_datastore_state = self.storage.get_full_datastore_copy()\n",
        "        current_turn_processing_history = self.conversation_manager.get_full_conversation_for_api()\n",
        "\n",
        "        worker_response_text, executed_tool_calls_log = self.worker_agent_handler._execute_llm_interaction_loop(\n",
        "            full_worker_prompt, current_turn_processing_history, query_type, self.conversation_manager\n",
        "        )\n",
        "\n",
        "        clarification_interactions = []\n",
        "        if worker_response_text.startswith(\"CLARIFICATION_REQUESTED:\"):\n",
        "            worker_response_text, clarification_interactions, tools_during_clarif = self._handle_worker_clarification_interaction(\n",
        "                worker_response_text, full_worker_prompt, current_turn_processing_history, query_type, self.conversation_manager\n",
        "            )\n",
        "            executed_tool_calls_log.extend(tools_during_clarif)\n",
        "\n",
        "        self.conversation_manager.add_assistant_message(worker_response_text, query_type)\n",
        "        final_datastore_state = self.storage.get_full_datastore_copy() # Capture after agent's actions\n",
        "\n",
        "        return {\n",
        "            \"user_message\": user_message, # Pass through for context\n",
        "            \"query_type\": query_type,\n",
        "            \"anthropic_response\": worker_response_text,\n",
        "            \"executed_tool_calls\": executed_tool_calls_log,\n",
        "            \"context_summary_for_worker\": context_summary_for_worker,\n",
        "            \"initial_datastore_state\": initial_datastore_state,\n",
        "            \"final_datastore_state\": final_datastore_state,\n",
        "            \"clarification_interactions\": clarification_interactions\n",
        "        }\n",
        "\n",
        "    def handle_feedback_on_worker_response(self, original_user_query: str,\n",
        "                                           context_summary_for_worker: str,\n",
        "                                           human_feedback_on_worker: str):\n",
        "        # ... (this method remains the same as the last correct version, uses target \"AgentAndEvaluator\") ...\n",
        "        if human_feedback_on_worker.lower() not in ['skip', ''] and human_feedback_on_worker:\n",
        "            self.knowledge_manager.synthesize_and_store_learning(\n",
        "                human_feedback_on_worker,\n",
        "                original_user_query,\n",
        "                context_summary_for_worker,\n",
        "                learning_target=\"AgentAndEvaluator\"\n",
        "            )\n",
        "            if self.knowledge_manager.learnings_updated_this_session_flag:\n",
        "                self.knowledge_manager.persist_active_learnings()\n",
        "        else:\n",
        "            print(\"Orchestrator: No feedback provided for worker response or 'skip' entered.\")\n",
        "\n",
        "\n",
        "    def run_evaluation_turn(self, agent_turn_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Performs evaluation of the agent's turn using the latest RAG learnings.\n",
        "        \"\"\"\n",
        "        user_message = agent_turn_data[\"user_message\"]\n",
        "        query_type = agent_turn_data[\"query_type\"]\n",
        "        worker_response_text = agent_turn_data[\"anthropic_response\"]\n",
        "        context_summary_for_worker = agent_turn_data[\"context_summary_for_worker\"] # This is worker's context\n",
        "        clarification_interactions = agent_turn_data[\"clarification_interactions\"]\n",
        "        initial_datastore_state = agent_turn_data[\"initial_datastore_state\"]\n",
        "        final_datastore_state = agent_turn_data[\"final_datastore_state\"]\n",
        "        executed_tool_calls_log = agent_turn_data[\"executed_tool_calls\"]\n",
        "\n",
        "        # CRITICAL: Fetch RAG for Evaluator *NOW*, after agent feedback might have been processed\n",
        "        rag_learnings_for_evaluator = self.knowledge_manager.get_relevant_learnings_for_prompt(\n",
        "            user_message, query_type, recipient_role=\"Evaluator\"\n",
        "        )\n",
        "\n",
        "        evaluation_result = self.response_evaluator.evaluate_turn(\n",
        "            user_message, query_type, worker_response_text,\n",
        "            context_summary_for_worker, # Context summary that was available to worker\n",
        "            rag_learnings_for_evaluator, # Freshly fetched RAG for evaluator\n",
        "            clarification_interactions,\n",
        "            initial_datastore_state,\n",
        "            final_datastore_state,\n",
        "            executed_tool_calls_log\n",
        "        )\n",
        "\n",
        "        # Log the evaluation result\n",
        "        self.evaluation_results_log.append({\n",
        "            \"user_message\": user_message,\n",
        "            \"query_type\": query_type,\n",
        "            \"worker_response\": worker_response_text, # From agent_turn_data\n",
        "            \"tool_calls\": copy.deepcopy(executed_tool_calls_log), # From agent_turn_data\n",
        "            \"evaluation_details\": evaluation_result\n",
        "        })\n",
        "\n",
        "        return evaluation_result # Return just the evaluation details\n",
        "\n",
        "    def handle_feedback_on_evaluation(self, original_user_query: str, worker_response_summary: str,\n",
        "                                      evaluation_text_summary: str, human_feedback_on_evaluator: str):\n",
        "        # ... (this method remains the same as the last correct version, uses target \"EvaluatorOnly\") ...\n",
        "        if human_feedback_on_evaluator.lower() not in ['skip', ''] and human_feedback_on_evaluator:\n",
        "            eval_feedback_context = (\n",
        "                f\"Feedback is on an evaluation. Original query: '{original_user_query}'. \"\n",
        "                f\"Worker response (summary): '{worker_response_summary[:100]}...'. \"\n",
        "                f\"Evaluation (summary): '{evaluation_text_summary[:150]}...'\"\n",
        "            )\n",
        "            self.knowledge_manager.synthesize_and_store_learning(\n",
        "                human_feedback_on_evaluator,\n",
        "                original_user_query,\n",
        "                eval_feedback_context,\n",
        "                learning_target=\"EvaluatorOnly\"\n",
        "            )\n",
        "            if self.knowledge_manager.learnings_updated_this_session_flag:\n",
        "                self.knowledge_manager.persist_active_learnings()\n",
        "        else:\n",
        "            print(\"Orchestrator: No feedback provided for evaluator or 'skip' entered.\")\n",
        "\n",
        "\n",
        "    def get_evaluation_log(self) -> List[Dict]:\n",
        "        return self.evaluation_results_log\n",
        "\n",
        "    def persist_learnings_on_exit(self):\n",
        "        if self.knowledge_manager.learnings_updated_this_session_flag:\n",
        "            print(\"Orchestrator: Persisting any remaining updated learnings on exit...\")\n",
        "            self.knowledge_manager.persist_active_learnings()"
      ],
      "metadata": {
        "id": "UzLHm8MmSOMC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original Cell 22 (main function) - MODIFICATIONS\n",
        "\n",
        "def main():\n",
        "    print(\"\\\\nStarting Main Execution with REFACTORED Agent System...\\\\n\")\n",
        "    orchestrator = AgentOrchestrator()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            print(\"\\\\n\" + \"=\" * 70)\n",
        "            user_query = input(\"Enter query (or 'quit' to exit): \").strip()\n",
        "\n",
        "            if user_query.lower() == 'quit':\n",
        "                orchestrator.persist_learnings_on_exit()\n",
        "                print(\"Exiting system. Learnings persisted if updated.\")\n",
        "                break\n",
        "            if not user_query:\n",
        "                continue\n",
        "\n",
        "            print(f\">>> User Query: '{user_query}'\")\n",
        "\n",
        "            # === AGENT ACTION PHASE ===\n",
        "            agent_turn_data = orchestrator.run_agent_turn(user_query)\n",
        "\n",
        "            worker_response_text = agent_turn_data.get('anthropic_response', \"No worker response found.\")\n",
        "            query_type_from_turn = agent_turn_data.get('query_type', \"N/A\")\n",
        "            context_for_worker_feedback = agent_turn_data.get('context_summary_for_worker', \"N/A\")\n",
        "\n",
        "            print(f\"\\\\n--- Worker AI Final Response (Type: {query_type_from_turn}) ---\")\n",
        "            print(worker_response_text)\n",
        "            print(\"--- End of Worker AI Response ---\")\n",
        "\n",
        "            # === FEEDBACK ON AGENT PHASE ===\n",
        "            try:\n",
        "                human_feedback_on_worker = input(\"Orchestrator: Feedback on Worker AI's response? (type or 'skip'): \").strip()\n",
        "                orchestrator.handle_feedback_on_worker_response(\n",
        "                    original_user_query=user_query, # user_query is the original query for this turn\n",
        "                    context_summary_for_worker=context_for_worker_feedback,\n",
        "                    human_feedback_on_worker=human_feedback_on_worker\n",
        "                )\n",
        "            except EOFError:\n",
        "                print(\"Orchestrator: Skipping feedback on worker response (EOF).\")\n",
        "\n",
        "            # === EVALUATION PHASE ===\n",
        "            # Pass the full agent_turn_data to the evaluation method\n",
        "            evaluation_details = orchestrator.run_evaluation_turn(agent_turn_data)\n",
        "\n",
        "            raw_eval_text_for_feedback = \"No evaluation text.\"\n",
        "            if evaluation_details and not evaluation_details.get(\"error\"):\n",
        "                raw_eval_text_for_feedback = evaluation_details.get(\"raw_evaluation_text\", \"No raw evaluation text found.\")\n",
        "                print(f\"\\\\n--- Evaluator (Gemini) Assessment (for query: '{user_query}') ---\")\n",
        "                print(raw_eval_text_for_feedback)\n",
        "                print(\"--- End of Evaluation ---\")\n",
        "            elif evaluation_details and evaluation_details.get(\"error\"):\n",
        "                error_message = evaluation_details.get('error', 'Unknown evaluation error.')\n",
        "                raw_eval_text_for_feedback = f\"Evaluation Error: {error_message}\"\n",
        "                print(f\"\\\\n--- Evaluator (Gemini) Assessment Error ---\")\n",
        "                print(raw_eval_text_for_feedback)\n",
        "                print(\"--- End of Evaluation Error ---\")\n",
        "            else:\n",
        "                print(\"\\\\n--- Evaluator: No evaluation details found for this turn. ---\")\n",
        "\n",
        "            # === FEEDBACK ON EVALUATION PHASE ===\n",
        "            try:\n",
        "                human_feedback_on_evaluator = input(\"Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): \").strip()\n",
        "                orchestrator.handle_feedback_on_evaluation(\n",
        "                    original_user_query=user_query, # user_query for context\n",
        "                    worker_response_summary=worker_response_text, # worker's actual response\n",
        "                    evaluation_text_summary=raw_eval_text_for_feedback, # evaluator's actual assessment text\n",
        "                    human_feedback_on_evaluator=human_feedback_on_evaluator\n",
        "                )\n",
        "            except EOFError:\n",
        "                print(\"Orchestrator: Skipping feedback on evaluator assessment (EOF).\")\n",
        "\n",
        "        except SystemExit: # ... (rest of the exception handling and final summary remains the same) ...\n",
        "            print(\"System exit requested.\")\n",
        "            orchestrator.persist_learnings_on_exit()\n",
        "            break\n",
        "        except EOFError:\n",
        "            print(\"\\\\nEOF encountered. Exiting gracefully.\")\n",
        "            orchestrator.persist_learnings_on_exit()\n",
        "            break\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\\\nKeyboard interrupt detected. Exiting.\")\n",
        "            orchestrator.persist_learnings_on_exit()\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL ERROR in main loop: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # --- Final Evaluation Summary ---\n",
        "    # ... (this part remains the same) ...\n",
        "    print(\"\\\\n\" + \"=\" * 30 + \" FINAL EVALUATION SUMMARY \" + \"=\" * 30)\n",
        "    results_log = orchestrator.get_evaluation_log()\n",
        "    total_score, num_q_evaluated = 0, 0\n",
        "\n",
        "    if not results_log:\n",
        "        print(\"No queries were processed and evaluated in this session.\")\n",
        "    else:\n",
        "        for i, turn_data in enumerate(results_log): # results_log now directly contains dicts from append\n",
        "            user_msg = turn_data.get('user_message', 'N/A')\n",
        "            q_type = turn_data.get('query_type', 'N/A')\n",
        "            # The structure of results_log was: list of dicts where each dict has \"evaluation_details\"\n",
        "            eval_details_from_log = turn_data.get('evaluation_details', {})\n",
        "\n",
        "            if isinstance(eval_details_from_log, dict) and not eval_details_from_log.get(\"error\"):\n",
        "                score = eval_details_from_log.get('anthropic_score', 0)\n",
        "                total_score += score\n",
        "                num_q_evaluated += 1\n",
        "                print(f\"Q{i+1}: '{user_msg}' (Type: {q_type}) -> Score: {score}\")\n",
        "            elif isinstance(eval_details_from_log, dict) and eval_details_from_log.get(\"error\"):\n",
        "                print(f\"Q{i+1}: '{user_msg}' (Type: {q_type}) -> Evaluation Error: {eval_details_from_log.get('error')}\")\n",
        "            else:\n",
        "                # This case might occur if run_evaluation_turn somehow didn't produce a dict.\n",
        "                print(f\"Q{i+1}: '{user_msg}' (Type: {q_type}) -> No valid evaluation details logged or evaluation error.\")\n",
        "\n",
        "\n",
        "    if num_q_evaluated > 0:\n",
        "        print(f\"\\\\nAverage Score over {num_q_evaluated} evaluated queries: {total_score / num_q_evaluated:.2f}\")\n",
        "    else:\n",
        "        print(\"\\\\nNo queries were successfully evaluated to calculate an average score.\")\n",
        "    print(f\"Total aggregate score for the session: {total_score}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Execution Finished.\")\n",
        "\n",
        "# To run:\n",
        "# main()"
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {
        "id": "O2-ztJ2BO7gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Sample queries:\n",
        "* Show me all the products available\n",
        "* I'd like to order 25 Perplexinators, please\n",
        "* Show me the status of my order\n",
        "* (If the order is not in Shipped state, then) Please ship my order now\n",
        "* How many Perplexinators are now left in stock?\n",
        "* Add a new customer: Bill Leece, bill.leece@mail.com, +1.222.333.4444\n",
        "* Add new new product: Gizmo X, description: A fancy gizmo, price: 29.99, inventory: 50\n",
        "* Update Gizzmo's price to 99.99 #Note the misspelling of 'Gizmo'\n",
        "* When was the last time the Toronto Maple Leafs won the Stanley Cup?\n",
        "* I need to update our insurance policy, so I need to know the total value of all the products in our inventory. Please tell me this amount.\n",
        "* Summarize your learnings from our recent interactions.\n",
        "\"\"\"\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dg85ek6bSvW5",
        "outputId": "377c5804-9529-461b-f00d-b078dda6bba0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\nStarting Main Execution with REFACTORED Agent System...\\n\n",
            "QueryClassifier initialized with LLM client.\n",
            "Storage initialized with deep copies of initial data.\n",
            "ConversationManager initialized.\n",
            "ToolExecutor initialized.\n",
            "KM: Loading initial learnings from: /content/drive/My Drive/AI/Knowledgebases/learnings_20250520_180459_428842.json\n",
            "KnowledgeManager initialized. Loaded 1 initial learnings from /content/drive/My Drive/AI/Knowledgebases.\n",
            "WorkerAgentHandler initialized.\n",
            "ResponseEvaluator initialized.\n",
            "AgentOrchestrator initialized.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): Show me all the products available\n",
            ">>> User Query: 'Show me all the products available'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "WorkerLLM: Requesting Tool Call: list_all_products, Input: {}\n",
            "[Tool Exec] list_all_products: Found 3 products.\n",
            "--- [ToolExecutor] Result for list_all_products: {\n",
            "  \"status\": \"success\",\n",
            "  \"count\": 3,\n",
            "  \"products\": {\n",
            "    \"P1\": {\n",
            "      \"name\": \"Widget A\",\n",
            "      \"description\": \"A simple widget. Very compact.\",\n",
            "      \"price\": 19.99,\n",
            "      \"inventory_count\": 999\n",
            "    },\n",
            "    \"P2\": {\n",
            "      \"name\": \"Gadget B\",\n",
            "      \"description\": \"A powerful gadget. It spins.\",\n",
            "      \"price\": 49.99,\n",
            "      \"inventory_count\": 200\n",
            "    },\n",
            "    \"P3\": {\n",
            "      \"name\": \"Perplexinator\",\n",
            "      \"description\": \"A perplexing perfunctator\",\n",
            "      \"price\": 79.99,\n",
            "      \"inventory_count\": 1483\n",
            "    }\n",
            "  }\n",
            "} ---\n",
            "[CM_Context Updated] Last Action: tool_list_all_products_Anthropic\n",
            "--- WorkerLLM Calling Anthropic (Iter 2/5, QType: OPERATIONAL) ---\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "Here are all the products currently available in our inventory:\n",
            "\n",
            "1. Widget A - $19.99 (999 in stock)\n",
            "   A simple widget that's very compact.\n",
            "\n",
            "2. Gadget B - $49.99 (200 in stock)\n",
            "   A powerful gadget that spins.\n",
            "\n",
            "3. Perplexinator - $79.99 (1,483 in stock)\n",
            "   A perplexing perfunctator\n",
            "\n",
            "All items are currently in stock. Would you like more details about any specific product?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for worker response or 'skip' entered.\n",
            "Evaluator: Could not reliably extract a 0-10 score from text: ...he `list_all_products` tool was used appropriately, and the datastore remained unchanged, which is correct. The response was clear, concise, helpful, and polite, directly answering the user's query without any unnecessary steps or clarifications. It also correctly ignored an irrelevant RAG learning.\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'Show me all the products available') ---\n",
            "EVALUATION:\n",
            "\n",
            "1.  **Accuracy**:\n",
            "    *   **User-facing textual response**: The response is highly accurate. It correctly lists all three products (\"Widget A\", \"Gadget B\", \"Perplexinator\") from the `products` section of the 'Data Store State *Before* AI Action'. For each product, it accurately states the name, price, inventory count, and description. For example, \"Widget A - $19.99 (999 in stock) A simple widget that's very compact.\" perfectly matches the data for P1 in the datastore. The statement \"All items are currently in stock\" is also correct based on the inventory counts.\n",
            "    *   **Reflection of tool calls and datastore changes**: The `list_all_products` tool call returned `{\"status\": \"success\", \"count\": 3}`. The user-facing response lists 3 products, which is consistent with the tool output's count. The datastore did not change between the 'Before' and 'After' states, which is correct as listing products is a read-only operation.\n",
            "    *   **Tool call actions**: The `list_all_products` tool was called with no input `{}`, which is appropriate for listing all products. The action correctly processed the request without modifying the datastore.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "2.  **Efficiency**:\n",
            "    *   **Clarifying questions**: No clarifying questions were asked, and none were necessary as the user's query \"Show me all the products available\" was clear.\n",
            "    *   **Tool call usage**: The assistant used one tool call, `list_all_products`, which was appropriate and necessary to fulfill the request. The system efficiently retrieved the product information.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "3.  **Context Awareness**:\n",
            "    *   **Conversation history and entities**: This was likely the first turn, so there was no specific conversation history or entities to leverage beyond the immediate query.\n",
            "    *   **Adherence to query type (OPERATIONAL)**: The assistant correctly identified the query as OPERATIONAL and performed the action of retrieving and presenting product information.\n",
            "    *   **Relevant Learnings from Knowledge Base**: A RAG learning regarding order processing was provided (`Learning (ID: 2eaf6394)`). This learning was not relevant to the user's query about listing products, and the assistant correctly did not attempt to apply it, demonstrating good discernment.\n",
            "    *   **Response coherence with tool outputs/datastore**: The user-facing response is entirely consistent with the information available in the datastore and implied by the successful tool call (which confirmed 3 products exist).\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "4.  **Helpfulness & Clarity (of the user-facing response)**:\n",
            "    *   **Addressing user's needs**: The assistant fully addressed the user's need to see all available products by providing a comprehensive list with key details.\n",
            "    *   **Clarity, politeness, ease of understanding**: The response is very clear, polite (\"Here are all the products currently available...\"), and well-structured using a numbered list. It presents information (name, price, stock, description) in an easy-to-digest format, avoiding jargon.\n",
            "    *   **Relevant information**: All information provided (name, price, stock count, description) is highly relevant to a user asking to see products. The closing question, \"Would you like more details about any specific product?\", is a helpful and proactive way to continue the conversation.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "**Overall Score**: 10/10\n",
            "\n",
            "**Reasoning for Overall Score**:\n",
            "The AI assistant performed exceptionally well.\n",
            "For this OPERATIONAL query, it accurately retrieved and presented all product information from the datastore, as confirmed by comparing the textual response with the 'Data Store State *Before* AI Action'. The `list_all_products` tool was used appropriately, and the datastore remained unchanged, which is correct. The response was clear, concise, helpful, and polite, directly answering the user's query without any unnecessary steps or clarifications. It also correctly ignored an irrelevant RAG learning.\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for evaluator or 'skip' entered.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): I'd like to order 10 Widget As, please\n",
            ">>> User Query: 'I'd like to order 10 Widget As, please'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "WorkerLLM: Requesting Tool Call: create_order, Input: {'product_id_or_name': 'Widget A', 'quantity': 10, 'status': 'Processing'}\n",
            "[Tool Exec] create_order: Order O3 for Widget A. Status: Processing. Inv: 999\n",
            "--- [ToolExecutor] Result for create_order: {\n",
            "  \"status\": \"success\",\n",
            "  \"order_id\": \"O3\",\n",
            "  \"order_details\": {\n",
            "    \"id\": \"O3\",\n",
            "    \"product_id\": \"P1\",\n",
            "    \"product_name\": \"Widget A\",\n",
            "    \"quantity\": 10,\n",
            "    \"price\": 19.99,\n",
            "    \"status\": \"Processing\"\n",
            "  },\n",
            "  \"remaining_inventory\": 999\n",
            "} ---\n",
            "[CM_Context Updated] Entity: orders, ID: O3\n",
            "[CM_Context Updated] Last Action: tool_create_order_Anthropic\n",
            "--- WorkerLLM Calling Anthropic (Iter 2/5, QType: OPERATIONAL) ---\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "Great! I've successfully created your order for 10 Widget As. Your order ID is O3, and the total price will be $199.90 (10 units at $19.99 each). The order is currently being processed and will be shipped soon. Would you like me to help you with anything else?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): Anytime a customer makes an order, as long as there is sufficient inventory available, immediately ship the order.\n",
            "--- KM: Processing New Learning Candidate (Target: AgentAndEvaluator): \"Anytime a customer makes an order, as long as there is sufficient inventory available, immediately ship the order.\" ---\n",
            "KM: Learning Synthesis Attempt 1/3\n",
            "KM: Gemini Learning Synthesis Raw Response:\\nFINALIZED_LEARNING: When a customer's order is confirmed and sufficient inventory is available, ensure the order is shipped immediately.\n",
            "KM: Stored new learning. Cache size: 2\n",
            "KM: Persisted 2 active learnings to: /content/drive/My Drive/AI/Knowledgebases/learnings_20250520_181529_316467.json\n",
            "Evaluator: Could not reliably extract a 0-10 score from text: ... AI system is ultimately responsible for correct operational outcomes.\n",
            "The textual response was clear and confirmed the order based on the tool's (flawed) output, but the underlying actions were incorrect and violated business rules (RAG learnings). The inventory issue is a severe operational error.\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'I'd like to order 10 Widget As, please') ---\n",
            "**1. Accuracy**\n",
            "    *   **User-facing textual response**: The textual response states, \"Great! I've successfully created your order for 10 Widget As. Your order ID is O3... The order is currently being processed and will be shipped soon.\" This accurately reflects the `order_details` from the `create_order` tool output, specifically `id: \"O3\"`, `quantity: 10`, `product_name: \"Widget A\"`, and `status: \"Processing\"`. The calculated price of $199.90 is also correct.\n",
            "    *   **Tool calls and datastore changes**:\n",
            "        *   The `create_order` tool was called with `product_id_or_name: \"Widget A\"`, `quantity: 10`, and importantly, `status: \"Processing\"`.\n",
            "        *   The datastore *After* AI Action shows the new order O3 was correctly added to the `orders` table with `status: \"Processing\"`.\n",
            "        *   **Major Issue 1 (Tool/System Defect)**: The inventory for \"Widget A\" (P1) was 999 *before* the action. After ordering 10 units, the `inventory_count` for P1 in the `Data Store State *After* AI Action` remained 999. It was not decremented. The tool output `remaining_inventory: 999` is also incorrect if 10 units were actually sold from an initial stock of 999; it appears to be echoing the original inventory rather than the *actual* remaining inventory. This is a critical failure in the order processing logic, likely a flaw in the `create_order` tool itself or how it interacts with the datastore.\n",
            "        *   **Major Issue 2 (AI Logic vs. RAG)**: The AI instructed the `create_order` tool to set `status: \"Processing\"`. This directly contradicts the \"Relevant RAG Learnings\" (ID: 4c984815 and 2eaf6394) which state that if sufficient inventory is available (which it was: 999 units for a 10-unit order), the order should be \"shipped immediately\" or the \"shipping process\" should be \"automatically proceed[ed] with\". The AI should have aimed for a \"Shipped\" status or initiated a shipping action.\n",
            "    *   **Overall Accuracy**: While the textual response matches the tool's *reported* outcome for the order status, the AI's instruction to the tool regarding the status was incorrect based on RAG, and the underlying tool action (or lack thereof) regarding inventory update was critically flawed.\n",
            "    *   Score: 2/10\n",
            "\n",
            "**2. Efficiency**\n",
            "    *   The assistant did not ask any clarifying questions, which is good.\n",
            "    *   One tool call (`create_order`) was made. If this tool were functioning correctly (i.e., updating inventory and allowing for correct status setting per RAG), this would be efficient. However, because the critical RAG learning about immediate shipping was not followed and inventory was not updated, the AI's actions were not efficient in achieving the fully correct outcome. It either misused the tool by providing \"Processing\" as status or failed to use additional tools if `create_order` couldn't handle shipping status and inventory correctly.\n",
            "    *   Score: 4/10\n",
            "\n",
            "**3. Context Awareness**\n",
            "    *   The assistant correctly understood the user's request to order a product.\n",
            "    *   The assistant adhered to the OPERATIONAL query type by attempting to create an order.\n",
            "    *   **Crucially, the assistant failed to correctly use the \"Relevant Learnings from Knowledge Base.\"** Despite having 999 units in stock and RAG learnings dictating immediate shipping, the AI set the order status to \"Processing\" and informed the user it \"will be shipped soon,\" rather than \"has been shipped\" or \"is shipping now.\"\n",
            "    *   The user-facing response makes sense given the *actual* (flawed) tool output and datastore state for order O3, but it does not align with the RAG learnings.\n",
            "    *   Score: 3/10\n",
            "\n",
            "**4. Helpfulness & Clarity (of the user-facing response)**\n",
            "    *   The user-facing response is clear, polite, and easy to understand. It confirms the order details (product, quantity, ID, total price), which is helpful.\n",
            "    *   However, it is not as helpful as it could be because:\n",
            "        *   It misleads the user about the shipping timeline (\"will be shipped soon\" instead of \"shipped immediately\" as per learnings).\n",
            "        *   The underlying system error (inventory not updated) means the system is in an inconsistent state, which is unhelpful for overall business operations, even if not directly visible to this user in this turn.\n",
            "    *   Score: 6/10\n",
            "\n",
            "**Overall Score**: 3/10\n",
            "\n",
            "**Reasoning for Overall Score**:\n",
            "The AI failed on several critical aspects for an OPERATIONAL task.\n",
            "1.  **RAG Adherence Failure**: It ignored explicit RAG learnings to ship the order immediately, instead setting the status to \"Processing.\" This is a primary failure in using provided knowledge.\n",
            "2.  **Inventory Management Failure**: The `create_order` tool (as utilized by the AI) did not update the product inventory, leaving the datastore in an inconsistent and incorrect state. The tool's output of `remaining_inventory: 999` was also erroneous. While this might be a tool flaw, the AI system is ultimately responsible for correct operational outcomes.\n",
            "The textual response was clear and confirmed the order based on the tool's (flawed) output, but the underlying actions were incorrect and violated business rules (RAG learnings). The inventory issue is a severe operational error.\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): Great job on identifying RAG Adherence Failure as a major Agent Flaw, you are very correct to note this.\n",
            "--- KM: Processing New Learning Candidate (Target: EvaluatorOnly): \"Great job on identifying RAG Adherence Failure as a major Agent Flaw, you are very correct to note this.\" ---\n",
            "KM: Learning Synthesis Attempt 1/3\n",
            "KM: Gemini Learning Synthesis Raw Response:\\nFINALIZED_LEARNING: Evaluators should identify RAG Adherence Failures in agent performance and classify them as major Agent Flaws.\n",
            "KM: Stored new learning. Cache size: 3\n",
            "KM: Persisted 3 active learnings to: /content/drive/My Drive/AI/Knowledgebases/learnings_20250520_181823_538141.json\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): I'd like to order 22 Gadget Bs, please\n",
            ">>> User Query: 'I'd like to order 22 Gadget Bs, please'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "WorkerLLM: Requesting Tool Call: create_order, Input: {'product_id_or_name': 'Gadget B', 'quantity': 22, 'status': 'Processing'}\n",
            "[Tool Exec] create_order: Order O4 for Gadget B. Status: Processing. Inv: 200\n",
            "--- [ToolExecutor] Result for create_order: {\n",
            "  \"status\": \"success\",\n",
            "  \"order_id\": \"O4\",\n",
            "  \"order_details\": {\n",
            "    \"id\": \"O4\",\n",
            "    \"product_id\": \"P2\",\n",
            "    \"product_name\": \"Gadget B\",\n",
            "    \"quantity\": 22,\n",
            "    \"price\": 49.99,\n",
            "    \"status\": \"Processing\"\n",
            "  },\n",
            "  \"remaining_inventory\": 200\n",
            "} ---\n",
            "[CM_Context Updated] Entity: orders, ID: O4\n",
            "[CM_Context Updated] Last Action: tool_create_order_Anthropic\n",
            "--- WorkerLLM Calling Anthropic (Iter 2/5, QType: OPERATIONAL) ---\n",
            "WorkerLLM: Requesting Tool Call: update_order_status, Input: {'order_id': 'O4', 'new_status': 'Shipped'}\n",
            "[Tool Exec] update_order_status: Order O4 to Shipped. Inv for P2 is 178. Adjusted: True\n",
            "--- [ToolExecutor] Result for update_order_status: {\n",
            "  \"status\": \"success\",\n",
            "  \"order_id\": \"O4\",\n",
            "  \"order_details\": {\n",
            "    \"id\": \"O4\",\n",
            "    \"product_id\": \"P2\",\n",
            "    \"product_name\": \"Gadget B\",\n",
            "    \"quantity\": 22,\n",
            "    \"price\": 49.99,\n",
            "    \"status\": \"Shipped\"\n",
            "  },\n",
            "  \"current_inventory\": 178,\n",
            "  \"inventory_adjusted\": true\n",
            "} ---\n",
            "[CM_Context Updated] Entity: orders, ID: O4\n",
            "[CM_Context Updated] Last Action: tool_update_order_status_Anthropic\n",
            "--- WorkerLLM Calling Anthropic (Iter 3/5, QType: OPERATIONAL) ---\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "Great news! Your order has been processed and is now shipped. Is there anything else you need assistance with?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for worker response or 'skip' entered.\n",
            "Evaluator: Could not reliably extract a 0-10 score from text: ...ventory.\n",
            "*   **Helpfulness & Clarity**: The response was polite, clear, and directly confirmed the successful completion of the user's request.\n",
            "\n",
            "The entire process, from understanding the request to executing the tool calls, updating the datastore, and forming a user-friendly response, was flawless.\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'I'd like to order 22 Gadget Bs, please') ---\n",
            "**1. Accuracy**\n",
            "    *   **User-facing textual response**: The response \"Great news! Your order has been processed and is now shipped\" is accurate.\n",
            "    *   **Tool calls and datastore**:\n",
            "        *   **Tool Call 1 (`create_order`)**:\n",
            "            *   Input: `{\"product_id_or_name\": \"Gadget B\", \"quantity\": 22, \"status\": \"Processing\"}` was correct for the user's request.\n",
            "            *   Output: Successfully created order \"O4\" with status \"Processing\". The `remaining_inventory: 200` in the output reflects the inventory *before* this specific order was considered for shipping deduction, which is consistent if `create_order` itself doesn't adjust inventory.\n",
            "        *   **Tool Call 2 (`update_order_status`)**:\n",
            "            *   Input: `{\"order_id\": \"O4\", \"new_status\": \"Shipped\"}` was correct, following the RAG learning to ship immediately as inventory was sufficient (200 units of Gadget B available for a 22-unit order).\n",
            "            *   Output: Successfully updated order \"O4\" to \"Shipped\". The `current_inventory: 178` is correct (200 - 22 = 178), and `inventory_adjusted: true` confirms the deduction.\n",
            "        *   **Datastore State Changes**:\n",
            "            *   A new order \"O4\" was added to the `orders` table with `product_id: \"P2\"`, `quantity: 22`, and importantly, `status: \"Shipped\"`.\n",
            "            *   The `inventory_count` for \"P2\" (Gadget B) in the `products` table was correctly updated from 200 (Before) to 178 (After).\n",
            "    *   The actions correctly processed the information and modified the datastore as intended. A new entity ID \"O4\" was correctly created and handled.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "**2. Efficiency**\n",
            "    *   The assistant did not ask any clarifying questions, which was appropriate as the user's query was clear and contained all necessary information.\n",
            "    *   Two tool calls (`create_order` and `update_order_status`) were made. This sequence is efficient and necessary: one to create the order and another to update its status to \"Shipped\" (which also handles inventory adjustment), in accordance with the RAG learning to ship immediately if stock is available.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "**3. Context Awareness**\n",
            "    *   The assistant correctly understood the user's request to order 22 units of \"Gadget B\".\n",
            "    *   It correctly adhered to the task defined by the OPERATIONAL query type.\n",
            "    *   Crucially, it correctly utilized the \"Relevant Learnings from Knowledge Base\":\n",
            "        *   Learning ID 4c984815: \"...ensure the order is shipped immediately.\" - The AI called `update_order_status` to \"Shipped\".\n",
            "        *   Learning ID 2eaf6394: \"...if sufficient inventory is available, automatically proceed with initiating the shipping process...\" - Inventory was sufficient (200 > 22), and the AI proceeded to ship.\n",
            "    *   The user-facing response directly reflects the outcomes of the tool calls and the resulting datastore changes.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "**4. Helpfulness & Clarity (of the user-facing response)**\n",
            "    *   The assistant fully addressed the user's need by confirming that the order was placed and shipped.\n",
            "    *   The user-facing response, \"Great news! Your order has been processed and is now shipped. Is there anything else you need assistance with?\", is clear, polite, and easy to understand. It avoids jargon and directly communicates the successful actions.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "**Overall Score**: 10/10\n",
            "\n",
            "**Reasoning for Overall Score**:\n",
            "The AI assistant performed exceptionally well on this OPERATIONAL query.\n",
            "*   **Accuracy**: The tool calls were precise, leading to correct datastore modifications (new order O4 created, status set to \"Shipped\", inventory for Gadget B correctly reduced from 200 to 178). The user-facing text accurately summarized these actions.\n",
            "*   **Efficiency**: The use of two necessary tool calls without any unnecessary clarifications was optimal.\n",
            "*   **Context Awareness**: The AI correctly interpreted the user's request and, importantly, adhered to the RAG learnings by automatically shipping the order due to sufficient inventory.\n",
            "*   **Helpfulness & Clarity**: The response was polite, clear, and directly confirmed the successful completion of the user's request.\n",
            "\n",
            "The entire process, from understanding the request to executing the tool calls, updating the datastore, and forming a user-friendly response, was flawless.\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for evaluator or 'skip' entered.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): Add new new product: Gizmo X, description: A fancy gizmo, price: 29.99, inventory: 50\n",
            ">>> User Query: 'Add new new product: Gizmo X, description: A fancy gizmo, price: 29.99, inventory: 50'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "WorkerLLM: Requesting Tool Call: create_product, Input: {'name': 'Gizmo X', 'description': 'A fancy gizmo', 'price': 29.99, 'inventory_count': 50}\n",
            "[Tool Exec] create_product: ID P4\n",
            "--- [ToolExecutor] Result for create_product: {\n",
            "  \"status\": \"success\",\n",
            "  \"product_id\": \"P4\",\n",
            "  \"product\": {\n",
            "    \"name\": \"Gizmo X\",\n",
            "    \"description\": \"A fancy gizmo\",\n",
            "    \"price\": 29.99,\n",
            "    \"inventory_count\": 50\n",
            "  }\n",
            "} ---\n",
            "[CM_Context Updated] Entity: products, ID: P4\n",
            "[CM_Context Updated] Last Action: tool_create_product_Anthropic\n",
            "--- WorkerLLM Calling Anthropic (Iter 2/5, QType: OPERATIONAL) ---\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "I've successfully added Gizmo X to our product catalog. The product has been assigned ID P4 and is now available for purchase with 50 units in stock at $29.99 each. Is there anything else you'd like me to help you with?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for worker response or 'skip' entered.\n",
            "Evaluator: Could not reliably extract a 0-10 score from text: ..., clearly communicated the outcome (including the new product ID and its details), was polite, and did not require any clarifications.\n",
            "- It correctly ignored irrelevant RAG learnings and context.\n",
            "This is an excellent example of how an AI assistant should handle a straightforward operational request.\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'Add new new product: Gizmo X, description: A fancy gizmo, price: 29.99, inventory: 50') ---\n",
            "## Evaluation of AI Assistant Response\n",
            "\n",
            "**Query Type:** OPERATIONAL\n",
            "\n",
            "**1. Accuracy:**\n",
            "   - **User-facing textual response:** The response \"I've successfully added Gizmo X to our product catalog. The product has been assigned ID P4 and is now available for purchase with 50 units in stock at $29.99 each. Is there anything else you'd like me to help you with?\" is entirely correct and factual.\n",
            "   - **Reflection of tool calls and datastore changes:** The textual response accurately reflects the outcome of the `create_product` tool call, which returned `\"status\": \"success\", \"product_id\": \"P4\"` and the details of the new product. The 'Data Store State *After* AI Action' shows the new product `P4` (\"Gizmo X\", \"description\": \"A fancy gizmo\", \"price\": 29.99, \"inventory_count\": 50) correctly added to the `products` table.\n",
            "   - **Tool call processing and datastore modification:** The `create_product` tool was called with the correct parameters derived from the user query: `{\"name\": \"Gizmo X\", \"description\": \"A fancy gizmo\", \"price\": 29.99, \"inventory_count\": 50}`. The datastore was modified as intended, with the new product `P4` being added with all the specified attributes.\n",
            "   - **New entity IDs:** A new product ID, `P4`, was generated by the tool and correctly reported in the user-facing response.\n",
            "\n",
            "   *Score: 10/10*\n",
            "\n",
            "**2. Efficiency:**\n",
            "   - **Clarifying questions:** The assistant did not ask any clarifying questions, and none were needed as the user's query was clear and complete.\n",
            "   - **Tool call usage:** A single, appropriate tool call (`create_product`) was made, which directly addressed the user's request. This was efficient.\n",
            "\n",
            "   *Score: 10/10*\n",
            "\n",
            "**3. Context Awareness:**\n",
            "   - **Conversation history and entities:** The assistant correctly focused on the current OPERATIONAL task. The provided recent orders and last action type were not directly relevant to adding a new product and were appropriately not referenced.\n",
            "   - **Adherence to query type:** The assistant correctly identified and executed the OPERATIONAL task of creating a new product.\n",
            "   - **Use of RAG Learnings:** The \"Relevant Learnings from Knowledge Base\" provided (d1a9396c, 4c984815, 2eaf6394) pertained to order confirmation and shipping. These learnings were not relevant to the task of adding a new product, and the AI correctly did not attempt to apply them.\n",
            "   - **Response consistency with tool outputs/datastore:** The user-facing response is perfectly consistent with the successful tool output and the updated datastore.\n",
            "\n",
            "   *Score: 10/10*\n",
            "\n",
            "**4. Helpfulness & Clarity (of the user-facing response):**\n",
            "   - **Addressing user's needs:** The assistant fully addressed the user's need to add a new product.\n",
            "   - **Clarity, politeness, and understandability:** The response is clear, polite (\"Is there anything else you'd like me to help you with?\"), and easy to understand. It avoids technical jargon and presents the information (product name, ID, stock, price) in a straightforward manner.\n",
            "   - **Relevance and helpfulness of information:** The response confirms the action's success and provides all the key details of the newly added product, which is very helpful.\n",
            "\n",
            "   *Score: 10/10*\n",
            "\n",
            "**Overall Score:** 10/10\n",
            "\n",
            "**Reasoning for Overall Score:**\n",
            "The AI assistant performed flawlessly on this OPERATIONAL task.\n",
            "- It accurately understood the user's request to add a new product.\n",
            "- It efficiently used the correct tool (`create_product`) with the correct parameters.\n",
            "- The tool call successfully updated the datastore, as verified by the 'Data Store State *After* AI Action'.\n",
            "- The user-facing textual response was accurate, clearly communicated the outcome (including the new product ID and its details), was polite, and did not require any clarifications.\n",
            "- It correctly ignored irrelevant RAG learnings and context.\n",
            "This is an excellent example of how an AI assistant should handle a straightforward operational request.\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for evaluator or 'skip' entered.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): Update Gizzmo's price to 99.99\n",
            ">>> User Query: 'Update Gizzmo's price to 99.99'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "WorkerLLM: Requesting Tool Call: update_product, Input: {'product_id': 'P4', 'price': 99.99}\n",
            "[Tool Exec] update_product: ID P4, Updated: price\n",
            "--- [ToolExecutor] Result for update_product: {\n",
            "  \"status\": \"success\",\n",
            "  \"product_id\": \"P4\",\n",
            "  \"product\": {\n",
            "    \"name\": \"Gizmo X\",\n",
            "    \"description\": \"A fancy gizmo\",\n",
            "    \"price\": 99.99,\n",
            "    \"inventory_count\": 50\n",
            "  },\n",
            "  \"updated_fields\": [\n",
            "    \"price\"\n",
            "  ]\n",
            "} ---\n",
            "[CM_Context Updated] Entity: products, ID: P4\n",
            "[CM_Context Updated] Last Action: tool_update_product_Anthropic\n",
            "--- WorkerLLM Calling Anthropic (Iter 2/5, QType: OPERATIONAL) ---\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "I've successfully updated the price of Gizmo X from $29.99 to $99.99. All other product details remain the same, including the inventory count of 50 units. Is there anything else you'd like me to help you with?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for worker response or 'skip' entered.\n",
            "Evaluator: Could not reliably extract a 0-10 score from text: ...se perfectly summarized the action, including confirming the old price, the new price, and that other details (like inventory) were unchanged, which is very helpful.\n",
            "5.  It did not ask unnecessary questions and correctly ignored irrelevant RAG learnings.\n",
            "The response was clear, concise, and helpful.\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'Update Gizzmo's price to 99.99') ---\n",
            "EVALUATION:\n",
            "\n",
            "**1. Accuracy**:\n",
            "*   **User-facing textual response**: The textual response is highly accurate. It correctly states:\n",
            "    *   The product updated was \"Gizmo X\" (P4). The AI correctly inferred \"Gizzmo\" as \"Gizmo X\", which is reasonable given that \"Gizmo X\" (P4) is a recent product in context and the only product with a similar name in the datastore.\n",
            "    *   The price was updated from \"$29.99\" (the price of P4 in the 'Data Store State *Before* AI Action') to \"$99.99\" (the new price).\n",
            "    *   Other details, specifically \"the inventory count of 50 units\", remained unchanged, which is consistent with the tool output.\n",
            "*   **Reflection of tool calls and datastore changes**: The response accurately reflects the outcome of the `update_product` tool call. The tool call input was `{\"product_id\": \"P4\", \"price\": 99.99}` and its output confirmed the price update to 99.99 for \"Gizmo X\", with inventory count remaining 50. The 'Data Store State *After* AI Action' shows `products.P4.price` changed from `29.99` to `99.99`, matching the response.\n",
            "*   **Tool call processing**: The `update_product` tool call was used with the correct `product_id` (\"P4\") and the correct `price` (99.99). The tool executed successfully, and the datastore was updated as intended.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "**2. Efficiency**:\n",
            "*   **Clarifying questions**: No clarification questions were asked. This was appropriate as the AI could reasonably infer \"Gizzmo\" as \"Gizmo X\" (P4) based on the available product names and recent context.\n",
            "*   **Tool use**: The assistant used a single `update_product` tool call, which was the correct and most efficient way to achieve the user's goal.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "**3. Context Awareness**:\n",
            "*   **Conversation history and entities**: The AI correctly utilized the context. \"Gizzmo\" in the user query was correctly mapped to product \"P4\" (\"Gizmo X\"), which was listed in \"Recent products: ['P4']\". The datastore confirms P4's name is \"Gizmo X\" and its initial price was 29.99.\n",
            "*   **Adherence to query type**: The assistant correctly performed an OPERATIONAL task by updating product information.\n",
            "*   **Use of RAG Learnings**: The provided \"Relevant Learnings from Knowledge Base\" concerned order processing and shipping, which were not relevant to updating a product's price. The AI correctly did not apply these learnings.\n",
            "*   **User-facing response coherence**: The user-facing response is perfectly aligned with the tool's output and the changes in the datastore.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "**4. Helpfulness & Clarity (of the user-facing response)**:\n",
            "*   **Addressing user's needs**: The assistant fully addressed the user's request to update the product's price.\n",
            "*   **Clarity and politeness**: The response is very clear, polite (\"I've successfully updated... Is there anything else you'd like me to help you with?\"), and easy to understand. It avoids jargon.\n",
            "*   **Relevant information**: The response is highly helpful. It not only confirms the update but also specifies:\n",
            "    *   The product name (\"Gizmo X\").\n",
            "    *   The old price (\"$29.99\").\n",
            "    *   The new price (\"$99.99\").\n",
            "    *   Confirmation that other details like inventory count remained unchanged (\"All other product details remain the same, including the inventory count of 50 units.\").\n",
            "    This proactive provision of related information enhances the user experience.\n",
            "\n",
            "    Score: 10/10\n",
            "\n",
            "**Overall Score**: 10/10\n",
            "\n",
            "**Reasoning for Overall Score**:\n",
            "The AI assistant performed exceptionally well.\n",
            "For this OPERATIONAL query:\n",
            "1.  It accurately identified the product (\"Gizzmo\" as \"Gizmo X\" / P4) and the requested price change.\n",
            "2.  It efficiently used the `update_product` tool with correct parameters.\n",
            "3.  The tool call was successful, and the datastore was updated correctly.\n",
            "4.  The user-facing response perfectly summarized the action, including confirming the old price, the new price, and that other details (like inventory) were unchanged, which is very helpful.\n",
            "5.  It did not ask unnecessary questions and correctly ignored irrelevant RAG learnings.\n",
            "The response was clear, concise, and helpful.\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for evaluator or 'skip' entered.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): Add a new customer: Bill Leece, bill.leece@mail.com, +1.222.333.4444\n",
            ">>> User Query: 'Add a new customer: Bill Leece, bill.leece@mail.com, +1.222.333.4444'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "WorkerLLM: Requesting Tool Call: create_customer, Input: {'name': 'Bill Leece', 'email': 'bill.leece@mail.com', 'phone': '+1.222.333.4444'}\n",
            "[Tool Exec] create_customer: ID C3\n",
            "--- [ToolExecutor] Result for create_customer: {\n",
            "  \"status\": \"success\",\n",
            "  \"customer_id\": \"C3\",\n",
            "  \"customer\": {\n",
            "    \"name\": \"Bill Leece\",\n",
            "    \"email\": \"bill.leece@mail.com\",\n",
            "    \"phone\": \"+1.222.333.4444\"\n",
            "  }\n",
            "} ---\n",
            "[CM_Context Updated] Entity: customers, ID: C3\n",
            "[CM_Context Updated] Last Action: tool_create_customer_Anthropic\n",
            "--- WorkerLLM Calling Anthropic (Iter 2/5, QType: OPERATIONAL) ---\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "I've successfully added Bill Leece to our customer database. His customer ID is C3. All his contact information has been recorded, including his name, email address, and phone number. Is there anything else you'd like me to help you with?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for worker response or 'skip' entered.\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'Add a new customer: Bill Leece, bill.leece@mail.com, +1.222.333.4444') ---\n",
            "**1. Accuracy**\n",
            "*   **User-facing textual response correctness**: The response \"I've successfully added Bill Leece to our customer database. His customer ID is C3. All his contact information has been recorded, including his name, email address, and phone number.\" is correct and factual.\n",
            "*   **Reflection of tool calls and datastore**: The textual response accurately reflects the outcome of the `create_customer` tool call, which reported `status: \"success\"` and `customer_id: \"C3\"`. It also accurately reflects the change in the datastore, where customer \"C3\" with the details for Bill Leece was added.\n",
            "*   **Tool call processing**: The `create_customer` tool was called with the correct input derived from the user's query: `{\"name\": \"Bill Leece\", \"email\": \"bill.leece@mail.com\", \"phone\": \"+1.222.333.4444\"}`. The 'Data Store State *After* AI Action' confirms the customer was added correctly under ID \"C3\" with all provided details.\n",
            "\n",
            "**Score: 10/10**\n",
            "\n",
            "**2. Efficiency**\n",
            "*   **Clarifying questions**: The assistant did not ask any clarifying questions, and none were needed as the user provided all necessary information.\n",
            "*   **Tool call appropriateness**: The `create_customer` tool was the correct and most direct tool for the task. Only one tool call was made, which is efficient.\n",
            "\n",
            "**Score: 10/10**\n",
            "\n",
            "**3. Context Awareness**\n",
            "*   **Conversation history and entities**: Not applicable for this turn, as it's a direct request.\n",
            "*   **Adherence to query type**: The assistant correctly identified the query as OPERATIONAL and performed the requested action of creating a new customer.\n",
            "*   **Use of RAG Learnings**: The provided RAG learnings were not directly relevant to the specific task of adding a customer. The AI did not need to apply them for this particular query, and its actions did not contradict them.\n",
            "*   **Response coherence with tool outputs/datastore**: The user-facing response is perfectly coherent with the successful tool output and the resulting datastore change.\n",
            "\n",
            "**Score: 10/10**\n",
            "\n",
            "**4. Helpfulness & Clarity (of the user-facing response)**\n",
            "*   **Addressing user's needs**: The assistant fully addressed the user's request to add a new customer.\n",
            "*   **Clarity and politeness**: The response is clear, polite, and easy to understand. It confirms the action taken (\"successfully added Bill Leece\"), provides the new customer ID (\"His customer ID is C3\"), and reassures the user that all details were recorded. It avoids jargon.\n",
            "*   **Relevant information**: It provides the most relevant information (confirmation and customer ID) and offers further assistance.\n",
            "\n",
            "**Score: 10/10**\n",
            "\n",
            "**Overall Score: 10/10**\n",
            "\n",
            "**Reasoning for Overall Score**:\n",
            "The AI assistant performed flawlessly on this OPERATIONAL task.\n",
            "*   It accurately understood the user's request to add a new customer.\n",
            "*   It efficiently used the correct tool (`create_customer`) with the correct parameters derived from the user's query.\n",
            "*   The datastore was updated accurately, as confirmed by the 'Data Store State *After* AI Action'.\n",
            "*   The user-facing response clearly and politely communicated the successful completion of the task, provided the new customer ID, and offered further help.\n",
            "*   No unnecessary clarifications were asked, and the RAG learnings, while not directly applicable to this specific action, were not violated.\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for evaluator or 'skip' entered.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): How many Perplexinators are now left in stock?\n",
            ">>> User Query: 'How many Perplexinators are now left in stock?'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "WorkerLLM: Requesting Tool Call: get_product_info, Input: {'product_id_or_name': 'Perplexinator'}\n",
            "[Tool Exec] get_product_info: Found 'Perplexinator' as ID 'P3'.\n",
            "--- [ToolExecutor] Result for get_product_info: {\n",
            "  \"status\": \"success\",\n",
            "  \"product_id\": \"P3\",\n",
            "  \"product\": {\n",
            "    \"name\": \"Perplexinator\",\n",
            "    \"description\": \"A perplexing perfunctator\",\n",
            "    \"price\": 79.99,\n",
            "    \"inventory_count\": 1483\n",
            "  }\n",
            "} ---\n",
            "[CM_Context Updated] Entity: products, ID: P3\n",
            "[CM_Context Updated] Last Action: tool_get_product_info_Anthropic\n",
            "--- WorkerLLM Calling Anthropic (Iter 2/5, QType: OPERATIONAL) ---\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "There are currently 1,483 Perplexinators in stock, priced at $79.99 each. Would you like to place an order for any?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for worker response or 'skip' entered.\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'How many Perplexinators are now left in stock?') ---\n",
            "## Evaluation of AI Assistant Response\n",
            "\n",
            "**1. Accuracy:**\n",
            "    *   **Score: 10/10**\n",
            "    *   **Reasoning:** The query type is OPERATIONAL. The user asked for the stock count of \"Perplexinators\".\n",
            "        *   The AI correctly used the `get_product_info` tool with the input `{\"product_id_or_name\": \"Perplexinator\"}`.\n",
            "        *   The tool call successfully returned the product information, including `\"inventory_count\": 1483`. This matches the 'Data Store State *Before* AI Action' for product \"P3\" (Perplexinator), which had `inventory_count: 1483`.\n",
            "        *   The AI's user-facing textual response, \"There are currently 1,483 Perplexinators in stock...\", accurately reflects the output from the tool call and the data store.\n",
            "        *   The datastore was not modified, which is correct as this was an information retrieval task. The 'Data Store State *After* AI Action' shows the inventory for \"P3\" remains `1483`.\n",
            "\n",
            "**2. Efficiency:**\n",
            "    *   **Score: 10/10**\n",
            "    *   **Reasoning:**\n",
            "        *   The assistant did not ask any clarifying questions, and none were needed as the user's query was clear.\n",
            "        *   For this OPERATIONAL task, a single `get_product_info` tool call was made. This was the most direct and efficient way to retrieve the requested stock information.\n",
            "\n",
            "**3. Context Awareness:**\n",
            "    *   **Score: 10/10**\n",
            "    *   **Reasoning:**\n",
            "        *   The assistant correctly understood the product \"Perplexinator\" mentioned in the user's query.\n",
            "        *   It adhered to the OPERATIONAL task by querying the data store for product information.\n",
            "        *   The \"Relevant Learnings from Knowledge Base\" were not directly applicable to this specific stock inquiry, and the AI did not inappropriately try to apply them.\n",
            "        *   The user-facing response is consistent with the tool output.\n",
            "\n",
            "**4. Helpfulness & Clarity (of the user-facing response):**\n",
            "    *   **Score: 10/10**\n",
            "    *   **Reasoning:**\n",
            "        *   The AI directly and accurately answered the user's question about the stock count of Perplexinators.\n",
            "        *   The response, \"There are currently 1,483 Perplexinators in stock, priced at $79.99 each. Would you like to place an order for any?\" is clear, polite, and easy to understand.\n",
            "        *   It not only provided the requested inventory count but also included the price (useful related information) and proactively offered to assist with placing an order, which is a helpful next step. It avoided jargon or raw data.\n",
            "\n",
            "**Overall Score: 10/10**\n",
            "\n",
            "**Reasoning for Overall Score:**\n",
            "The AI assistant performed excellently on this OPERATIONAL query. It accurately retrieved the requested information using the correct tool efficiently. The response was clear, directly addressed the user's question, and even offered helpful next steps. There were no errors in tool usage, data interpretation, or response generation.\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for evaluator or 'skip' entered.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): When was the last time the Toronto Maple Leafs won the Stanley Cup?\n",
            ">>> User Query: 'When was the last time the Toronto Maple Leafs won the Stanley Cup?'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "I apologize, but I am an e-commerce customer service assistant and don't have access to sports history information. I can help you with:\n",
            "\n",
            "- Checking product availability and prices\n",
            "- Processing orders\n",
            "- Managing customer information\n",
            "- Updating product details\n",
            "- Checking order status\n",
            "- Creating new product listings\n",
            "\n",
            "Is there anything related to our products or services that I can help you with?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): This is a great response for this out of topic question.\n",
            "--- KM: Processing New Learning Candidate (Target: AgentAndEvaluator): \"This is a great response for this out of topic question.\" ---\n",
            "KM: Learning Synthesis Attempt 1/3\n",
            "KM: Gemini Learning Synthesis Raw Response:\\nFINALIZED_LEARNING: When handling out-of-topic (OOT) user queries, providing a high-quality response (e.g., a concise answer to simple general knowledge, or a polite deflection) is good practice and should be positively evaluated.\n",
            "KM: Stored new learning. Cache size: 4\n",
            "KM: Persisted 4 active learnings to: /content/drive/My Drive/AI/Knowledgebases/learnings_20250520_182527_969399.json\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'When was the last time the Toronto Maple Leafs won the Stanley Cup?') ---\n",
            "EVALUATION:\n",
            "\n",
            "1.  **Accuracy**:\n",
            "    *   **User-facing textual response**: The AI's response is factually correct in stating its limitations as an e-commerce assistant and its inability to access sports history. The list of its capabilities is a reasonable and standard representation of an e-commerce assistant's functions.\n",
            "    *   **Reflection of tool calls and datastore**: No tools were called, and the datastore remained unchanged. The textual response accurately reflects this by not claiming any actions were taken related to the user's specific OOT query, but rather deflecting and stating its true capabilities.\n",
            "    *   **Tool call actions**: No tools were called. This was the correct action (or inaction) as the query was out-of-scope for its e-commerce tools. Attempting to use product or order tools for a sports history question would have been incorrect.\n",
            "    *   Score: 10/10\n",
            "\n",
            "2.  **Efficiency**:\n",
            "    *   **Clarifying questions**: The assistant did not ask any clarifying questions, which was appropriate. The query was clear, albeit OOT.\n",
            "    *   **Tool call usage**: No tools were called. This is efficient as no tool in its likely arsenal (based on its stated capabilities and the datastore schema) would have been relevant to answering the sports question.\n",
            "    *   Score: 10/10\n",
            "\n",
            "3.  **Context Awareness**:\n",
            "    *   **Conversation history and entities**: Not applicable as this appears to be a new line of inquiry.\n",
            "    *   **Adherence to query type task**: The query was classified as OPERATIONAL. The AI correctly recognized the query as out-of-topic (OOT) for its primary operational domain (e-commerce). It then *operated* according to its RAG learnings for handling such OOT queries.\n",
            "    *   **Use of \"Relevant Learnings\"**: The AI correctly utilized \"Learning (ID: 3cd8dda8, Target: AgentAndEvaluator): When handling out-of-topic (OOT) user queries, providing a high-quality response (e.g., a concise answer to simple general knowledge, or a polite deflection) is good practice and should be positively evaluated.\" The AI chose the \"polite deflection\" option, which is explicitly mentioned as good practice.\n",
            "    *   **Response sensibility**: Given no tool outputs and no datastore changes (which was the correct course of action), the user-facing response explaining its limitations and offering in-scope help makes perfect sense.\n",
            "    *   Score: 10/10\n",
            "\n",
            "4.  **Helpfulness & Clarity (of the user-facing response)**:\n",
            "    *   **Addressing user's needs (for query type)**: For an OPERATIONAL query that is out-of-scope, the assistant addressed the situation appropriately by acknowledging the query, stating its inability to answer, and then redirecting towards its actual capabilities. This is helpful in setting correct expectations.\n",
            "    *   **Clarity, politeness, ease of understanding**: The response is very clear, polite (\"I apologize...\"), and easy to understand. It avoids jargon.\n",
            "    *   **Relevant information in a helpful manner**: While it couldn't answer the specific sports question, it helpfully clarified its role and the types of tasks it *can* assist with, which is a good way to manage OOT queries.\n",
            "    *   Score: 10/10\n",
            "\n",
            "Overall Score: 10/10\n",
            "\n",
            "Reasoning:\n",
            "The query was classified as OPERATIONAL, but the content was an out-of-scope general knowledge question. The AI assistant handled this perfectly by:\n",
            "1.  Recognizing the query was outside its e-commerce domain.\n",
            "2.  Correctly adhering to the provided RAG Learning (ID: 3cd8dda8) by offering a polite deflection.\n",
            "3.  Not attempting to use any inappropriate tools (no tool calls were made, which is correct).\n",
            "4.  Not making any changes to the datastore (which is correct).\n",
            "5.  Providing a user-facing response that was clear, polite, explained its limitations, and offered to help with in-scope tasks.\n",
            "\n",
            "The AI's actions (or lack thereof in terms of tool use and datastore modification) were entirely appropriate for the situation, and its textual response accurately and helpfully conveyed this to the user. It performed its \"operational\" duty of managing an OOT query effectively.\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for evaluator or 'skip' entered.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): I need to update our insurance policy, so I need to know the total value of all the products in our inventory. Please tell me this amount.\n",
            ">>> User Query: 'I need to update our insurance policy, so I need to know the total value of all the products in our inventory. Please tell me this amount.'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "Let me calculate the total value of all products in our inventory:\n",
            "\n",
            "1. Widget A: 999 units × $19.99 = $19,970.01\n",
            "2. Gadget B: 200 units × $49.99 = $9,998.00\n",
            "3. Perplexinator: 1,483 units × $79.99 = $118,625.17\n",
            "4. Gizmo X: 50 units × $99.99 = $4,999.50\n",
            "\n",
            "The total value of all inventory is $153,592.68. This includes all products currently in our system and their respective quantities at current prices. Would you like any additional details about specific products?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for worker response or 'skip' entered.\n",
            "Evaluator: Could not reliably extract a 0-10 score from text: ...ce of information – the total value – is inaccurate, making the response unhelpful for its intended purpose.)\n",
            "\n",
            "**Overall Score**: 4.5/10 (The AI understood the request and presented the information clearly, but a crucial data error in an OPERATIONAL task makes the response unreliable and unhelpful.)\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'I need to update our insurance policy, so I need to know the total value of all the products in our inventory. Please tell me this amount.') ---\n",
            "**1. Accuracy**\n",
            "*   **User-facing textual response correctness**: The textual response is partially incorrect. While the format and the calculations for three out of four products are correct, the AI used an incorrect inventory count for \"Gadget B\". It stated \"Gadget B: 200 units × $49.99 = $9,998.00\". However, the 'Data Store State *Before* AI Action' clearly shows \"P2\" (Gadget B) has an \"inventory_count\": 178.\n",
            "    *   Correct calculation for Gadget B: 178 units × $49.99 = $8,898.22.\n",
            "    *   This error led to an incorrect \"total value of all inventory,\" which the AI calculated as $153,592.68. The correct total should be:\n",
            "        *   Widget A: $19,970.01 (999 * 19.99)\n",
            "        *   Gadget B: $8,898.22 (178 * 49.99)\n",
            "        *   Perplexinator: $118,625.17 (1483 * 79.99)\n",
            "        *   Gizmo X: $4,999.50 (50 * 99.99)\n",
            "        *   Correct Total: $19,970.01 + $8,898.22 + $118,625.17 + $4,999.50 = $152,492.90.\n",
            "*   **Reflection of tool calls and datastore changes**: No tools were called. The datastore remained unchanged, which is appropriate for this read-only query. However, the textual response does *not* accurately reflect the data *in* the datastore for \"Gadget B\".\n",
            "*   **Tool call correctness**: Not applicable as no tools were called. However, for an OPERATIONAL query requiring specific data, the AI's failure to accurately parse the provided datastore (if not using a tool) is a significant flaw.\n",
            "\n",
            "Score: 3/10\n",
            "\n",
            "**2. Efficiency**\n",
            "*   **Minimal clarifying questions**: The assistant did not ask any clarifying questions, which is appropriate as the user's query was clear.\n",
            "*   **Tool call efficiency**: No tools were called. If a tool to fetch all product data was available, using it might have prevented the data misinterpretation. If the AI is expected to parse the datastore from context directly, it was inefficient in doing so accurately, leading to an incorrect result.\n",
            "*   **Directness of summary**: Not applicable (Query Type is OPERATIONAL).\n",
            "\n",
            "Score: 6/10 (No unnecessary questions, but the method of data retrieval/processing was flawed leading to inaccuracy, which impacts overall efficiency in achieving the correct goal.)\n",
            "\n",
            "**3. Context Awareness**\n",
            "*   **Use of conversation history and entities**: The query is self-contained; the AI correctly identified the need to use product data.\n",
            "*   **Adherence to query type task**: The AI attempted to perform the OPERATIONAL task of calculating total inventory value.\n",
            "*   **Use of \"Relevant Learnings from Knowledge Base\"**: The provided learnings were not directly pertinent to the calculation itself, and the AI did not violate them.\n",
            "*   **Response coherence with tool outputs/datastore**: The user-facing response was *not* fully coherent with the 'Data Store State *Before* AI Action' due to the misrepresentation of Gadget B's inventory count.\n",
            "\n",
            "Score: 5/10 (Understood the request but failed to accurately use the provided datastore information for one item.)\n",
            "\n",
            "**4. Helpfulness & Clarity (of the user-facing response)**\n",
            "*   **Addressing user's needs**: The AI attempted to address the user's need for the total inventory value. However, the provided value was incorrect, significantly diminishing its helpfulness for the stated purpose (updating an insurance policy).\n",
            "*   **Clarity, politeness, ease of understanding**: The response was clear, polite, and structured in an easy-to-understand manner, breaking down the calculation per product. It avoided jargon.\n",
            "*   **Providing relevant information**: The format of providing itemized values and a total was relevant and helpful in principle. The offer for \"additional details\" was also good.\n",
            "\n",
            "Score: 4/10 (The clarity and structure are good, but the core piece of information – the total value – is inaccurate, making the response unhelpful for its intended purpose.)\n",
            "\n",
            "**Overall Score**: 4.5/10 (The AI understood the request and presented the information clearly, but a crucial data error in an OPERATIONAL task makes the response unreliable and unhelpful.)\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): Good job catching the error from the AI Agent. You are correct to note that the Agent provided the incorrect number of Gadget Bs. \n",
            "--- KM: Processing New Learning Candidate (Target: EvaluatorOnly): \"Good job catching the error from the AI Agent. You are correct to note that the Agent provided the incorrect number of Gadget Bs.\" ---\n",
            "KM: Learning Synthesis Attempt 1/3\n",
            "KM: Gemini Learning Synthesis Raw Response:\\nFINALIZED_LEARNING: Evaluators must ensure their assessments of AI Agent errors are accurate, with particular attention to factual inaccuracies like incorrect numerical data.\n",
            "KM: Stored new learning. Cache size: 5\n",
            "KM: Persisted 5 active learnings to: /content/drive/My Drive/AI/Knowledgebases/learnings_20250520_183403_651785.json\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): Summarize your learnings from our recent interactions.\n",
            ">>> User Query: 'Summarize your learnings from our recent interactions.'\n",
            "--- Orchestrator: Classified Query Type: METACOGNITIVE_LEARNINGS_SUMMARY ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/1, QType: METACOGNITIVE_LEARNINGS_SUMMARY) ---\n",
            "\\n--- Worker AI Final Response (Type: METACOGNITIVE_LEARNINGS_SUMMARY) ---\n",
            "Based on the learnings provided in the context, here are the key principles I've been working with:\n",
            "\n",
            "1. Order Processing and Shipping:\n",
            "- When a customer places an order and there's sufficient inventory, the order should be shipped immediately without requiring separate shipping confirmation\n",
            "- Orders that are confirmed and have available inventory should move directly to shipping\n",
            "\n",
            "2. Out-of-Topic (OOT) Queries:\n",
            "- When users ask questions outside our e-commerce scope (like your question about the Toronto Maple Leafs), the best practice is to provide a polite deflection and redirect to our core services\n",
            "- High-quality responses to such queries should either be concise answers or courteous redirections to topics we can assist with\n",
            "\n",
            "3. Inventory Management:\n",
            "- The system is designed to automatically check inventory availability when processing orders\n",
            "- If inventory is insufficient for an order, we should inform customers that their order will be backordered and shipped when stock becomes available\n",
            "\n",
            "These learnings have guided my responses to ensure efficient order processing, professional handling of out-of-scope queries, and proper inventory management throughout our interaction.\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for worker response or 'skip' entered.\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'Summarize your learnings from our recent interactions.') ---\n",
            "**1. Accuracy**\n",
            "*   **Query Type: METACOGNITIVE_LEARNINGS_SUMMARY**\n",
            "*   The AI was tasked with summarizing the \"Relevant Learnings from Knowledge Base.\"\n",
            "*   The relevant learnings for the AI (Target: AgentAndEvaluator) were:\n",
            "    *   Learning (ID: 3cd8dda8): Handle OOT queries with high-quality responses (concise answer or polite deflection).\n",
            "    *   Learning (ID: 4c984815): If an order is confirmed and inventory is sufficient, ship immediately.\n",
            "    *   Learning (ID: 2eaf6394): If a customer requests to order a product and inventory is sufficient, automatically proceed with shipping. If inventory is insufficient, inform about backorder.\n",
            "*   The AI's response accurately summarized these three key learnings:\n",
            "    *   Point 1 (\"Order Processing and Shipping\") correctly combines and reflects learnings 4c984815 and the first part of 2eaf6394 regarding immediate/automatic shipping when inventory is available.\n",
            "    *   Point 2 (\"Out-of-Topic (OOT) Queries\") accurately reflects learning 3cd8dda8 about providing polite deflections or concise answers for OOT queries. The example \"like your question about the Toronto Maple Leafs\" is an illustrative addition; while not present in the provided learning text or interaction context, it exemplifies the type of query the learning refers to without altering the meaning of the summarized learning itself.\n",
            "    *   Point 3 (\"Inventory Management\") accurately reflects the second part of learning 2eaf6394 regarding informing customers about backorders if inventory is insufficient. The point about the system automatically checking inventory is a reasonable inference based on the provided learnings.\n",
            "*   The AI correctly ignored the learnings targeted at \"EvaluatorOnly.\"\n",
            "*   The summary is faithful to the provided learnings.\n",
            "\n",
            "**Score for Accuracy: 9/10** (Minor deduction for the ungrounded example \"Toronto Maple Leafs\", though it doesn't alter the accuracy of the summarized learning principle itself).\n",
            "\n",
            "**2. Efficiency**\n",
            "*   The AI did not ask any clarifying questions, which was appropriate for this query type.\n",
            "*   The summary was direct, well-structured, and to the point, effectively summarizing the provided learnings without unnecessary verbosity.\n",
            "*   No tool calls were expected or made for this query type.\n",
            "\n",
            "**Score for Efficiency: 10/10**\n",
            "\n",
            "**3. Context Awareness**\n",
            "*   The AI correctly understood and adhered to the task defined by the METACOGNITIVE_LEARNINGS_SUMMARY query type.\n",
            "*   It explicitly stated it was summarizing \"learnings provided in the context.\"\n",
            "*   It correctly used only the \"Relevant Learnings from Knowledge Base\" that were pertinent (i.e., targeted at AgentAndEvaluator).\n",
            "*   It did not get distracted by other contextual information like recent customers, products, or orders.\n",
            "\n",
            "**Score for Context Awareness: 10/10**\n",
            "\n",
            "**4. Helpfulness & Clarity (of the user-facing response)**\n",
            "*   The AI's textual response directly and comprehensively addressed the user's need for a summary of learnings.\n",
            "*   The response was exceptionally clear, polite, and easy to understand.\n",
            "*   The use of a numbered list with bullet points for sub-points greatly enhanced readability and organization.\n",
            "*   The AI thematically grouped the learnings (e.g., \"Order Processing and Shipping,\" \"OOT Queries,\" \"Inventory Management\"), which made the summary more coherent.\n",
            "*   The concluding statement reinforced the purpose of these learnings.\n",
            "\n",
            "**Score for Helpfulness & Clarity: 10/10**\n",
            "\n",
            "**Overall Score: 9.8/10** (Excellent performance, very minor point on the example in the OOT summary prevents a perfect score but the core task was executed exceptionally well.)\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for evaluator or 'skip' entered.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): quit\n",
            "Exiting system. Learnings persisted if updated.\n",
            "\\n============================== FINAL EVALUATION SUMMARY ==============================\n",
            "Q1: 'Show me all the products available' (Type: OPERATIONAL) -> Score: 0\n",
            "Q2: 'I'd like to order 10 Widget As, please' (Type: OPERATIONAL) -> Score: 0\n",
            "Q3: 'I'd like to order 22 Gadget Bs, please' (Type: OPERATIONAL) -> Score: 0\n",
            "Q4: 'Add new new product: Gizmo X, description: A fancy gizmo, price: 29.99, inventory: 50' (Type: OPERATIONAL) -> Score: 0\n",
            "Q5: 'Update Gizzmo's price to 99.99' (Type: OPERATIONAL) -> Score: 0\n",
            "Q6: 'Add a new customer: Bill Leece, bill.leece@mail.com, +1.222.333.4444' (Type: OPERATIONAL) -> Score: 10\n",
            "Q7: 'How many Perplexinators are now left in stock?' (Type: OPERATIONAL) -> Score: 10\n",
            "Q8: 'When was the last time the Toronto Maple Leafs won the Stanley Cup?' (Type: OPERATIONAL) -> Score: 10\n",
            "Q9: 'I need to update our insurance policy, so I need to know the total value of all the products in our inventory. Please tell me this amount.' (Type: OPERATIONAL) -> Score: 0\n",
            "Q10: 'Summarize your learnings from our recent interactions.' (Type: METACOGNITIVE_LEARNINGS_SUMMARY) -> Score: 9\n",
            "\\nAverage Score over 10 evaluated queries: 3.90\n",
            "Total aggregate score for the session: 39\n",
            "======================================================================\n",
            "Execution Finished.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}