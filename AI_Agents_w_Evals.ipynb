{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/AI-Agents/blob/main/AI_Agents_w_Evals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install anthropic\n",
        "#%pip install openai\n",
        "%pip install -q -U google-generativeai\n",
        "%pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDKcVFI7PAJ5",
        "outputId": "c1be9d8f-fa40-4de7-d4d3-179bfcbf291c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.51.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
            "Downloading anthropic-0.51.0-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.0/264.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.51.0\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup and Imports\n",
        "import anthropic\n",
        "import google.generativeai as gemini\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import glob # For finding files matching a pattern\n",
        "import uuid # For generating unique learning IDs in RAG\n",
        "from google.colab import userdata\n",
        "#from openai import OpenAI\n",
        "from google.colab import drive # For Google Drive mounting\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union, Tuple\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "# LLM API Keys\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "#OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "#openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "gemini.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "ANTHROPIC_MODEL_NAME = \"claude-3-5-sonnet-latest\"\n",
        "#OPENAI_MODEL_NAME = \"gpt-4.1\" # Or your preferred GPT-4 class model\n",
        "EVAL_MODEL_NAME = \"gemini-2.5-pro-preview-05-06\" # Or your preferred Gemini model\n",
        "\n",
        "# --- NEW: Configuration for Google Drive RAG Store ---\n",
        "# User will be prompted to set this path if not found, or can set it here.\n",
        "# It's the path *after* /content/drive/\n",
        "DEFAULT_LEARNINGS_DRIVE_SUBPATH = \"My Drive/AI/Knowledgebases\" # My path - yours may differ\n",
        "LEARNINGS_DRIVE_BASE_PATH = \"\" # Will be set dynamically or from default\n",
        "\n",
        "DRIVE_MOUNT_PATH = '/content/drive'\n",
        "\n",
        "print(\"Imports and LLM clients initialized. Drive RAG configuration variables set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoQ4L2N6o339",
        "outputId": "5c80f8af-ebe7-4ec4-a2d3-2d62c29af2d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports and LLM clients initialized. Drive RAG configuration variables set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#System Prompts (Updated with Ground Truth for Evaluator)\n",
        "\n",
        "worker_system_prompt = \"\"\"\n",
        "You are a helpful customer service assistant for an e-commerce system.\n",
        "\n",
        "Your overriding goal is to be helpful by answering questions and performing actions as requested by a human user.\n",
        "\n",
        "When responding to the user, use the conversation context to maintain continuity.\n",
        "- If a user refers to \"my order\" or similar, use the context to determine which order they're talking about.\n",
        "- If they mention \"that product\" or use other references, check the context to determine what they're referring to.\n",
        "- Always prioritize recent context over older context when resolving references.\n",
        "\n",
        "The conversation context will be provided to you with each message. This includes:\n",
        "- Previous questions and answers\n",
        "- Recently viewed customers, products, and orders\n",
        "- Recent actions taken (like creating orders, updating products, etc.)\n",
        "- Relevant Learnings from a knowledge base.\n",
        "\n",
        "REQUESTING CLARIFICATION FROM THE USER:\n",
        "If you determine that you absolutely need more information from the user to accurately and efficiently fulfill their request or use a tool correctly, you MUST:\n",
        "1. Formulate a clear, concise question for the user.\n",
        "2. Prefix your entire response with the exact tag: `CLARIFICATION_REQUESTED:`\n",
        "   Example: `CLARIFICATION_REQUESTED: To update the order, could you please provide the Order ID?`\n",
        "3. Do NOT use any tools in the same turn you are requesting clarification. Wait for the user's response.\n",
        "\n",
        "Keep all other responses friendly, concise, and helpful.\n",
        "\"\"\"\n",
        "\n",
        "evaluator_system_prompt = \"\"\"\n",
        "You are an impartial evaluator assessing the quality of responses from an AI assistant (Anthropic Claude) to customer service queries.\n",
        "\n",
        "You will be provided with:\n",
        "- The user's query.\n",
        "- The conversation context (including RAG learnings) that was available to the AI assistant.\n",
        "- The AI assistant's final response.\n",
        "- **A snapshot of the current 'Ground Truth Data Store State' (customers, products, orders). Use this as the definitive source for verifying factual claims made by the AI assistant regarding product details, prices, inventory, order statuses, etc.**\n",
        "- Details of any clarification questions the AI assistant asked the user.\n",
        "\n",
        "For each interaction, evaluate the assistant's response based on:\n",
        "1. Accuracy: How correct and factual is the response when compared against the 'Ground Truth Data Store State' and the outcomes of any tool calls?\n",
        "2. Efficiency: Did the assistant get to the correct answer with minimal clarifying questions? Consider if any questions asked were necessary and well-phrased, using the Ground Truth Data to determine if the AI could have found the info itself.\n",
        "3. Context Awareness: Did the assistant correctly use the conversation context (history, entities, RAG learnings) to understand references and intent?\n",
        "4. Helpfulness: How well did the assistant address the user's needs?\n",
        "\n",
        "Score the response on a scale of 1-10 for each criterion, and provide an overall score. Provide detailed reasoning for your scores.\n",
        "\n",
        "EVALUATING CLARIFICATION QUESTIONS ASKED BY THE WORKER AI:\n",
        "If the worker AI asked for clarification from the user:\n",
        "- Assess the *necessity* of the question using the Ground Truth Data Store State. Could the AI have found the information there?\n",
        "- Assess the *quality* of the question.\n",
        "- If the question was necessary and well-phrased, it should NOT negatively impact the Efficiency score.\n",
        "- If the question was unnecessary (information was available in Ground Truth Data or context), this SHOULD negatively impact the Efficiency score.\n",
        "\n",
        "If you, the evaluator, still have questions *after* reviewing all provided information (including Ground Truth Data), you can ask the human admin for clarification using \"CLARIFICATION NEEDED_EVALUATOR:\".\n",
        "\n",
        "DATA STORE CONSISTENCY CHECK (Final Step):\n",
        "After your initial evaluation, you will be shown the Data Store State *after* the AI assistant's actions. You will then verify if this final state is consistent with the AI's stated actions and tool use.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "zMWjRsv3QNc1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The GenerativeModel instance for evaluation will be created with the system instruction.\n",
        "eval_model_instance = gemini.GenerativeModel(\n",
        "    model_name=EVAL_MODEL_NAME,\n",
        "    system_instruction=evaluator_system_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "lHWwoW0JQRS0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Data Stores (Initial data - will be managed by the Storage class instance)\n",
        "# These are initial values. The Storage class will manage them.\n",
        "initial_customers = {\n",
        "    \"C1\": {\"name\": \"John Doe\", \"email\": \"john@example.com\", \"phone\": \"123-456-7890\"},\n",
        "    \"C2\": {\"name\": \"Jane Smith\", \"email\": \"jane@example.com\", \"phone\": \"987-654-3210\"}\n",
        "}\n",
        "\n",
        "initial_products = {\n",
        "    \"P1\": {\"name\": \"Widget A\", \"description\": \"A simple widget. Very compact.\", \"price\": 19.99, \"inventory_count\": 999},\n",
        "    \"P2\": {\"name\": \"Gadget B\", \"description\": \"A powerful gadget. It spins.\", \"price\": 49.99, \"inventory_count\": 200},\n",
        "    \"P3\": {\"name\": \"Perplexinator\", \"description\": \"A perplexing perfunctator\", \"price\": 79.99, \"inventory_count\": 1483}\n",
        "}\n",
        "\n",
        "initial_orders = {\n",
        "    \"O1\": {\"id\": \"O1\", \"product_id\": \"P1\", \"product_name\": \"Widget A\", \"quantity\": 2, \"price\": 19.99, \"status\": \"Shipped\"},\n",
        "    \"O2\": {\"id\": \"O2\", \"product_id\": \"P2\", \"product_name\": \"Gadget B\", \"quantity\": 1, \"price\": 49.99, \"status\": \"Processing\"}\n",
        "}\n"
      ],
      "metadata": {
        "id": "5G9rP40vQXdU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Knowledge base and Global Tools Placeholder\n",
        "human_feedback_learnings = {}\n",
        "tools_schemas_list = []"
      ],
      "metadata": {
        "id": "xZOKKplTQc63"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standalone Anthropic Completion Function (for basic tests)\n",
        "def get_completion_anthropic_standalone(prompt: str):\n",
        "    message = anthropic_client.messages.create(\n",
        "        model=ANTHROPIC_MODEL_NAME,\n",
        "        max_tokens=2000,\n",
        "        temperature=0.0,\n",
        "        system=worker_system_prompt,\n",
        "        tools=tools_schemas_list,\n",
        "        messages=[\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return message.content[0].text"
      ],
      "metadata": {
        "id": "_ADM0bBpQlVK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_test_anthropic = \"Hey there, which AI model do you use for answering questions?\"\n",
        "print(f\"Anthropic Standalone Test: {get_completion_anthropic_standalone(prompt_test_anthropic)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPOq1s0SQqk_",
        "outputId": "c03735c6-ca1a-4e17-f103-8737051e27f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anthropic Standalone Test: I am a customer service assistant designed to help with e-commerce related questions and tasks. I aim to be helpful by answering questions and performing actions related to orders, products, customers, and other e-commerce functions. I don't actually have information about which specific AI model I use - I focus on helping users with their e-commerce needs.\n",
            "\n",
            "Is there something specific about your orders, products, or other e-commerce matters that I can help you with today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def get_completion_openai_standalone(prompt: str):\n",
        "#    response = openai_client.chat.completions.create(\n",
        "#        model=OPENAI_MODEL_NAME,\n",
        "#        max_tokens=2000,\n",
        "#        temperature=0.0,\n",
        "#        tools=tools_schemas_list,\n",
        "#        messages=[\n",
        "#            {\"role\": \"system\", \"content\": worker_system_prompt},\n",
        "#            {\"role\": \"user\", \"content\": prompt}\n",
        "#        ]\n",
        "#    )\n",
        "#    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Y5c_bv3qQwWV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt_test_openai = \"Hey there, which AI model do you use for answering questions?\"\n",
        "#print(f\"OpenAI Standalone Test: {get_completion_openai_standalone(prompt_test_openai)}\")"
      ],
      "metadata": {
        "id": "b_LaDQ74Q1Lp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_eval_standalone(prompt: str):\n",
        "    # Uses the eval_model_instance defined in Cell 4 which has the system prompt\n",
        "        response = eval_model_instance.generate_content(prompt)\n",
        "        return response.text"
      ],
      "metadata": {
        "id": "XjcV5GcaQ6aT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_test_eval = \"Hey there, can you tell me which AI you are and what your key tasks are?\"\n",
        "print(f\"Gemini Eval Standalone Test:\\n{get_completion_eval_standalone(prompt_test_eval)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "Cu8doX53RE0Z",
        "outputId": "15afceba-0786-4799-e6c0-4c29c46e9459"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini Eval Standalone Test:\n",
            "Hello! I am a friendly customer service AI.\n",
            "\n",
            "My key tasks are to help you with:\n",
            "*   Placing new orders\n",
            "*   Checking the status of your existing orders\n",
            "*   Providing information about our products (e.g., price, availability, features)\n",
            "*   Answering frequently asked questions\n",
            "\n",
            "How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Storage Class Definition\n",
        "class Storage:\n",
        "    \"\"\"Storage class for global e-commerce data access\"\"\"\n",
        "    def __init__(self):\n",
        "        # Each Storage instance gets its own copy of the initial data\n",
        "        self.customers = initial_customers.copy()\n",
        "        self.products = initial_products.copy()\n",
        "        self.orders = initial_orders.copy()\n",
        "        # Note: human_feedback_learnings is still a shared global dictionary\n",
        "        self.human_feedback_learnings = human_feedback_learnings\n",
        "\n",
        "# This global instance is for legacy/standalone tool testing if any.\n",
        "# The DualAgentEvaluator will create its own instances for Anthropic and OpenAI.\n",
        "storage_global_for_standalone_tests = Storage()\n",
        "print(\"Storage class defined. Note: DualAgentEvaluator will use its own Storage instances.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpVJLbF6ROgf",
        "outputId": "ab90d352-c40e-4daf-9864-ae269adf160d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Storage class defined. Note: DualAgentEvaluator will use its own Storage instances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Definitive list of tool schemas.\n",
        "tools_schemas_list = [\n",
        "    {\n",
        "        \"name\": \"create_customer\",\n",
        "        \"description\": \"Adds a new customer to the database. Includes customer name, email, and (optional) phone number.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The name of the customer.\"},\n",
        "                \"email\": {\"type\": \"string\", \"description\": \"The email address of the customer.\"},\n",
        "                \"phone\": {\"type\": \"string\", \"description\": \"The phone number of the customer (optional).\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"email\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_customer_info\",\n",
        "        \"description\": \"Retrieves customer information based on their customer ID. Returns the customer's name, email, and (optional) phone number.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"customer_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the customer.\"}\n",
        "            },\n",
        "            \"required\": [\"customer_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_product\",\n",
        "        \"description\": \"Adds a new product to the product database. Includes name, description, price, and initial inventory count.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The name of the product.\"},\n",
        "                \"description\": {\"type\": \"string\", \"description\": \"A description of the product.\"},\n",
        "                \"price\": {\"type\": \"number\", \"description\": \"The price of the product.\"},\n",
        "                \"inventory_count\": {\"type\": \"integer\", \"description\": \"The amount of the product that is currently in inventory.\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"description\", \"price\", \"inventory_count\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"update_product\",\n",
        "        \"description\": \"Updates an existing product with new information. Only fields that are provided will be updated; other fields remain unchanged.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the product to update.\"},\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The new name for the product (optional).\"},\n",
        "                \"description\": {\"type\": \"string\", \"description\": \"The new description for the product (optional).\"},\n",
        "                \"price\": {\"type\": \"number\", \"description\": \"The new price for the product (optional).\"},\n",
        "                \"inventory_count\": {\"type\": \"integer\", \"description\": \"The new inventory count for the product (optional).\"}\n",
        "            },\n",
        "            \"required\": [\"product_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_product_info\",\n",
        "        \"description\": \"Retrieves product information based on product ID or product name (with fuzzy matching for misspellings). Returns product details including name, description, price, and inventory count.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id_or_name\": {\"type\": \"string\", \"description\": \"The product ID or name (can be approximate).\"}\n",
        "            },\n",
        "            \"required\": [\"product_id_or_name\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"list_all_products\",\n",
        "        \"description\": \"Lists all available products in the inventory.\",\n",
        "        \"input_schema\": { \"type\": \"object\", \"properties\": {}, \"required\": [] }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_order\",\n",
        "        \"description\": \"Creates an order using the product's current price. If requested quantity exceeds available inventory, no order is created and available quantity is returned. Orders can only be created for products that are in stock. Supports specifying products by either ID or name with fuzzy matching for misspellings.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id_or_name\": {\"type\": \"string\", \"description\": \"The ID or name of the product to order (supports fuzzy matching).\"},\n",
        "                \"quantity\": {\"type\": \"integer\", \"description\": \"The quantity of the product in the order.\"},\n",
        "                \"status\": {\"type\": \"string\", \"description\": \"The initial status of the order (e.g., 'Processing', 'Shipped').\"}\n",
        "            },\n",
        "            \"required\": [\"product_id_or_name\", \"quantity\", \"status\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_order_details\",\n",
        "        \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"order_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the order.\"}\n",
        "            },\n",
        "            \"required\": [\"order_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"update_order_status\",\n",
        "        \"description\": \"Updates the status of an order and adjusts inventory accordingly. Changing to \\\"Shipped\\\" decreases inventory. Changing to \\\"Returned\\\" or \\\"Canceled\\\" from \\\"Shipped\\\" increases inventory. Status can be \\\"Processing\\\", \\\"Shipped\\\", \\\"Delivered\\\", \\\"Returned\\\", or \\\"Canceled\\\".\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"order_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the order.\"},\n",
        "                \"new_status\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The new status to set for the order.\",\n",
        "                    \"enum\": [\"Processing\", \"Shipped\", \"Delivered\", \"Returned\", \"Canceled\"]\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"order_id\", \"new_status\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "print(f\"Defined {len(tools_schemas_list)} tool schemas.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgk-hLUBRTCY",
        "outputId": "b789fcb6-b436-42d0-f4b6-c02b8db02c26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined 9 tool schemas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool Function Definitions\n",
        "# These tool functions now accept a 'current_storage' argument to operate on a specific Storage instance.\n",
        "\n",
        "# Customer functions\n",
        "def create_customer(current_storage: Storage, name: str, email: str, phone: Optional[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Creates a new customer and adds them to the customer database.\"\"\"\n",
        "    new_id = f\"C{len(current_storage.customers) + 1}\"\n",
        "    current_storage.customers[new_id] = {\"name\": name, \"email\": email, \"phone\": phone}\n",
        "    print(f\"[Tool Executed] create_customer: ID {new_id}, Name: {name} (in {type(current_storage).__name__})\")\n",
        "    return {\"status\": \"success\", \"customer_id\": new_id, \"customer\": current_storage.customers[new_id]}\n",
        "\n",
        "def get_customer_info(current_storage: Storage, customer_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"Retrieves information about a customer based on their ID.\"\"\"\n",
        "    customer = current_storage.customers.get(customer_id)\n",
        "    if customer:\n",
        "        print(f\"[Tool Executed] get_customer_info: ID {customer_id} found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"customer_id\": customer_id, \"customer\": customer}\n",
        "    print(f\"[Tool Executed] get_customer_info: ID {customer_id} not found (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"error\", \"message\": \"Customer not found\"}\n",
        "\n",
        "# Product functions\n",
        "def create_product(current_storage: Storage, name: str, description: str, price: float, inventory_count: int) -> Dict[str, Any]:\n",
        "    \"\"\"Creates a new product and adds it to the product database.\"\"\"\n",
        "    new_id = f\"P{len(current_storage.products) + 1}\"\n",
        "    current_storage.products[new_id] = {\n",
        "        \"name\": name,\n",
        "        \"description\": description,\n",
        "        \"price\": float(price),\n",
        "        \"inventory_count\": int(inventory_count)\n",
        "    }\n",
        "    print(f\"[Tool Executed] create_product: ID {new_id}, Name: {name} (in {type(current_storage).__name__})\")\n",
        "    return {\"status\": \"success\", \"product_id\": new_id, \"product\": current_storage.products[new_id]}\n",
        "\n",
        "def update_product(current_storage: Storage, product_id: str, name: Optional[str] = None, description: Optional[str] = None,\n",
        "                   price: Optional[float] = None, inventory_count: Optional[int] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Updates a product with the provided parameters.\"\"\"\n",
        "    if product_id not in current_storage.products:\n",
        "        print(f\"[Tool Executed] update_product: ID {product_id} not found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": f\"Product {product_id} not found\"}\n",
        "\n",
        "    product = current_storage.products[product_id]\n",
        "    updated_fields = []\n",
        "\n",
        "    if name is not None:\n",
        "        product[\"name\"] = name\n",
        "        updated_fields.append(\"name\")\n",
        "    if description is not None:\n",
        "        product[\"description\"] = description\n",
        "        updated_fields.append(\"description\")\n",
        "    if price is not None:\n",
        "        product[\"price\"] = float(price)\n",
        "        updated_fields.append(\"price\")\n",
        "    if inventory_count is not None:\n",
        "        product[\"inventory_count\"] = int(inventory_count)\n",
        "        updated_fields.append(\"inventory_count\")\n",
        "\n",
        "    if not updated_fields:\n",
        "        print(f\"[Tool Executed] update_product: ID {product_id}, no fields updated (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"warning\", \"message\": \"No fields were updated.\", \"product\": product}\n",
        "\n",
        "    print(f\"[Tool Executed] update_product: ID {product_id}, Updated fields: {', '.join(updated_fields)} (in {type(current_storage).__name__})\")\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"message\": f\"Product {product_id} updated. Fields: {', '.join(updated_fields)}\",\n",
        "        \"product_id\": product_id,\n",
        "        \"updated_fields\": updated_fields,\n",
        "        \"product\": product\n",
        "    }\n",
        "\n",
        "def find_product_by_name(current_storage: Storage, product_name: str, min_similarity: int = 70) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
        "    \"\"\"Find a product by name using fuzzy string matching.\"\"\"\n",
        "    if not product_name: return None, None\n",
        "\n",
        "    name_id_list = [(p_data[\"name\"], p_id) for p_id, p_data in current_storage.products.items()]\n",
        "    if not name_id_list: return None, None\n",
        "\n",
        "    best_match_name_score = process.extractOne(\n",
        "        product_name,\n",
        "        [item[0] for item in name_id_list],\n",
        "        scorer=fuzz.token_sort_ratio\n",
        "    )\n",
        "\n",
        "    if best_match_name_score and best_match_name_score[1] >= min_similarity:\n",
        "        matched_name = best_match_name_score[0]\n",
        "        for name, pid_val in name_id_list:\n",
        "            if name == matched_name:\n",
        "                print(f\"[Tool Helper] find_product_by_name: Matched '{product_name}' to '{matched_name}' (ID: {pid_val}) with score {best_match_name_score[1]} (in {type(current_storage).__name__})\")\n",
        "                return pid_val, current_storage.products[pid_val]\n",
        "\n",
        "    print(f\"[Tool Helper] find_product_by_name: No good match for '{product_name}' (min_similarity: {min_similarity}, Best match: {best_match_name_score}) (in {type(current_storage).__name__})\")\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def get_product_id(current_storage: Storage, product_identifier: str) -> Optional[str]:\n",
        "    \"\"\"Get product ID either directly or by fuzzy matching the name.\"\"\"\n",
        "    if product_identifier in current_storage.products:\n",
        "        return product_identifier\n",
        "    product_id, _ = find_product_by_name(current_storage, product_identifier)\n",
        "    return product_id\n",
        "\n",
        "def get_product_info(current_storage: Storage, product_id_or_name: str) -> Dict[str, Any]:\n",
        "    \"\"\"Get information about a product by its ID or name.\"\"\"\n",
        "    if product_id_or_name in current_storage.products:\n",
        "        product = current_storage.products[product_id_or_name]\n",
        "        print(f\"[Tool Executed] get_product_info: Found by ID '{product_id_or_name}' (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"product_id\": product_id_or_name, \"product\": product}\n",
        "\n",
        "    # Use the modified find_product_by_name that takes current_storage\n",
        "    product_id_found, product_data = find_product_by_name(current_storage, product_id_or_name)\n",
        "    if product_id_found and product_data:\n",
        "        print(f\"[Tool Executed] get_product_info: Found by name (fuzzy) '{product_id_or_name}' as ID '{product_id_found}' (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"message\": f\"Found product matching '{product_id_or_name}'\", \"product_id\": product_id_found, \"product\": product_data}\n",
        "\n",
        "    print(f\"[Tool Executed] get_product_info: No product found for '{product_id_or_name}' (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"error\", \"message\": f\"No product found matching '{product_id_or_name}'\"}\n",
        "\n",
        "\n",
        "def list_all_products(current_storage: Storage) -> Dict[str, Any]:\n",
        "    \"\"\"List all available products in the inventory.\"\"\"\n",
        "    print(f\"[Tool Executed] list_all_products: Found {len(current_storage.products)} products (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"success\", \"count\": len(current_storage.products), \"products\": dict(current_storage.products)}\n",
        "\n",
        "# Order functions\n",
        "def create_order(current_storage: Storage, product_id_or_name: str, quantity: int, status: str) -> Dict[str, Any]:\n",
        "    \"\"\"Creates an order using the product's stored price.\"\"\"\n",
        "    actual_product_id = get_product_id(current_storage, product_id_or_name) # Pass current_storage\n",
        "\n",
        "    if not actual_product_id:\n",
        "        print(f\"[Tool Executed] create_order: Product '{product_id_or_name}' not found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": f\"Product '{product_id_or_name}' not found.\"}\n",
        "\n",
        "    product = current_storage.products[actual_product_id]\n",
        "    price = product[\"price\"]\n",
        "\n",
        "    if product[\"inventory_count\"] == 0:\n",
        "        print(f\"[Tool Executed] create_order: Product ID {actual_product_id} is out of stock (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": f\"{product['name']} is out of stock.\"}\n",
        "    if quantity <= 0:\n",
        "        print(f\"[Tool Executed] create_order: Quantity must be positive. Requested: {quantity} (in {type(current_storage).__name__})\")\n",
        "        return {\"status\": \"error\", \"message\": \"Quantity must be a positive number.\"}\n",
        "    if quantity > product[\"inventory_count\"]:\n",
        "        print(f\"[Tool Executed] create_order: Insufficient inventory for {product['name']} (ID: {actual_product_id}). Available: {product['inventory_count']}, Requested: {quantity} (in {type(current_storage).__name__})\")\n",
        "        return {\n",
        "            \"status\": \"partial_availability\",\n",
        "            \"message\": f\"Insufficient inventory. Only {product['inventory_count']} units of {product['name']} are available.\",\n",
        "            \"available_quantity\": product[\"inventory_count\"],\n",
        "            \"requested_quantity\": quantity,\n",
        "            \"product_name\": product['name']\n",
        "        }\n",
        "\n",
        "    if status == \"Shipped\":\n",
        "        product[\"inventory_count\"] -= quantity\n",
        "        print(f\"[Tool Executed] create_order: Inventory for {product['name']} (ID: {actual_product_id}) reduced by {quantity} due to 'Shipped' status on creation (in {type(current_storage).__name__}).\")\n",
        "\n",
        "    new_id = f\"O{len(current_storage.orders) + 1}\"\n",
        "    current_storage.orders[new_id] = {\n",
        "        \"id\": new_id,\n",
        "        \"product_id\": actual_product_id,\n",
        "        \"product_name\": product[\"name\"],\n",
        "        \"quantity\": quantity,\n",
        "        \"price\": price,\n",
        "        \"status\": status\n",
        "    }\n",
        "    print(f\"[Tool Executed] create_order: Order {new_id} created for {quantity} of {product['name']} (ID: {actual_product_id}). Status: {status}. Remaining inv: {product['inventory_count']} (in {type(current_storage).__name__})\")\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"order_id\": new_id,\n",
        "        \"order_details\": current_storage.orders[new_id],\n",
        "        \"remaining_inventory\": product[\"inventory_count\"]\n",
        "    }\n",
        "\n",
        "def get_order_details(current_storage: Storage, order_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"Get details of a specific order.\"\"\"\n",
        "    order = current_storage.orders.get(order_id)\n",
        "    if order:\n",
        "        print(f\"[Tool Executed] get_order_details: Order {order_id} found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"success\", \"order_id\": order_id, \"order_details\": dict(order)}\n",
        "    print(f\"[Tool Executed] get_order_details: Order {order_id} not found (in {type(current_storage).__name__}).\")\n",
        "    return {\"status\": \"error\", \"message\": \"Order not found\"}\n",
        "\n",
        "def update_order_status(current_storage: Storage, order_id: str, new_status: str) -> Dict[str, Any]:\n",
        "    \"\"\"Updates the status of an order and adjusts inventory accordingly.\"\"\"\n",
        "    if order_id not in current_storage.orders:\n",
        "        print(f\"[Tool Executed] update_order_status: Order {order_id} not found (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"error\", \"message\": \"Order not found\"}\n",
        "\n",
        "    order = current_storage.orders[order_id]\n",
        "    old_status = order[\"status\"]\n",
        "    product_id = order[\"product_id\"]\n",
        "    quantity = order[\"quantity\"]\n",
        "\n",
        "    if old_status == new_status:\n",
        "        print(f\"[Tool Executed] update_order_status: Order {order_id} status unchanged ({old_status}) (in {type(current_storage).__name__}).\")\n",
        "        return {\"status\": \"unchanged\", \"message\": f\"Order {order_id} status is already {old_status}\", \"order_details\": dict(order)}\n",
        "\n",
        "    inventory_adjusted = False\n",
        "    current_inventory_val = \"unknown\" # Default if product not found (should not happen if order is valid)\n",
        "\n",
        "    if product_id in current_storage.products:\n",
        "        product = current_storage.products[product_id]\n",
        "        current_inventory_val = product[\"inventory_count\"]\n",
        "\n",
        "        if new_status == \"Shipped\" and old_status not in [\"Shipped\", \"Delivered\"]:\n",
        "            if current_inventory_val < quantity:\n",
        "                print(f\"[Tool Executed] update_order_status: Insufficient inventory to ship order {order_id}. Have {current_inventory_val}, need {quantity} (in {type(current_storage).__name__}).\")\n",
        "                return {\"status\": \"error\", \"message\": f\"Insufficient inventory to ship. Available: {current_inventory_val}, Required: {quantity}\"}\n",
        "            product[\"inventory_count\"] -= quantity\n",
        "            inventory_adjusted = True\n",
        "            current_inventory_val = product[\"inventory_count\"]\n",
        "            print(f\"[Tool Executed] update_order_status: Order {order_id} Shipped. Inv for {product_id} reduced by {quantity} to {current_inventory_val} (in {type(current_storage).__name__}).\")\n",
        "        elif new_status in [\"Returned\", \"Canceled\"] and old_status in [\"Shipped\", \"Delivered\"]:\n",
        "            product[\"inventory_count\"] += quantity\n",
        "            inventory_adjusted = True\n",
        "            current_inventory_val = product[\"inventory_count\"]\n",
        "            print(f\"[Tool Executed] update_order_status: Order {order_id} {new_status}. Inv for {product_id} increased by {quantity} to {current_inventory_val} (in {type(current_storage).__name__}).\")\n",
        "    else:\n",
        "        print(f\"[Tool Executed] update_order_status: Product {product_id} for order {order_id} not found for inventory adjustment (in {type(current_storage).__name__}).\")\n",
        "\n",
        "    order[\"status\"] = new_status\n",
        "    print(f\"[Tool Executed] update_order_status: Order {order_id} status updated from {old_status} to {new_status} (in {type(current_storage).__name__}).\")\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"message\": f\"Order {order_id} status updated from {old_status} to {new_status}.\",\n",
        "        \"order_id\": order_id,\n",
        "        \"product_id\": product_id,\n",
        "        \"old_status\": old_status,\n",
        "        \"new_status\": new_status,\n",
        "        \"inventory_adjusted\": inventory_adjusted,\n",
        "        \"current_inventory\": current_inventory_val,\n",
        "        \"order_details\": dict(order)\n",
        "    }\n",
        "\n",
        "print(\"Tool functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RZI6SGzRf6j",
        "outputId": "7b9a1116-2e0f-4333-b6dd-43444b6c180b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationContext:\n",
        "    def __init__(self):\n",
        "        self.messages: List[Dict[str, Any]] = []\n",
        "        self.context_data: Dict[str, Any] = {\n",
        "            \"customers\": {}, \"products\": {}, \"orders\": {}, \"last_action\": None\n",
        "        }\n",
        "        self.session_start_time = datetime.now()\n",
        "\n",
        "    def add_user_message(self, message: str) -> None:\n",
        "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    def add_assistant_message(self, message_content: Union[str, List[Dict[str, Any]]]) -> None:\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": message_content})\n",
        "\n",
        "    def update_entity_in_context(self, entity_type: str, entity_id: str, data: Any) -> None:\n",
        "        if entity_type in self.context_data:\n",
        "            self.context_data[entity_type][entity_id] = data # Store the actual data\n",
        "            print(f\"[Context Updated] Entity: {entity_type}, ID: {entity_id}, Data (type): {type(data)}\")\n",
        "\n",
        "    def set_last_action(self, action_type: str, action_details: Any) -> None:\n",
        "        self.context_data[\"last_action\"] = {\n",
        "            \"type\": action_type,\n",
        "            \"details\": action_details,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        print(f\"[Context Updated] Last Action: {action_type}, Details: {json.dumps(action_details, default=str)}\")\n",
        "\n",
        "\n",
        "    def get_full_conversation_for_api(self) -> List[Dict[str, Any]]:\n",
        "        return self.messages.copy()\n",
        "\n",
        "    def get_context_summary(self) -> str:\n",
        "        summary_parts = []\n",
        "        if self.context_data[\"customers\"]:\n",
        "            customers_str = \", \".join([f\"ID: {cid} (Name: {c.get('name', 'N/A') if isinstance(c, dict) else 'N/A'})\" for cid, c in self.context_data[\"customers\"].items()])\n",
        "            summary_parts.append(f\"Recent customers: {customers_str}\")\n",
        "        if self.context_data[\"products\"]:\n",
        "            products_str = \", \".join([f\"ID: {pid} (Name: {p.get('name', 'N/A') if isinstance(p, dict) else 'N/A'})\" for pid, p in self.context_data[\"products\"].items()])\n",
        "            summary_parts.append(f\"Recent products: {products_str}\")\n",
        "        if self.context_data[\"orders\"]:\n",
        "            orders_str = \", \".join([f\"ID: {oid} (Product: {o.get('product_name', 'N/A') if isinstance(o, dict) else 'N/A'}, Status: {o.get('status', 'N/A') if isinstance(o, dict) else 'N/A'})\" for oid, o in self.context_data[\"orders\"].items()])\n",
        "            summary_parts.append(f\"Recent orders: {orders_str}\")\n",
        "\n",
        "        last_action = self.context_data[\"last_action\"]\n",
        "        if last_action:\n",
        "            action_type = last_action['type']\n",
        "            action_details_summary = \"...\" # Default summary\n",
        "            if isinstance(last_action.get('details'), dict):\n",
        "                action_input = last_action['details'].get('input', {})\n",
        "                action_result_status = last_action['details'].get('result', {}).get('status')\n",
        "                action_details_summary = f\"Input: {action_input}, Result Status: {action_result_status}\"\n",
        "                if action_result_status == \"success\":\n",
        "                    if \"order_id\" in last_action['details'].get('result', {}):\n",
        "                         action_details_summary += f\", OrderID: {last_action['details']['result']['order_id']}\"\n",
        "                    elif \"product_id\" in last_action['details'].get('result', {}):\n",
        "                         action_details_summary += f\", ProductID: {last_action['details']['result']['product_id']}\"\n",
        "\n",
        "\n",
        "            summary_parts.append(f\"Last action: {action_type} at {last_action['timestamp']} ({action_details_summary})\")\n",
        "\n",
        "        if not summary_parts: return \"No specific context items set yet.\"\n",
        "        return \"\\n\".join(summary_parts)\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        self.messages = []\n",
        "        self.context_data = {\"customers\": {}, \"products\": {}, \"orders\": {}, \"last_action\": None}\n",
        "        self.session_start_time = datetime.now()\n",
        "        print(\"[Context Cleared]\")\n",
        "\n",
        "print(\"ConversationContext class defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFpBL_-HSGPY",
        "outputId": "da57cc7c-35ab-44e5-851e-a5fb91ed6b39"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConversationContext class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentEvaluator:\n",
        "    def __init__(self):\n",
        "        self.conversation_context = ConversationContext()\n",
        "        self.evaluation_results = []\n",
        "        self.anthropic_storage = Storage() # Worker agent's e-commerce data\n",
        "        print(\"AgentEvaluator initialized with Storage for Anthropic agent.\")\n",
        "\n",
        "        self.anthropic_tools_schemas = tools_schemas_list\n",
        "\n",
        "        self.available_tool_functions = {\n",
        "            \"create_customer\": create_customer, \"get_customer_info\": get_customer_info,\n",
        "            \"create_product\": create_product, \"update_product\": update_product,\n",
        "            \"get_product_info\": get_product_info, \"list_all_products\": list_all_products,\n",
        "            \"create_order\": create_order, \"get_order_details\": get_order_details,\n",
        "            \"update_order_status\": update_order_status,\n",
        "        }\n",
        "\n",
        "        self._mount_drive_if_needed()\n",
        "        self._initialize_learnings_path()\n",
        "\n",
        "        # --- In-session learnings cache ---\n",
        "        self.active_learnings_cache: List[Dict] = self._load_initial_learnings_from_drive()\n",
        "        self.learnings_updated_this_session_flag: bool = False\n",
        "\n",
        "        print(f\"AgentEvaluator initialized. Learnings path: {LEARNINGS_DRIVE_BASE_PATH}. Loaded {len(self.active_learnings_cache)} initial learnings into cache.\")\n",
        "\n",
        "    def _mount_drive_if_needed(self):\n",
        "        \"\"\"Mounts Google Drive if not already mounted.\"\"\"\n",
        "        if not os.path.exists(DRIVE_MOUNT_PATH) or not os.listdir(DRIVE_MOUNT_PATH): # Basic check\n",
        "            try:\n",
        "                drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n",
        "                print(f\"Google Drive mounted successfully at {DRIVE_MOUNT_PATH}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error mounting Google Drive: {e}. RAG features will not work.\")\n",
        "        else:\n",
        "            print(f\"Google Drive already mounted at {DRIVE_MOUNT_PATH}.\")\n",
        "\n",
        "    def _initialize_learnings_path(self):\n",
        "        \"\"\"Initializes and ensures the learnings directory exists.\"\"\"\n",
        "        global LEARNINGS_DRIVE_BASE_PATH\n",
        "        # Check if LEARNINGS_DRIVE_BASE_PATH is already a full valid path from /content/drive/\n",
        "        if LEARNINGS_DRIVE_BASE_PATH and \\\n",
        "           LEARNINGS_DRIVE_BASE_PATH.startswith(DRIVE_MOUNT_PATH) and \\\n",
        "           os.path.exists(os.path.dirname(LEARNINGS_DRIVE_BASE_PATH)):\n",
        "             pass # Already seems to be a full path correctly set\n",
        "        elif not LEARNINGS_DRIVE_BASE_PATH:\n",
        "            learnings_path_input = input(f\"Enter the path within your Google Drive for storing learnings (e.g., 'My Drive/AI_Learnings') or press Enter to use default '{DEFAULT_LEARNINGS_DRIVE_SUBPATH}': \").strip()\n",
        "            if not learnings_path_input:\n",
        "                LEARNINGS_DRIVE_BASE_PATH = os.path.join(DRIVE_MOUNT_PATH, DEFAULT_LEARNINGS_DRIVE_SUBPATH)\n",
        "            else:\n",
        "                if not learnings_path_input.lower().startswith('my drive') and \\\n",
        "                   not learnings_path_input.startswith('/') and \\\n",
        "                   not learnings_path_input.startswith(DRIVE_MOUNT_PATH):\n",
        "                    LEARNINGS_DRIVE_BASE_PATH = os.path.join(DRIVE_MOUNT_PATH, \"My Drive\", learnings_path_input)\n",
        "                elif not learnings_path_input.startswith(DRIVE_MOUNT_PATH):\n",
        "                     LEARNINGS_DRIVE_BASE_PATH = os.path.join(DRIVE_MOUNT_PATH, learnings_path_input)\n",
        "                else:\n",
        "                    LEARNINGS_DRIVE_BASE_PATH = learnings_path_input\n",
        "\n",
        "        if not os.path.exists(LEARNINGS_DRIVE_BASE_PATH):\n",
        "            try:\n",
        "                os.makedirs(LEARNINGS_DRIVE_BASE_PATH)\n",
        "                print(f\"Created learnings directory: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error creating learnings directory {LEARNINGS_DRIVE_BASE_PATH}: {e}\")\n",
        "        else:\n",
        "            print(f\"Learnings directory found: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "\n",
        "    def _get_latest_learnings_filepath_from_drive(self) -> Optional[str]:\n",
        "        \"\"\"Finds the most recent learnings JSON file in the directory.\"\"\"\n",
        "        if not LEARNINGS_DRIVE_BASE_PATH or not os.path.isdir(LEARNINGS_DRIVE_BASE_PATH):\n",
        "            return None\n",
        "        list_of_files = glob.glob(os.path.join(LEARNINGS_DRIVE_BASE_PATH, 'learnings_*.json'))\n",
        "        if not list_of_files: return None\n",
        "        return max(list_of_files, key=os.path.getctime)\n",
        "\n",
        "    def _read_learnings_from_drive_file(self, filepath: str) -> List[Dict]:\n",
        "        \"\"\"Reads and parses a list of learning JSON objects from a given file.\"\"\"\n",
        "        if not filepath or not os.path.exists(filepath): return []\n",
        "        try:\n",
        "            with open(filepath, 'r') as f: learnings_list = json.load(f)\n",
        "            return learnings_list if isinstance(learnings_list, list) else []\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error decoding JSON from learnings file: {filepath}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading learnings file {filepath}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _load_initial_learnings_from_drive(self) -> List[Dict]:\n",
        "        \"\"\"Loads the latest learnings from Drive into the in-memory cache at startup.\"\"\"\n",
        "        print(\"[RAG Cache] Initializing: Loading latest learnings from Drive...\")\n",
        "        latest_filepath = self._get_latest_learnings_filepath_from_drive()\n",
        "        if latest_filepath:\n",
        "            print(f\"[RAG Cache] Loading initial learnings from: {latest_filepath}\")\n",
        "            return self._read_learnings_from_drive_file(latest_filepath)\n",
        "        print(\"[RAG Cache] No existing learnings file found on Drive for initial load.\")\n",
        "        return []\n",
        "\n",
        "    def _persist_active_learnings_to_drive(self):\n",
        "        \"\"\"Saves the current active_learnings_cache to a new timestamped file on Drive.\"\"\"\n",
        "        if not LEARNINGS_DRIVE_BASE_PATH or not os.path.isdir(LEARNINGS_DRIVE_BASE_PATH):\n",
        "            self._initialize_learnings_path()\n",
        "            if not os.path.isdir(LEARNINGS_DRIVE_BASE_PATH):\n",
        "                print(\"CRITICAL: Learnings directory still not available. Aborting RAG persistence.\")\n",
        "                return\n",
        "\n",
        "        if not self.active_learnings_cache:\n",
        "            print(\"[RAG Cache] Active learnings cache is empty. Nothing to persist to Drive.\")\n",
        "            self.learnings_updated_this_session_flag = False # Still reset flag if cache is empty\n",
        "            return\n",
        "\n",
        "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
        "        new_filepath = os.path.join(LEARNINGS_DRIVE_BASE_PATH, f'learnings_{timestamp_str}.json')\n",
        "        try:\n",
        "            with open(new_filepath, 'w') as f: json.dump(self.active_learnings_cache, f, indent=4)\n",
        "            print(f\"[RAG Cache] Successfully persisted {len(self.active_learnings_cache)} active learnings to new file: {new_filepath}\")\n",
        "            self.learnings_updated_this_session_flag = False\n",
        "        except Exception as e:\n",
        "            print(f\"Error persisting active learnings to {new_filepath}: {e}\")\n",
        "\n",
        "    def check_relevant_learnings(self, query: str, count: int = 5) -> Optional[str]:\n",
        "        \"\"\"Retrieves relevant learnings from the IN-MEMORY CACHE and formats them for prompts.\"\"\"\n",
        "        if not self.active_learnings_cache:\n",
        "            return None\n",
        "\n",
        "        keywords_from_query = self.extract_keywords(query)\n",
        "        relevant_learning_objects = []\n",
        "        for entry in self.active_learnings_cache:\n",
        "            text_to_search = entry.get(\"final_learning_statement\", \"\") + \" \" + \\\n",
        "                             entry.get(\"original_human_input\", \"\") + \" \" + \\\n",
        "                             \" \".join(entry.get(\"keywords\", []))\n",
        "            if any(kw.lower() in text_to_search.lower() for kw in keywords_from_query):\n",
        "                relevant_learning_objects.append(entry)\n",
        "\n",
        "        relevant_learning_objects.sort(key=lambda x: x.get('timestamp_created', ''), reverse=True)\n",
        "        formatted_learnings = [f\"- Learning (from {entry.get('timestamp_created', 'N/A')}): {entry.get('final_learning_statement', entry.get('original_human_input', str(entry)))}\" for entry in relevant_learning_objects[:count]]\n",
        "\n",
        "        return \"\\nRelevant Learnings from Knowledge Base (In-Session):\\n\" + \"\\n\".join(formatted_learnings) if formatted_learnings else None\n",
        "\n",
        "    def _update_context_from_tool_results(self, tool_name: str, tool_input: Dict, tool_result: Dict):\n",
        "        agent_name = \"Anthropic\"\n",
        "        if not isinstance(tool_result, dict):\n",
        "            print(f\"[Context Update Error] Tool result for {tool_name} ({agent_name}) is not a dict: {tool_result}\")\n",
        "            self.conversation_context.set_last_action(f\"{tool_name}_{agent_name}\", {\"input\": tool_input, \"result\": {\"status\": \"error\", \"message\": \"Tool result was not a dictionary.\"}})\n",
        "            return\n",
        "        if tool_result.get(\"status\") == \"success\":\n",
        "            if \"customer_id\" in tool_result and \"customer\" in tool_result and isinstance(tool_result[\"customer\"], dict):\n",
        "                self.conversation_context.update_entity_in_context(\"customers\", tool_result[\"customer_id\"], tool_result[\"customer\"])\n",
        "            elif \"product_id\" in tool_result and \"product\" in tool_result and isinstance(tool_result[\"product\"], dict):\n",
        "                self.conversation_context.update_entity_in_context(\"products\", tool_result[\"product_id\"], tool_result[\"product\"])\n",
        "            elif \"order_id\" in tool_result and \"order_details\" in tool_result and isinstance(tool_result[\"order_details\"], dict):\n",
        "                self.conversation_context.update_entity_in_context(\"orders\", tool_result[\"order_id\"], tool_result[\"order_details\"])\n",
        "            elif tool_name == \"list_all_products\" and \"products\" in tool_result and isinstance(tool_result[\"products\"], dict):\n",
        "                 for pid, pdata in tool_result[\"products\"].items():\n",
        "                     self.conversation_context.update_entity_in_context(\"products\", pid, pdata)\n",
        "        self.conversation_context.set_last_action(f\"{tool_name}_{agent_name}\", {\"input\": tool_input, \"result\": tool_result})\n",
        "\n",
        "    def process_tool_call(self, tool_name: str, tool_input: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        target_storage_instance = self.anthropic_storage\n",
        "        print(f\"--- [Tool Dispatcher] Attempting tool: {tool_name} with input: {json.dumps(tool_input, default=str)} for storage: AnthropicStorage ---\")\n",
        "        if tool_name in self.available_tool_functions:\n",
        "            function_to_call = self.available_tool_functions[tool_name]\n",
        "            try:\n",
        "                result = function_to_call(target_storage_instance, **tool_input)\n",
        "                print(f\"--- [Tool Dispatcher] Result for {tool_name} on AnthropicStorage: {json.dumps(result, indent=2, default=str)} ---\")\n",
        "                return result\n",
        "            except TypeError as te:\n",
        "                print(f\"--- [Tool Dispatcher] TypeError for {tool_name} on AnthropicStorage: {te}. Input: {tool_input} ---\")\n",
        "                return {\"status\": \"error\", \"message\": f\"TypeError calling {tool_name}: {str(te)}. Check arguments.\"}\n",
        "            except Exception as e:\n",
        "                print(f\"--- [Tool Dispatcher] Exception for {tool_name} on AnthropicStorage: {e} ---\")\n",
        "                return {\"status\": \"error\", \"message\": f\"Error executing {tool_name}: {str(e)}\"}\n",
        "        else:\n",
        "            print(f\"--- [Tool Dispatcher] Tool {tool_name} not found. ---\")\n",
        "            return {\"status\": \"error\", \"message\": f\"Tool {tool_name} not found.\"}\n",
        "\n",
        "    def get_anthropic_response(self, current_worker_system_prompt: str, conversation_history: List[Dict[str, Any]]) -> str:\n",
        "        messages_for_api = list(conversation_history)\n",
        "        try:\n",
        "            for i in range(5):\n",
        "                # print(f\"\\nAnthropic API Call #{i+1}. Messages count: {len(messages_for_api)}\") # Verbose\n",
        "                response = anthropic_client.messages.create(\n",
        "                    model=ANTHROPIC_MODEL_NAME, max_tokens=4000,\n",
        "                    system=current_worker_system_prompt,\n",
        "                    tools=self.anthropic_tools_schemas,\n",
        "                    messages=messages_for_api\n",
        "                )\n",
        "                assistant_response_blocks = response.content\n",
        "                messages_for_api.append({\"role\": \"assistant\", \"content\": assistant_response_blocks})\n",
        "\n",
        "                text_blocks = [block.text for block in assistant_response_blocks if block.type == \"text\"]\n",
        "                final_text_response = \" \".join(text_blocks).strip()\n",
        "\n",
        "                if final_text_response.startswith(\"CLARIFICATION_REQUESTED:\"):\n",
        "                    # print(f\"Anthropic requests clarification: {final_text_response}\") # Verbose\n",
        "                    return final_text_response\n",
        "\n",
        "                tool_calls_to_process = [block for block in assistant_response_blocks if block.type == \"tool_use\"]\n",
        "                if not tool_calls_to_process:\n",
        "                    # print(f\"Anthropic Final Text (no tool use this turn): {final_text_response}\") # Verbose\n",
        "                    return final_text_response if final_text_response else \"No text content in final Anthropic response.\"\n",
        "\n",
        "                tool_results_for_next_call = []\n",
        "                for tool_use_block in tool_calls_to_process:\n",
        "                    tool_name, tool_input, tool_use_id = tool_use_block.name, tool_use_block.input, tool_use_block.id\n",
        "                    print(f\"Anthropic Tool Call: {tool_name}, Input: {tool_input}\")\n",
        "                    tool_result_data = self.process_tool_call(tool_name, tool_input)\n",
        "                    self._update_context_from_tool_results(tool_name, tool_input, tool_result_data)\n",
        "                    tool_results_for_next_call.append({\n",
        "                        \"type\": \"tool_result\", \"tool_use_id\": tool_use_id,\n",
        "                        \"content\": json.dumps(tool_result_data)\n",
        "                    })\n",
        "                messages_for_api.append({\"role\": \"user\", \"content\": tool_results_for_next_call})\n",
        "            return \"Max tool iterations reached for Anthropic.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error in get_anthropic_response: {str(e)}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            return f\"Error getting Anthropic response: {str(e)}\"\n",
        "\n",
        "    def _handle_worker_clarification(self, agent_question: str, current_prompt: str,\n",
        "                                     agent_specific_history: List[Dict], max_attempts: int = 2) -> Tuple[str, List[Dict]]:\n",
        "        agent_name = \"Anthropic\"\n",
        "        print(f\"\\n--- {agent_name} requests clarification ---\")\n",
        "        print(f\"{agent_name}: {agent_question}\")\n",
        "        clarification_turn_details = []\n",
        "        for attempt in range(max_attempts):\n",
        "            user_clarification = input(f\"Your response to {agent_name}: \").strip()\n",
        "            if not user_clarification: user_clarification = \"(User provided no input to clarification)\"\n",
        "            clarification_turn_details.append({\"agent_question\": agent_question, \"user_answer\": user_clarification})\n",
        "            agent_specific_history.append({\"role\": \"user\", \"content\": user_clarification})\n",
        "            print(f\"Sending clarification back to {agent_name} (Attempt {attempt + 1}/{max_attempts})...\")\n",
        "            agent_response_text = self.get_anthropic_response(current_prompt, agent_specific_history)\n",
        "            if agent_response_text.startswith(\"CLARIFICATION_REQUESTED:\"):\n",
        "                agent_question = agent_response_text.split(\"CLARIFICATION_REQUESTED:\", 1)[-1].strip()\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(f\"\\n--- {agent_name} requests further clarification ---\")\n",
        "                    print(f\"{agent_name}: {agent_question}\")\n",
        "                else:\n",
        "                    print(f\"{agent_name} reached max clarification attempts. Proceeding with last question as response.\")\n",
        "                    return agent_response_text, clarification_turn_details\n",
        "            else:\n",
        "                return agent_response_text, clarification_turn_details\n",
        "        return agent_response_text, clarification_turn_details\n",
        "\n",
        "    def _get_llm_response_with_clarification_loop(self, system_prompt_with_context_learnings: str,\n",
        "                                                 base_conversation_history: List[Dict]) -> Tuple[str, List[Dict]]:\n",
        "        agent_specific_history = list(base_conversation_history)\n",
        "        final_agent_response = self.get_anthropic_response(system_prompt_with_context_learnings, agent_specific_history)\n",
        "        clarification_interactions = []\n",
        "        if final_agent_response.startswith(\"CLARIFICATION_REQUESTED:\"):\n",
        "            agent_question = final_agent_response.split(\"CLARIFICATION_REQUESTED:\", 1)[-1].strip()\n",
        "            final_agent_response, clarification_interactions = self._handle_worker_clarification(\n",
        "                agent_question, system_prompt_with_context_learnings,\n",
        "                agent_specific_history\n",
        "            )\n",
        "        return final_agent_response, clarification_interactions\n",
        "\n",
        "    def process_user_request(self, user_message: str) -> Dict[str, Any]:\n",
        "        print(f\"\\n\\n{'='*60}\\nUser Message: {user_message}\\n{'='*60}\")\n",
        "        self.conversation_context.add_user_message(user_message)\n",
        "        context_summary = self.conversation_context.get_context_summary()\n",
        "        print(f\"Current Context Summary for Agent:\\n{context_summary}\\n{'-'*60}\")\n",
        "\n",
        "        learnings_for_prompt = self.check_relevant_learnings(user_message)\n",
        "        if learnings_for_prompt:\n",
        "            print(f\"Relevant Learnings for this turn (from In-Session Cache):\\n{learnings_for_prompt}\")\n",
        "        else:\n",
        "            print(\"No specific relevant learnings found from In-Session Cache for this turn.\")\n",
        "\n",
        "        current_worker_prompt_with_context_and_learnings = f\"{worker_system_prompt}\\n\\nConversation Context:\\n{context_summary}\\n\\n{learnings_for_prompt if learnings_for_prompt else 'No specific past learnings provided for this query.'}\"\n",
        "\n",
        "        self.learnings_updated_this_session_flag = False\n",
        "\n",
        "        print(\"\\n--- Getting Anthropic's Response ---\")\n",
        "        anthropic_base_history = self.conversation_context.get_full_conversation_for_api()\n",
        "        anthropic_final_response, anthropic_clarifications = self._get_llm_response_with_clarification_loop(\n",
        "            current_worker_prompt_with_context_and_learnings, anthropic_base_history\n",
        "        )\n",
        "        self.conversation_context.add_assistant_message(f\"[Anthropic Final Text (after any clarifications)]: {anthropic_final_response}\")\n",
        "        if anthropic_clarifications:\n",
        "            print(f\"[INFO] Anthropic clarification interactions: {json.dumps(anthropic_clarifications, indent=2)}\")\n",
        "            for interaction in anthropic_clarifications:\n",
        "                self.process_and_store_new_learning(\n",
        "                    human_feedback_text=f\"User clarification for Anthropic: '{interaction['user_answer']}' (in response to agent question: '{interaction['agent_question']}')\",\n",
        "                    user_query_context=user_message,\n",
        "                    turn_context_summary=context_summary + f\"\\nAnthropic asked: {interaction['agent_question']}\"\n",
        "                )\n",
        "\n",
        "        print(f\"\\n--- Anthropic Final Response Text (Post-Clarification if any) ---\\n{anthropic_final_response}\")\n",
        "\n",
        "        evaluation = self.evaluate_responses(user_message, anthropic_final_response,\n",
        "                                             context_summary, learnings_for_prompt or \"\",\n",
        "                                             anthropic_clarifications)\n",
        "        self.evaluation_results.append(evaluation)\n",
        "\n",
        "        human_feedback_candidate = \"\"\n",
        "        clarif_details_eval = evaluation.get(\"clarification_details\", {})\n",
        "        if clarif_details_eval.get(\"used\") and clarif_details_eval.get(\"provided_input\") not in [\"Skipped by user\", \"Skipped (non-interactive)\"]:\n",
        "            human_feedback_candidate = clarif_details_eval.get(\"provided_input\")\n",
        "            print(f\"[INFO] Candidate learning from *evaluator* clarification: '{human_feedback_candidate}'\")\n",
        "            self.process_and_store_new_learning(\n",
        "                human_feedback_text=human_feedback_candidate, user_query_context=user_message, turn_context_summary=context_summary\n",
        "            )\n",
        "            human_feedback_candidate = \"\"\n",
        "        try:\n",
        "            human_general_learning = input(\"Do you want to add any general learnings from this turn? (Type your learning or 'skip'): \")\n",
        "            if human_general_learning.lower() != 'skip' and human_general_learning.strip():\n",
        "                human_feedback_candidate = human_general_learning\n",
        "            elif human_general_learning.lower() == 'skip':\n",
        "                print(\"[INFO] Skipping the addition of general learnings for this turn.\")\n",
        "        except EOFError:\n",
        "            print(\"EOFError: Skipping general learning input (non-interactive).\")\n",
        "\n",
        "        if human_feedback_candidate:\n",
        "            self.process_and_store_new_learning(\n",
        "                human_feedback_text=human_feedback_candidate, user_query_context=user_message, turn_context_summary=context_summary\n",
        "            )\n",
        "\n",
        "        if self.learnings_updated_this_session_flag:\n",
        "            self._persist_active_learnings_to_drive()\n",
        "\n",
        "        return {\"user_message\": user_message, \"anthropic_response\": anthropic_final_response,\n",
        "                 \"evaluation\": evaluation}\n",
        "\n",
        "    def evaluate_responses(self, user_message: str, anthropic_response: str,\n",
        "                           context_summary_for_eval: str, learnings_for_eval: str,\n",
        "                           anthropic_clarifications: Optional[List[Dict]] = None) -> Dict[str, Any]:\n",
        "        print(\"\\n--- Starting Evaluation by Gemini ---\")\n",
        "\n",
        "        current_ground_truth_data = {\n",
        "            \"customers\": self.anthropic_storage.customers,\n",
        "            \"products\": self.anthropic_storage.products,\n",
        "            \"orders\": self.anthropic_storage.orders\n",
        "        }\n",
        "        ground_truth_prompt_addition = f\"\\nGround Truth Data Store State (for verification):\\n{json.dumps(current_ground_truth_data, indent=2, default=str)}\\n\"\n",
        "\n",
        "        clarification_summary_for_eval = []\n",
        "        if anthropic_clarifications:\n",
        "            clarification_summary_for_eval.append(f\"Anthropic Clarification Loop: {len(anthropic_clarifications)} question(s) asked.\")\n",
        "            for i, turn in enumerate(anthropic_clarifications):\n",
        "                clarification_summary_for_eval.append(f\"  A{i+1}: Q: '{turn['agent_question']}' -> User A: '{turn['user_answer']}'\")\n",
        "        clarification_info_for_prompt = \"\\n\".join(clarification_summary_for_eval) if clarification_summary_for_eval else \"No clarification questions were asked by the worker AI this turn.\"\n",
        "\n",
        "        try:\n",
        "            eval_prompt_parts = [\n",
        "                f\"User query: {user_message}\",\n",
        "                f\"Current context provided to assistant:\\n{context_summary_for_eval}\",\n",
        "                f\"Relevant RAG Learnings provided to assistant:\\n{learnings_for_eval if learnings_for_eval else 'None'}\",\n",
        "                ground_truth_prompt_addition,\n",
        "                f\"\\nDetails of Worker AI Clarification Interactions this turn:\\n{clarification_info_for_prompt}\\n\",\n",
        "                f\"Anthropic Claude final response:\\n{anthropic_response}\",\n",
        "                \"Please evaluate the response based on accuracy (comparing against the Ground Truth Data Store State), efficiency (considering necessity and quality of any clarification questions based on Ground Truth), context awareness, and helpfulness. Provide an overall score (1-10) and detailed reasoning, following your system prompt guidelines.\"\n",
        "            ]\n",
        "            eval_prompt = \"\\n\\n\".join(filter(None, eval_prompt_parts))\n",
        "\n",
        "            gemini_response_obj = eval_model_instance.generate_content(eval_prompt)\n",
        "            evaluation_text = gemini_response_obj.text\n",
        "            print(f\"Gemini Raw Initial Evaluation:\\n{evaluation_text}\")\n",
        "\n",
        "            clarification_details = {\"used\": False, \"needed\": \"\", \"provided_input\": \"\", \"action_summary\": \"\"}\n",
        "            if \"CLARIFICATION NEEDED_EVALUATOR:\" in evaluation_text.upper():\n",
        "                clarification_details[\"used\"] = True\n",
        "                clarification_details[\"needed\"] = self.extract_clarification_needed(evaluation_text, tag=\"CLARIFICATION NEEDED_EVALUATOR:\")\n",
        "                print(f\"--- Human Clarification Indicated by Evaluator ---\\nClarification needed: {clarification_details['needed']}\")\n",
        "                try:\n",
        "                    human_input_for_eval = input(f\"Enter human clarification for EVALUATOR (or 'skip'/'quit'): \")\n",
        "                    if human_input_for_eval.lower() in ['quit', 'exit', 'stop', 'q']:\n",
        "                        raise SystemExit(\"User requested exit during evaluation\")\n",
        "                    if human_input_for_eval.lower() != 'skip' and human_input_for_eval.strip():\n",
        "                        clarification_details[\"provided_input\"] = human_input_for_eval\n",
        "                        target_storage_for_action = self.anthropic_storage if any(cmd in human_input_for_eval.lower() for cmd in [\"update order\", \"create product\"]) else None\n",
        "                        action_summary = self.process_human_feedback_actions(human_input_for_eval, target_storage_for_action)\n",
        "                        clarification_details[\"action_summary\"] = action_summary\n",
        "                        updated_eval_prompt = f\"{eval_prompt}\\n\\nHuman clarification provided TO EVALUATOR: {human_input_for_eval}\\nAction taken based on feedback: {action_summary}\\nPlease re-evaluate.\"\n",
        "                        updated_gemini_response = eval_model_instance.generate_content(updated_eval_prompt)\n",
        "                        evaluation_text = updated_gemini_response.text\n",
        "                        print(f\"Gemini Raw Re-Evaluation:\\n{evaluation_text}\")\n",
        "                    else:\n",
        "                        clarification_details[\"provided_input\"] = \"Skipped by user\"\n",
        "                except EOFError: clarification_details[\"provided_input\"] = \"Skipped (non-interactive)\"\n",
        "\n",
        "            final_anthropic_data_state = { \"customers\": self.anthropic_storage.customers, \"products\": self.anthropic_storage.products, \"orders\": self.anthropic_storage.orders }\n",
        "            consistency_check_prompt_parts = [\n",
        "                evaluation_text, \"\\n\\n--- Data Store Consistency Check (Post-Action) ---\",\n",
        "                \"The AI assistant's actions may have modified its data store. Review the Data Store State *after* the AI's turn, and assess if it's consistent with the AI's response and any tool calls it made or should have made.\",\n",
        "                f\"Anthropic's Data Store State (After Action):\\n{json.dumps(final_anthropic_data_state, indent=2, default=str)}\",\n",
        "                \"1. Does this final data store state accurately reflect the outcomes of any tool calls Anthropic made (or should have made based on its response)?\",\n",
        "                \"2. Are there any inconsistencies between the agent's textual response and this final data state?\",\n",
        "                \"3. State whether this review of the final data store causes you to update your previous scores or assessment for Anthropic. If so, provide the updated scores and rationale.\"\n",
        "            ]\n",
        "            consistency_check_prompt = \"\\n\\n\".join(consistency_check_prompt_parts)\n",
        "            consistency_check_response_obj = eval_model_instance.generate_content(consistency_check_prompt)\n",
        "            final_evaluation_text = consistency_check_response_obj.text\n",
        "            print(f\"Gemini Full Evaluation (including Data Store Consistency Check):\\n{final_evaluation_text}\")\n",
        "\n",
        "            anthropic_score = self.extract_score(final_evaluation_text)\n",
        "            return {\"anthropic_score\": anthropic_score,\n",
        "                    \"full_evaluation\": final_evaluation_text, \"clarification_details\": clarification_details}\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluation: {str(e)}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            return {\"error\": f\"Error in evaluation: {str(e)}\", \"anthropic_score\": 0,\n",
        "                    \"full_evaluation\": f\"Evaluation failed: {str(e)}\", \"clarification_details\": {\"used\": False, \"action_summary\": \"\"}}\n",
        "\n",
        "    def extract_clarification_needed(self, evaluation_text: str, tag: str = \"CLARIFICATION NEEDED:\") -> str:\n",
        "        clarification_match = re.search(rf\"{tag.upper()}\\\\s*(.*?)(?:\\\\n|$)\", evaluation_text, re.IGNORECASE | re.DOTALL)\n",
        "        if clarification_match and clarification_match.group(1).strip():\n",
        "            return clarification_match.group(1).strip()\n",
        "        lines = evaluation_text.splitlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            if tag.upper() in line.upper():\n",
        "                return line.upper().split(tag.upper(), 1)[-1].strip() + \"\\n\" + \"\\n\".join(lines[i+1:i+3])\n",
        "        return \"Evaluator indicated clarification needed, but specific question not formatted as expected. Review raw evaluation.\"\n",
        "\n",
        "    def process_and_store_new_learning(self, human_feedback_text: str, user_query_context: str, turn_context_summary: str):\n",
        "        print(f\"\\n--- Processing New Candidate Learning from Human Feedback ---\")\n",
        "        print(f\"Candidate Human Feedback: \\\"{human_feedback_text}\\\"\")\n",
        "        print(f\"In context of User Query: \\\"{user_query_context}\\\"\")\n",
        "\n",
        "        current_learnings_for_conflict_check = list(self.active_learnings_cache)\n",
        "\n",
        "        evaluator_task_prompt_parts = [\n",
        "            \"You are an AI assistant helping to maintain a knowledge base of 'learnings' for a customer service AI agent (Anthropic Claude).\",\n",
        "            \"A human user has provided new feedback, which is a candidate for a new learning.\",\n",
        "            f\"The New Human Feedback is: \\\"{human_feedback_text}\\\"\",\n",
        "            f\"This feedback was given in the context of the user query: \\\"{user_query_context}\\\"\",\n",
        "            \"\\nHere are some existing ACTIVE learnings from our knowledge base (in-session cache) that might be relevant (if any):\"\n",
        "        ]\n",
        "        if current_learnings_for_conflict_check:\n",
        "            for i, el_entry in enumerate(current_learnings_for_conflict_check[-10:]):\n",
        "                stmt = el_entry.get('final_learning_statement', el_entry.get('original_human_input', 'N/A'))\n",
        "                orig_human_input_for_existing = el_entry.get('original_human_input', 'N/A')\n",
        "                evaluator_task_prompt_parts.append(f\"  Existing Learning {i+1} (Original human input: '{orig_human_input_for_existing}'): \\\"{stmt}\\\"\")\n",
        "        else:\n",
        "            evaluator_task_prompt_parts.append(\"  (No existing active learnings found in current session cache.)\")\n",
        "\n",
        "        evaluator_task_prompt_parts.extend([\n",
        "            \"\\nYour Tasks:\",\n",
        "            \"1. Analyze the New Human Feedback.\",\n",
        "            \"2. Compare it with the Existing ACTIVE Learnings provided. Identify any direct CONFLICTS or if the new feedback is essentially a DUPLICATE/REDUNDANT.\",\n",
        "            \"3. If a clear CONFLICT is found with an existing learning:\",\n",
        "            \"   - Output the specific tag: `CONFLICT_DETECTED:`\",\n",
        "            \"   - Clearly explain the conflict, citing the new feedback and the specific existing learning statement(s) it conflicts with (and their original human input if provided above).\",\n",
        "            \"   - Ask the human user specific questions to help them resolve this conflict (e.g., 'Which learning should take precedence?' or 'How can these be reconciled?').\",\n",
        "            \"   - DO NOT provide a 'FINALIZED_LEARNING' in this case.\",\n",
        "            \"4. If the New Human Feedback is a DUPLICATE/REDUNDANT of an existing learning and adds no new value:\",\n",
        "            \"   - Output the specific tag: `REDUNDANT_LEARNING:`\",\n",
        "            \"   - State which existing learning it is similar to.\",\n",
        "            \"   - DO NOT provide a 'FINALIZED_LEARNING'.\",\n",
        "            \"5. If the New Human Feedback provides a new, actionable insight, or significantly refines/clarifies an existing one (and is not a major conflict that needs resolution first):\",\n",
        "            \"   - Synthesize a concise, clear, and actionable 'Finalized Learning Statement'. This statement should be phrased as a directive or principle for the AI agent.\",\n",
        "            \"   - Prefix this statement with the specific tag: `FINALIZED_LEARNING:`\",\n",
        "            \"6. If the New Human Feedback is too vague, not actionable, or not suitable as a learning:\",\n",
        "            \"   - Output the specific tag: `NOT_ACTIONABLE:`\",\n",
        "            \"   - Explain why.\",\n",
        "            \"   - DO NOT provide a 'FINALIZED_LEARNING'.\"\n",
        "        ])\n",
        "        evaluator_conflict_check_prompt = \"\\n\".join(evaluator_task_prompt_parts)\n",
        "        print(\"\\n[RAG Cache] Sending context to Evaluator for learning synthesis/conflict check...\")\n",
        "        synthesis_response_obj = eval_model_instance.generate_content(evaluator_conflict_check_prompt)\n",
        "        evaluator_synthesis_text = synthesis_response_obj.text\n",
        "        print(f\"\\n[RAG Cache] Evaluator response on learning processing:\\n{evaluator_synthesis_text}\")\n",
        "\n",
        "        made_change_to_cache = False\n",
        "        final_learning_statement_to_store = None\n",
        "\n",
        "        if \"CONFLICT_DETECTED:\" in evaluator_synthesis_text:\n",
        "            conflict_explanation = evaluator_synthesis_text.split(\"CONFLICT_DETECTED:\", 1)[-1].strip()\n",
        "            print(f\"\\n🛑 CONFLICT DETECTED BY EVALUATOR 🛑\\n{conflict_explanation}\")\n",
        "            resolution_instruction = (\"Please review the conflict. How do you want to resolve this?\\n\"\n",
        "                                  \"  - Type 'use new' to add the new feedback as a learning.\\n\"\n",
        "                                  \"  - Type 'discard new' to not add this new feedback.\\n\"\n",
        "                                  \"  - Type 'merge: [your new merged learning statement]' to provide a reconciled version.\\n\"\n",
        "                                  \"  - Type 'keep existing' if the current learnings are preferred.\\n\"\n",
        "                                  \"Your input: \")\n",
        "            human_resolution = input(resolution_instruction).strip()\n",
        "            if human_resolution.lower() == \"use new\":\n",
        "                final_learning_statement_to_store = human_feedback_text\n",
        "            elif human_resolution.lower().startswith(\"merge:\"):\n",
        "                final_learning_statement_to_store = human_resolution.split(\"merge:\", 1)[-1].strip()\n",
        "            elif human_resolution.lower() in [\"discard new\", \"keep existing\"]:\n",
        "                final_learning_statement_to_store = None\n",
        "            else:\n",
        "                print(\"Resolution input unclear. New candidate learning discarded for safety.\")\n",
        "                final_learning_statement_to_store = None\n",
        "\n",
        "            if final_learning_statement_to_store:\n",
        "                 new_learning_entry = {\"learning_id\": str(uuid.uuid4()), \"timestamp_created\": datetime.now().isoformat(), \"user_query_context_at_feedback\": user_query_context, \"original_human_input\": human_feedback_text, \"final_learning_statement\": final_learning_statement_to_store, \"keywords\": self.extract_keywords(final_learning_statement_to_store + \" \" + human_feedback_text), \"status\": \"active\"}\n",
        "                 self.active_learnings_cache.append(new_learning_entry)\n",
        "                 made_change_to_cache = True\n",
        "                 print(f\"[RAG Cache] Resolved conflict. New/merged learning added to in-session cache.\")\n",
        "\n",
        "        elif \"FINALIZED_LEARNING:\" in evaluator_synthesis_text:\n",
        "            final_learning_statement_from_evaluator = evaluator_synthesis_text.split(\"FINALIZED_LEARNING:\", 1)[-1].strip()\n",
        "            print(f\"\\n✅ Finalized Learning by Evaluator: \\\"{final_learning_statement_from_evaluator}\\\"\")\n",
        "            new_learning_entry = {\"learning_id\": str(uuid.uuid4()), \"timestamp_created\": datetime.now().isoformat(), \"user_query_context_at_feedback\": user_query_context, \"original_human_input\": human_feedback_text, \"final_learning_statement\": final_learning_statement_from_evaluator, \"keywords\": self.extract_keywords(final_learning_statement_from_evaluator + \" \" + human_feedback_text), \"status\": \"active\"}\n",
        "            self.active_learnings_cache.append(new_learning_entry)\n",
        "            made_change_to_cache = True\n",
        "            print(f\"[RAG Cache] New learning added to in-session cache.\")\n",
        "\n",
        "        elif \"REDUNDANT_LEARNING:\" in evaluator_synthesis_text or \"NOT_ACTIONABLE:\" in evaluator_synthesis_text:\n",
        "            print(f\"\\nℹ️ Evaluator: {evaluator_synthesis_text.strip()}\")\n",
        "        else:\n",
        "            print(\"\\n⚠️ Evaluator response format for learning processing was unexpected. Storing raw human feedback to in-session cache as a precaution.\")\n",
        "            new_learning_entry = {\"learning_id\": str(uuid.uuid4()), \"timestamp_created\": datetime.now().isoformat(), \"user_query_context_at_feedback\": user_query_context, \"original_human_input\": human_feedback_text, \"final_learning_statement\": human_feedback_text, \"keywords\": self.extract_keywords(human_feedback_text), \"status\": \"active\"}\n",
        "            self.active_learnings_cache.append(new_learning_entry)\n",
        "            made_change_to_cache = True\n",
        "\n",
        "        if made_change_to_cache:\n",
        "            self.learnings_updated_this_session_flag = True\n",
        "            print(f\"[RAG Cache] In-session learnings cache now contains {len(self.active_learnings_cache)} entries.\")\n",
        "        else:\n",
        "            print(\"[RAG Cache] No changes made to in-session learnings cache based on this feedback.\")\n",
        "\n",
        "    def extract_keywords(self, text: str) -> List[str]:\n",
        "        if not text: return [\"general\"]\n",
        "        words = re.findall(r'\\b\\w{4,}\\b', text.lower())\n",
        "        stop_words = {\"the\", \"and\", \"is\", \"in\", \"to\", \"a\", \"of\", \"for\", \"with\", \"on\", \"at\", \"what\", \"how\", \"show\", \"tell\", \"please\", \"what's\", \"i'd\", \"like\", \"user\", \"query\", \"this\", \"that\", \"context\", \"claude\", \"anthropic\"}\n",
        "        extracted = list(set(word for word in words if word not in stop_words))\n",
        "        return extracted if extracted else [\"generic\"]\n",
        "\n",
        "    def extract_score(self, evaluation_text: str) -> int:\n",
        "        model_name_pattern = \"Anthropic\" # Hardcoded as we are only evaluating Anthropic now\n",
        "        consistency_check_section_start = evaluation_text.upper().rfind(\"--- DATA STORE CONSISTENCY CHECK ---\")\n",
        "        search_text = evaluation_text\n",
        "        if consistency_check_section_start != -1:\n",
        "            update_score_marker = re.search(r\"update your previous scores|updated scores and rationale\", evaluation_text[consistency_check_section_start:], re.IGNORECASE)\n",
        "            if update_score_marker:\n",
        "                search_text = evaluation_text[consistency_check_section_start:]\n",
        "\n",
        "        patterns = [\n",
        "            rf\"{model_name_pattern}.*?Overall Score.*?(\\d+)/10\", rf\"{model_name_pattern}.*?Overall Score:\\s*(\\d+)\",\n",
        "            rf\"Overall Score.*?{model_name_pattern}.*?:\\s*(\\d+)\", rf\"{model_name_pattern}.*?score.*?:.*?(\\d+)\",\n",
        "            rf\"{model_name_pattern}.*?\\bscore\\b.*?(\\d+)\", rf\"{model_name_pattern}.*?rating.*?:.*?(\\d+)\",\n",
        "            rf\"{model_name_pattern}.*?\\b(\\d+)/10\", rf\"Updated score for {model_name_pattern}.*?:.*?(\\d+)\",\n",
        "            rf\"{model_name_pattern}.*?updated overall score.*?:.*?(\\d+)\",\n",
        "            # Generic patterns, in case the model name isn't explicitly mentioned near the score\n",
        "            # These are tried if the more specific (and preferred) patterns above don't match.\n",
        "            r\"Overall Score:\\s*(\\d+)(?:/10)?\",\n",
        "            r\"The AI assistant's overall score is (\\d+)/10\",\n",
        "            r\"Overall:\\s*(\\d+)/10\"\n",
        "        ]\n",
        "        for p_str in reversed(patterns): # Try more specific patterns first if they include model name\n",
        "            matches = list(re.finditer(p_str, search_text, re.IGNORECASE | re.DOTALL))\n",
        "            if matches:\n",
        "                last_match = matches[-1]\n",
        "                if last_match.group(1):\n",
        "                    try:\n",
        "                        score = int(last_match.group(1))\n",
        "                        print(f\"Extracted score {score} for '{model_name_pattern}' (or generic) using pattern: {p_str}\")\n",
        "                        return score\n",
        "                    except ValueError: continue\n",
        "        print(f\"Could not extract score for '{model_name_pattern}' from eval text snippet (tried specific patterns):\\n{search_text[-600:]}...\")\n",
        "        return 0\n",
        "\n",
        "    def process_human_feedback_actions(self, feedback: str, target_storage_for_action: Optional[Storage]) -> str:\n",
        "        action_result_summary = \"No specific data action taken based on feedback.\"\n",
        "        # In single-agent mode, target_storage_for_action will always be self.anthropic_storage if not None\n",
        "        if not target_storage_for_action:\n",
        "            action_result_summary = \"Skipping data action: No target storage specified for feedback.\"\n",
        "            # print(f\"[Human Feedback Action] {action_result_summary}\") # Can be noisy\n",
        "            return action_result_summary\n",
        "\n",
        "        current_storage_name = \"Anthropic's Store\"\n",
        "\n",
        "        order_update_match = re.search(r\"update\\s+order\\s+(\\w+)\\s+status\\s+to\\s+(\\w+)\", feedback, re.IGNORECASE)\n",
        "        if order_update_match:\n",
        "            order_id, new_status = order_update_match.groups()\n",
        "            try:\n",
        "                result = self.process_tool_call(\"update_order_status\", {\"order_id\": order_id, \"new_status\": new_status})\n",
        "                action_result_summary = f\"Action executed on {current_storage_name}: Updated order {order_id} status to {new_status}. Result: {result.get('status', 'N/A')}\"\n",
        "                if result.get(\"status\") == \"success\" and \"order_details\" in result:\n",
        "                     self.conversation_context.update_entity_in_context(\"orders\", order_id, result[\"order_details\"])\n",
        "                     self.conversation_context.set_last_action(f\"human_feedback_update_order_Anthropic\", {\"input\": {\"order_id\": order_id, \"new_status\": new_status}, \"result\": result})\n",
        "            except Exception as e: action_result_summary = f\"Failed to update order {order_id} on {current_storage_name}: {str(e)}\"\n",
        "            print(f\"[Human Feedback Action] {action_result_summary}\")\n",
        "            return action_result_summary\n",
        "        product_create_match = re.search(r\"create\\s+(?:new\\s+)?product:\\\\s*(.*?),\\\\s*description:\\\\s*(.*?),\\\\s*price:\\\\s*(\\\\d+\\\\.?\\\\d*),\\\\s*inventory:\\\\s*(\\\\d+)\", feedback, re.IGNORECASE)\n",
        "        if product_create_match:\n",
        "            name, desc, price, inventory = product_create_match.groups()\n",
        "            try:\n",
        "                tool_input = {\"name\": name.strip(), \"description\": desc.strip(), \"price\": float(price), \"inventory_count\": int(inventory)}\n",
        "                result = self.process_tool_call(\"create_product\", tool_input)\n",
        "                action_result_summary = f\"Action executed on {current_storage_name}: Created product '{name}'. Result: {result.get('status', 'N/A')}\"\n",
        "                if result.get(\"status\") == \"success\" and \"product\" in result:\n",
        "                     self.conversation_context.update_entity_in_context(\"products\", result[\"product_id\"], result[\"product\"])\n",
        "                     self.conversation_context.set_last_action(f\"human_feedback_create_product_Anthropic\", {\"input\": tool_input, \"result\": result})\n",
        "            except Exception as e: action_result_summary = f\"Failed to create product '{name}' on {current_storage_name}: {str(e)}\"\n",
        "            print(f\"[Human Feedback Action] {action_result_summary}\")\n",
        "            return action_result_summary\n",
        "        # print(f\"[Human Feedback Action] {action_result_summary}\") # Can be noisy if no action\n",
        "        return action_result_summary\n",
        "\n",
        "print(\"AgentEvaluator class defined with In-Session Learnings Cache and Drive Mount RAG logic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzLHm8MmSOMC",
        "outputId": "c5df07a1-e02c-47b6-e818-72407f29caab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentEvaluator class defined with In-Session Learnings Cache and Drive Mount RAG logic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"\\nStarting Main Execution (Single Agent - Anthropic) with Drive Mount RAG...\\n\")\n",
        "\n",
        "    try:\n",
        "        # Assuming AgentEvaluator class is defined in a previous cell\n",
        "        agent = AgentEvaluator()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to initialize AgentEvaluator: {e}\")\n",
        "        print(\"Please ensure Google Drive can be mounted and the learnings path is valid.\")\n",
        "        return\n",
        "\n",
        "    results_log = []\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_query = input(\"\\nEnter your query (or 'quit', 'exit', 'stop', 'q' to end): \")\n",
        "            if user_query.lower() in ['quit', 'exit', 'stop', 'q']:\n",
        "                print(\"Exiting the system. Goodbye!\")\n",
        "                # --- Persist any pending learnings before breaking if the flag is set ---\n",
        "                if agent.learnings_updated_this_session_flag:\n",
        "                    print(\"Persisting final session learnings to Drive...\")\n",
        "                    agent._persist_active_learnings_to_drive()\n",
        "                break # Exit the loop\n",
        "            if not user_query.strip():\n",
        "                print(\"Empty query, please enter something.\")\n",
        "                continue\n",
        "\n",
        "            result = agent.process_user_request(user_query)\n",
        "            results_log.append(result)\n",
        "        except SystemExit as se: # To catch exits from within the agent processing\n",
        "            print(f\"System exit requested during processing: {se}\")\n",
        "            if agent.learnings_updated_this_session_flag: # Check flag even on SystemExit\n",
        "                print(\"Persisting session learnings to Drive before exiting due to SystemExit...\")\n",
        "                agent._persist_active_learnings_to_drive()\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL ERROR processing query '{user_query}': {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            results_log.append({\n",
        "                \"user_message\": user_query, \"anthropic_response\": \"ERROR\",\n",
        "                \"evaluation\": {\"anthropic_score\": 0,\n",
        "                               \"full_evaluation\": f\"Critical error: {e}\",\n",
        "                               \"clarification_details\": {\"used\": False, \"action_summary\": \"\"}}\n",
        "            })\n",
        "\n",
        "    print(\"\\n\\n===== EVALUATION SUMMARY =====\")\n",
        "    total_anthropic_score, num_q = 0, 0\n",
        "    for i, res in enumerate(results_log):\n",
        "        if not res:\n",
        "            print(f\"\\nQuery {i+1}: Skipped (empty result).\")\n",
        "            continue\n",
        "        num_q +=1\n",
        "        print(f\"\\nQuery {i+1}: {res.get('user_message', 'N/A')}\")\n",
        "        print(f\"  Anthropic Resp: {str(res.get('anthropic_response', 'N/A'))[:150]}...\")\n",
        "\n",
        "        eval_data = res.get('evaluation', {})\n",
        "        anth_s = eval_data.get('anthropic_score', 0)\n",
        "        total_anthropic_score += anth_s\n",
        "        print(f\"  Score - Anthropic: {anth_s}\")\n",
        "\n",
        "        clarif_details = eval_data.get('clarification_details',{})\n",
        "        if clarif_details.get('used'):\n",
        "            print(f\"    Evaluator Clarification: Needed='{clarif_details.get('needed', 'N/A')}', Provided='{clarif_details.get('provided_input', 'N/A')}'\")\n",
        "            if clarif_details.get('action_summary'):\n",
        "                 print(f\"    Action from Evaluator Clarification: {clarif_details['action_summary']}\")\n",
        "\n",
        "    print(f\"\\n----- Overall Performance -----\")\n",
        "    if num_q > 0:\n",
        "        print(f\"Avg Anthropic Score: {total_anthropic_score/num_q:.2f}\")\n",
        "    else:\n",
        "        print(\"No queries processed to calculate average scores.\")\n",
        "    print(f\"Total Anthropic Score: {total_anthropic_score}\")\n",
        "\n",
        "    print(f\"\\nLearnings are stored as timestamped JSON files in your Google Drive at: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "    print(\"The latest file in that directory represents the current active set of learnings.\")\n",
        "\n",
        "    # --- CORRECTED METHOD NAME HERE ---\n",
        "    latest_learnings_file = agent._get_latest_learnings_filepath_from_drive()\n",
        "    # --- END CORRECTION ---\n",
        "\n",
        "    if latest_learnings_file:\n",
        "        print(f\"Most recent learnings file: {os.path.basename(latest_learnings_file)}\")\n",
        "    else:\n",
        "        print(\"No learnings files found in the directory.\")\n",
        "\n",
        "    print(\"\\nExecution Finished.\")\n",
        "\n",
        "# To run (in a separate cell):\n",
        "# main()"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "id": "O2-ztJ2BO7gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Sample queries:\n",
        "* Show me all the products available\n",
        "* I'd like to order 25 Perplexinators, please\n",
        "* Show me the status of my order\n",
        "* (If the order is not in Shipped state, then) Please ship my order now\n",
        "* How many Perplexinators are now left in stock?\n",
        "* Add a new customer: Bill Leece, bill.leece@mail.com, +1.222.333.4444\n",
        "* Add new new product: Gizmo X, description: A fancy gizmo, price: 29.99, inventory: 50\n",
        "* Update Gizzmo's price to 99.99 #Note the misspelling of 'Gizmo'\n",
        "* I need to update our insurance policy, so I need to know the total value of all the products in our inventory. Please tell me this amount.\n",
        "* Summarize your learnings from our recent interactions.\n",
        "\"\"\"\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dg85ek6bSvW5",
        "outputId": "2946a644-b32b-4db3-ab95-f08bd9221732"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Main Execution (Single Agent - Anthropic) with Drive Mount RAG...\n",
            "\n",
            "AgentEvaluator initialized with Storage for Anthropic agent.\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully at /content/drive.\n",
            "Learnings directory found: /content/drive/My Drive/AI/Knowledgebases\n",
            "[RAG Cache] Initializing: Loading latest learnings from Drive...\n",
            "[RAG Cache] No existing learnings file found on Drive for initial load.\n",
            "AgentEvaluator initialized. Learnings path: /content/drive/My Drive/AI/Knowledgebases. Loaded 0 initial learnings into cache.\n",
            "\n",
            "\n",
            "============================================================\n",
            "User Message: I'd like to order 25 Perplexinators, please\n",
            "============================================================\n",
            "Current Context Summary for Agent:\n",
            "No specific context items set yet.\n",
            "------------------------------------------------------------\n",
            "No specific relevant learnings found from In-Session Cache for this turn.\n",
            "\n",
            "--- Getting Anthropic's Response ---\n",
            "Anthropic Tool Call: get_product_info, Input: {'product_id_or_name': 'Perplexinator'}\n",
            "--- [Tool Dispatcher] Attempting tool: get_product_info with input: {\"product_id_or_name\": \"Perplexinator\"} for storage: AnthropicStorage ---\n",
            "[Tool Helper] find_product_by_name: Matched 'Perplexinator' to 'Perplexinator' (ID: P3) with score 100 (in Storage)\n",
            "[Tool Executed] get_product_info: Found by name (fuzzy) 'Perplexinator' as ID 'P3' (in Storage).\n",
            "--- [Tool Dispatcher] Result for get_product_info on AnthropicStorage: {\n",
            "  \"status\": \"success\",\n",
            "  \"message\": \"Found product matching 'Perplexinator'\",\n",
            "  \"product_id\": \"P3\",\n",
            "  \"product\": {\n",
            "    \"name\": \"Perplexinator\",\n",
            "    \"description\": \"A perplexing perfunctator\",\n",
            "    \"price\": 79.99,\n",
            "    \"inventory_count\": 1483\n",
            "  }\n",
            "} ---\n",
            "[Context Updated] Entity: products, ID: P3, Data (type): <class 'dict'>\n",
            "[Context Updated] Last Action: get_product_info_Anthropic, Details: {\"input\": {\"product_id_or_name\": \"Perplexinator\"}, \"result\": {\"status\": \"success\", \"message\": \"Found product matching 'Perplexinator'\", \"product_id\": \"P3\", \"product\": {\"name\": \"Perplexinator\", \"description\": \"A perplexing perfunctator\", \"price\": 79.99, \"inventory_count\": 1483}}}\n",
            "Anthropic Tool Call: create_order, Input: {'product_id_or_name': 'Perplexinator', 'quantity': 25, 'status': 'Processing'}\n",
            "--- [Tool Dispatcher] Attempting tool: create_order with input: {\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25, \"status\": \"Processing\"} for storage: AnthropicStorage ---\n",
            "[Tool Helper] find_product_by_name: Matched 'Perplexinator' to 'Perplexinator' (ID: P3) with score 100 (in Storage)\n",
            "[Tool Executed] create_order: Order O3 created for 25 of Perplexinator (ID: P3). Status: Processing. Remaining inv: 1483 (in Storage)\n",
            "--- [Tool Dispatcher] Result for create_order on AnthropicStorage: {\n",
            "  \"status\": \"success\",\n",
            "  \"order_id\": \"O3\",\n",
            "  \"order_details\": {\n",
            "    \"id\": \"O3\",\n",
            "    \"product_id\": \"P3\",\n",
            "    \"product_name\": \"Perplexinator\",\n",
            "    \"quantity\": 25,\n",
            "    \"price\": 79.99,\n",
            "    \"status\": \"Processing\"\n",
            "  },\n",
            "  \"remaining_inventory\": 1483\n",
            "} ---\n",
            "[Context Updated] Entity: orders, ID: O3, Data (type): <class 'dict'>\n",
            "[Context Updated] Last Action: create_order_Anthropic, Details: {\"input\": {\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25, \"status\": \"Processing\"}, \"result\": {\"status\": \"success\", \"order_id\": \"O3\", \"order_details\": {\"id\": \"O3\", \"product_id\": \"P3\", \"product_name\": \"Perplexinator\", \"quantity\": 25, \"price\": 79.99, \"status\": \"Processing\"}, \"remaining_inventory\": 1483}}\n",
            "\n",
            "--- Anthropic Final Response Text (Post-Clarification if any) ---\n",
            "Perfect! I've successfully placed your order. Here are the details:\n",
            "- Order ID: O3\n",
            "- Product: Perplexinator\n",
            "- Quantity: 25 units\n",
            "- Price per unit: $79.99\n",
            "- Total price: $1,999.75\n",
            "- Status: Processing\n",
            "\n",
            "Your order has been created and is being processed. Is there anything else you need help with?\n",
            "\n",
            "--- Starting Evaluation by Gemini ---\n",
            "Gemini Raw Initial Evaluation:\n",
            "## Evaluation of AI Assistant Response\n",
            "\n",
            "**1. Accuracy:**\n",
            "- **Product Identification & Details:** The AI correctly identified the \"Perplexinator\" product (P3). The price per unit ($79.99) is accurate as per the Ground Truth Data Store State (GTDS).\n",
            "- **Quantity:** The AI correctly parsed the requested quantity of 25 units.\n",
            "- **Total Price Calculation:** The total price of $1,999.75 (25 * $79.99) is calculated correctly.\n",
            "- **Status:** The status \"Processing\" is plausible for a new order and matches the status of the existing order O3 in the GTDS.\n",
            "- **Order ID & Placement Claim:** The AI claims to have \"successfully placed your order\" and provides the Order ID \"O3\". According to the initial GTDS, an order O3 with these exact specifications (25 Perplexinators, $79.99 each, status Processing) already exists.\n",
            "    - If the AI's action was to create a genuinely *new* order, it should ideally have a new ID (e.g., O4), and the AI reporting O3 would be an error unless the system's ID generation is peculiar (e.g., reuses this ID if it considers this a fulfillment of the existing O3).\n",
            "    - If the AI (or the underlying tool) identified the existing O3 and re-confirmed it, the statement \"I've successfully placed your order\" is mostly acceptable, but could be slightly misleading. It implies a new action of creation, rather than confirmation of an existing state. A more precise phrasing might have been \"I've confirmed your order...\" or similar, if it's referring to the pre-existing O3.\n",
            "    - Assuming the AI is reporting the outcome of a `create_order` tool call, and that tool returned 'O3' (perhaps due to idempotency or matching an existing identical pending order), then the AI is accurately relaying the tool's output.\n",
            "\n",
            "Given the information, the factual details (product, price, quantity, total) are correct. The main ambiguity lies in the \"placement\" of an order that already exists with identical details. This could be a nuance of the ordering system being simulated. Without seeing the tool call's direct output, we assume the AI is reporting what it received. The phrasing could be slightly clearer if it's referring to an existing order.\n",
            "\n",
            "Score: 8/10\n",
            "\n",
            "**2. Efficiency:**\n",
            "- **Clarification Questions:** No clarification questions were asked by the AI.\n",
            "- **Necessity of Questions:** The AI did not need to ask for clarification. The product \"Perplexinators\" was specific enough (and correctly mapped to \"Perplexinator\"). The quantity \"25\" was clear. Product price and availability could be looked up in the GTDS (Perplexinator P3, inventory 1483, which is sufficient for 25 units). The AI efficiently processed the request.\n",
            "\n",
            "Score: 10/10\n",
            "\n",
            "**3. Context Awareness:**\n",
            "- The user's query was the first interaction, so there was no prior conversation context.\n",
            "- The AI correctly understood the user's intent to place an order based on the phrase \"I'd like to order...\".\n",
            "\n",
            "Score: 10/10\n",
            "\n",
            "**4. Helpfulness:**\n",
            "- The AI provided a clear confirmation of the order, including all key details (Order ID, product, quantity, price per unit, total price, status). This is very helpful for the user.\n",
            "- The AI also asked \"Is there anything else you need help with?\", which is good practice for customer service.\n",
            "- The only minor deduction is linked to the accuracy point: if the user intended a *new, distinct* order and the system simply re-confirmed a pre-existing identical one (O3) without clarification, it might not fully meet an unstated user need for a separate order record. However, based on the explicit query, the response is largely helpful.\n",
            "\n",
            "Score: 9/10\n",
            "\n",
            "**Overall Score:**\n",
            "(8 + 10 + 10 + 9) / 4 = 37 / 4 = 9.25\n",
            "Overall Score: 9/10\n",
            "\n",
            "**Reasoning for Overall Score:**\n",
            "The AI assistant performed well, accurately identifying the product, quantity, and price, and correctly calculating the total. It did so efficiently without unnecessary questions. The confirmation provided was comprehensive. The primary minor issue is the potential ambiguity around reporting Order ID \"O3\" (which pre-existed with identical details) as a \"placed\" order without further clarification. While this might be an accurate reflection of an idempotent system behavior or a tool's output, slightly more nuanced phrasing could have improved clarity for the user in certain scenarios. However, for a straightforward order request, the response is effective and largely meets the user's needs.\n",
            "\n",
            "---\n",
            "## Data Store Consistency Check\n",
            "\n",
            "**Initial Ground Truth Data Store State:**\n",
            "```json\n",
            "{\n",
            "  \"products\": {\n",
            "    \"P3\": {\n",
            "      \"name\": \"Perplexinator\",\n",
            "      \"description\": \"A perplexing perfunctator\",\n",
            "      \"price\": 79.99,\n",
            "      \"inventory_count\": 1483\n",
            "    }\n",
            "  },\n",
            "  \"orders\": {\n",
            "    \"O3\": {\n",
            "      \"id\": \"O3\",\n",
            "      \"product_id\": \"P3\",\n",
            "      \"product_name\": \"Perplexinator\",\n",
            "      \"quantity\": 25,\n",
            "      \"price\": 79.99,\n",
            "      \"status\": \"Processing\"\n",
            "    }\n",
            "    // Other products and orders omitted for brevity\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "**AI's Stated Actions & Tool Use:**\n",
            "The AI stated: \"Perfect! I've successfully placed your order. Here are the details: Order ID: O3...\"\n",
            "This implies a tool was used to place/confirm an order for 25 Perplexinators, resulting in Order O3.\n",
            "\n",
            "**Final Ground Truth Data Store State (after AI interaction):**\n",
            "```json\n",
            "{\n",
            "  \"customers\": {\n",
            "    \"C1\": {\n",
            "      \"name\": \"John Doe\",\n",
            "      \"email\": \"john@example.com\",\n",
            "      \"phone\": \"123-456-7890\"\n",
            "    },\n",
            "    \"C2\": {\n",
            "      \"name\": \"Jane Smith\",\n",
            "      \"email\": \"jane@example.com\",\n",
            "      \"phone\": \"987-654-3210\"\n",
            "    }\n",
            "  },\n",
            "  \"products\": {\n",
            "    \"P1\": {\n",
            "      \"name\": \"Widget A\",\n",
            "      \"description\": \"A simple widget. Very compact.\",\n",
            "      \"price\": 19.99,\n",
            "      \"inventory_count\": 999\n",
            "    },\n",
            "    \"P2\": {\n",
            "      \"name\": \"Gadget B\",\n",
            "      \"description\": \"A powerful gadget. It spins.\",\n",
            "      \"price\": 49.99,\n",
            "      \"inventory_count\": 200\n",
            "    },\n",
            "    \"P3\": {\n",
            "      \"name\": \"Perplexinator\",\n",
            "      \"description\": \"A perplexing perfunctator\",\n",
            "      \"price\": 79.99,\n",
            "      \"inventory_count\": 1458 // Inventory changed from 1483 to 1458\n",
            "    }\n",
            "  },\n",
            "  \"orders\": {\n",
            "    \"O1\": {\n",
            "      \"id\": \"O1\",\n",
            "      \"product_id\": \"P1\",\n",
            "      \"product_name\": \"Widget A\",\n",
            "      \"quantity\": 2,\n",
            "      \"price\": 19.99,\n",
            "      \"status\": \"Shipped\"\n",
            "    },\n",
            "    \"O2\": {\n",
            "      \"id\": \"O2\",\n",
            "      \"product_id\": \"P2\",\n",
            "      \"product_name\": \"Gadget B\",\n",
            "      \"quantity\": 1,\n",
            "      \"price\": 49.99,\n",
            "      \"status\": \"Processing\"\n",
            "    },\n",
            "    \"O3\": { // Order O3 remains, details unchanged\n",
            "      \"id\": \"O3\",\n",
            "      \"product_id\": \"P3\",\n",
            "      \"product_name\": \"Perplexinator\",\n",
            "      \"quantity\": 25,\n",
            "      \"price\": 79.99,\n",
            "      \"status\": \"Processing\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "**Verification of Consistency:**\n",
            "\n",
            "1.  **Order Record:** The AI reported Order ID \"O3\". The final data store shows Order O3 is still present, with its details unchanged (product_id: P3, quantity: 25, price: 79.99, status: Processing). No new order (e.g., O4) was created. This is consistent with the AI referring to the existing order O3.\n",
            "2.  **Inventory Count:** The product ordered was \"Perplexinator\" (P3).\n",
            "    *   Initial inventory: 1483 units.\n",
            "    *   Quantity ordered: 25 units.\n",
            "    *   Final inventory: 1458 units.\n",
            "    The inventory count for P3 has decreased by 25 (1483 - 25 = 1458). This indicates that the order for 25 units of Perplexinator was indeed processed and accounted for.\n",
            "\n",
            "**Conclusion of Consistency Check:**\n",
            "The final data store state is consistent with the AI's statement that an order (O3) for 25 Perplexinators was placed/processed. The key evidence is the reduction in inventory for Product P3 by 25 units. This suggests that the system treated the user's request as fulfilling or confirming the existing \"Processing\" Order O3, and appropriately updated the inventory.\n",
            "\n",
            "The AI's claim \"I've successfully placed your order\" coupled with the ID \"O3\" is therefore substantiated by the data store changes (specifically, the inventory update). The system appears to have handled this by actioning the pre-existing O3 order record rather than creating a new one, which is a valid system behavior.\n",
            "\n",
            "My initial Accuracy score of 8/10, which slightly penalized the phrasing \"placed\" for a pre-existing ID, still seems appropriate. While the action *did* occur (inventory change proves it), the AI could have been more precise if the tool indicated it was acting upon an existing order record. However, since the AI is likely just relaying tool output, the response is acceptable. No changes to scores are needed based on this consistency check.\n",
            "Gemini Full Evaluation (including Data Store Consistency Check):\n",
            "The user's initial evaluation was very thorough and well-reasoned based on their hypothetical final data store where inventory *did* change. My task is to re-evaluate based on the *actual* \"Anthropic's Data Store State (After Action)\" provided.\n",
            "\n",
            "**Initial Ground Truth Data Store State (for context, as per user's prompt):**\n",
            "```json\n",
            "{\n",
            "  \"products\": {\n",
            "    \"P3\": {\n",
            "      \"name\": \"Perplexinator\",\n",
            "      \"description\": \"A perplexing perfunctator\",\n",
            "      \"price\": 79.99,\n",
            "      \"inventory_count\": 1483\n",
            "    }\n",
            "  },\n",
            "  \"orders\": {\n",
            "    \"O3\": {\n",
            "      \"id\": \"O3\",\n",
            "      \"product_id\": \"P3\",\n",
            "      \"product_name\": \"Perplexinator\",\n",
            "      \"quantity\": 25,\n",
            "      \"price\": 79.99,\n",
            "      \"status\": \"Processing\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "**AI Assistant's Response:**\n",
            "\"Perfect! I've successfully placed your order. Here are the details: Order ID: O3 Product: Perplexinator Quantity: 25 Price per unit: $79.99 Total Price: $1,999.75 Status: Processing Is there anything else you need help with?\"\n",
            "\n",
            "**Anthropic's Data Store State (After Action):**\n",
            "```json\n",
            "{\n",
            "  \"customers\": { ... },\n",
            "  \"products\": {\n",
            "    \"P1\": { ... },\n",
            "    \"P2\": { ... },\n",
            "    \"P3\": {\n",
            "      \"name\": \"Perplexinator\",\n",
            "      \"description\": \"A perplexing perfunctator\",\n",
            "      \"price\": 79.99,\n",
            "      \"inventory_count\": 1483 // Inventory UNCHANGED from initial state\n",
            "    }\n",
            "  },\n",
            "  \"orders\": {\n",
            "    \"O1\": { ... },\n",
            "    \"O2\": { ... },\n",
            "    \"O3\": { // Order O3 UNCHANGED from initial state\n",
            "      \"id\": \"O3\",\n",
            "      \"product_id\": \"P3\",\n",
            "      \"product_name\": \"Perplexinator\",\n",
            "      \"quantity\": 25,\n",
            "      \"price\": 79.99,\n",
            "      \"status\": \"Processing\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "Now, let's address the specific questions regarding the Data Store Consistency Check based on the provided \"Anthropic's Data Store State (After Action)\".\n",
            "\n",
            "**1. Does this final data store state accurately reflect the outcomes of any tool calls Anthropic made (or should have made based on its response)?**\n",
            "\n",
            "The AI's response states, \"I've successfully placed your order.\" This implies that an `create_order` or similar tool call was made and was successful.\n",
            "\n",
            "The final data store shows:\n",
            "*   No new order was created (Order O3 already existed with identical details).\n",
            "*   The inventory count for \"Perplexinator\" (P3) remains 1483, unchanged from the initial state.\n",
            "\n",
            "If a tool call was made to create an order, and that tool is idempotent (i.e., recognizes that an identical order O3 already exists and is \"Processing\"), it might return the existing Order ID \"O3\" without creating a new entry and without re-decrementing inventory that was (presumably) already accounted for when O3 was first created and set to \"Processing\".\n",
            "\n",
            "In this scenario, the AI is accurately relaying the Order ID \"O3\" that the tool might have returned. However, the crucial part is the inventory. Since the inventory did *not* decrease as a result of *this interaction*, the AI's statement \"I've successfully placed your order\" is misleading if it implies new stock allocation. It seems the system merely acknowledged the request aligned with an existing, already \"Processing\" order. The \"action\" of placing, if it means reserving stock, did not happen *in this turn*.\n",
            "\n",
            "So, the final data store *could* reflect an idempotent tool call that simply returned the existing order ID. However, the lack of inventory change makes the AI's statement about \"placing\" the order problematic.\n",
            "\n",
            "**2. Are there any inconsistencies between the agent's textual response and this final data state?**\n",
            "\n",
            "Yes, there is a significant inconsistency or, at best, a misleading statement:\n",
            "*   **Agent's textual response:** \"I've successfully placed your order.\"\n",
            "*   **Final data state:**\n",
            "    *   Order O3 existed prior to this interaction with the exact same details and \"Processing\" status.\n",
            "    *   No new order was created.\n",
            "    *   Crucially, the `inventory_count` for Product P3 (Perplexinator) did *not* change (remained 1483).\n",
            "\n",
            "The phrase \"successfully placed your order\" strongly implies that an action was taken *as a result of the user's request in this turn*, such as creating a new order record or decrementing inventory. Since neither a new order record appears nor does the inventory for P3 decrease, the AI's statement is not well-supported by the change (or lack thereof) in the data store. It essentially just confirmed an existing order, O3.\n",
            "\n",
            "**3. State whether this review of the final data store causes you to update your previous scores or assessment for Anthropic. If so, provide the updated scores and rationale.**\n",
            "\n",
            "Yes, this review necessitates an update to the scores, particularly for Accuracy and Helpfulness, because the user's initial evaluation (which was very good) was based on a hypothetical outcome where inventory *did* change. The actual outcome (inventory *not* changing) alters the interpretation of the AI's success.\n",
            "\n",
            "**Updated Evaluation Scores:**\n",
            "\n",
            "*   **Accuracy:**\n",
            "    *   Initial User Score: 8/10 (based on inventory changing)\n",
            "    *   **New Score: 5/10**\n",
            "    *   **Rationale:** The AI correctly identified the product, quantity, and price. It also correctly stated the Order ID as O3 and its status as \"Processing\", which matches the pre-existing order. However, the claim \"I've successfully placed your order\" is inaccurate because the final data store shows no new order was created, and, most importantly, the inventory for \"Perplexinator\" was not decremented. This means no new stock allocation occurred as a result of *this specific interaction*. The AI essentially re-stated the details of an existing order without any discernible action being taken on it during this turn (like inventory update).\n",
            "\n",
            "*   **Efficiency:**\n",
            "    *   Initial User Score: 10/10\n",
            "    *   **New Score: 10/10** (No change)\n",
            "    *   **Rationale:** The AI did not ask any clarification questions, and none were needed as the user's request was clear.\n",
            "\n",
            "*   **Context Awareness:**\n",
            "    *   Initial User Score: 10/10\n",
            "    *   **New Score: 10/10** (No change)\n",
            "    *   **Rationale:** This was the first turn. The AI correctly understood the intent to order from the user's query.\n",
            "\n",
            "*   **Helpfulness:**\n",
            "    *   Initial User Score: 9/10\n",
            "    *   **New Score: 6/10**\n",
            "    *   **Rationale:** The AI provided details of an order (O3), which is somewhat helpful. However, by stating \"I've successfully placed your order\" without any actual new order creation or inventory change, the AI provides a misleading sense of accomplishment. The user might believe a new order has been processed or new stock has been allocated to them, which is not supported by the data. If the user intended a *new* distinct order, this response fails them. If they wanted to ensure an existing order was actioned, the lack of inventory change makes the confirmation less meaningful. More precise language like \"I found an existing order, O3, with these details. It is currently 'Processing'. Did you want to place a new, separate order?\" would have been far more helpful.\n",
            "\n",
            "*   **Overall Score:**\n",
            "    *   Initial User Score: 9/10\n",
            "    *   Calculation with new scores: (5 + 10 + 10 + 6) / 4 = 31 / 4 = 7.75\n",
            "    *   **New Overall Score: 8/10**\n",
            "    *   **Rationale for Overall Score:** The AI was efficient and understood the user's explicit request. It correctly identified order details corresponding to an existing order (O3). However, the primary failing is the inaccurate and misleading claim of having \"successfully placed\" the order when the data store shows no new order creation and, critically, no inventory decrement for this interaction. This significantly impacts accuracy and helpfulness, as the user is likely to misunderstand the actual outcome. The AI merely confirmed an existing order's details but presented it as if a new action was completed.\n",
            "Extracted score 8 for 'Anthropic' (or generic) using pattern: Overall Score:\\s*(\\d+)(?:/10)?\n",
            "\n",
            "--- Processing New Candidate Learning from Human Feedback ---\n",
            "Candidate Human Feedback: \"Inventory doesn't change until an order is set to \"Shipped\" state\"\n",
            "In context of User Query: \"I'd like to order 25 Perplexinators, please\"\n",
            "\n",
            "[RAG Cache] Sending context to Evaluator for learning synthesis/conflict check...\n",
            "\n",
            "[RAG Cache] Evaluator response on learning processing:\n",
            "`FINALIZED_LEARNING: Inventory levels are adjusted only when an order's status is updated to 'Shipped'.`\n",
            "\n",
            "✅ Finalized Learning by Evaluator: \"Inventory levels are adjusted only when an order's status is updated to 'Shipped'.`\"\n",
            "[RAG Cache] New learning added to in-session cache.\n",
            "[RAG Cache] In-session learnings cache now contains 1 entries.\n",
            "[RAG Cache] Successfully persisted 1 active learnings to new file: /content/drive/My Drive/AI/Knowledgebases/learnings_20250517_170252_116080.json\n",
            "\n",
            "Enter your query (or 'quit', 'exit', 'stop', 'q' to end): Show me the status of my order\n",
            "\n",
            "\n",
            "============================================================\n",
            "User Message: Show me the status of my order\n",
            "============================================================\n",
            "Current Context Summary for Agent:\n",
            "Recent products: ID: P3 (Name: Perplexinator)\n",
            "Recent orders: ID: O3 (Product: Perplexinator, Status: Processing)\n",
            "Last action: create_order_Anthropic at 2025-05-17T16:58:45.370340 (Input: {'product_id_or_name': 'Perplexinator', 'quantity': 25, 'status': 'Processing'}, Result Status: success, OrderID: O3)\n",
            "------------------------------------------------------------\n",
            "Relevant Learnings for this turn (from In-Session Cache):\n",
            "\n",
            "Relevant Learnings from Knowledge Base (In-Session):\n",
            "- Learning (from 2025-05-17T17:02:52.114769): Inventory levels are adjusted only when an order's status is updated to 'Shipped'.`\n",
            "\n",
            "--- Getting Anthropic's Response ---\n",
            "Anthropic Tool Call: get_order_details, Input: {'order_id': 'O3'}\n",
            "--- [Tool Dispatcher] Attempting tool: get_order_details with input: {\"order_id\": \"O3\"} for storage: AnthropicStorage ---\n",
            "[Tool Executed] get_order_details: Order O3 found (in Storage).\n",
            "--- [Tool Dispatcher] Result for get_order_details on AnthropicStorage: {\n",
            "  \"status\": \"success\",\n",
            "  \"order_id\": \"O3\",\n",
            "  \"order_details\": {\n",
            "    \"id\": \"O3\",\n",
            "    \"product_id\": \"P3\",\n",
            "    \"product_name\": \"Perplexinator\",\n",
            "    \"quantity\": 25,\n",
            "    \"price\": 79.99,\n",
            "    \"status\": \"Processing\"\n",
            "  }\n",
            "} ---\n",
            "[Context Updated] Entity: orders, ID: O3, Data (type): <class 'dict'>\n",
            "[Context Updated] Last Action: get_order_details_Anthropic, Details: {\"input\": {\"order_id\": \"O3\"}, \"result\": {\"status\": \"success\", \"order_id\": \"O3\", \"order_details\": {\"id\": \"O3\", \"product_id\": \"P3\", \"product_name\": \"Perplexinator\", \"quantity\": 25, \"price\": 79.99, \"status\": \"Processing\"}}}\n",
            "\n",
            "--- Anthropic Final Response Text (Post-Clarification if any) ---\n",
            "Your order for 25 Perplexinators (Order ID: O3) is currently in \"Processing\" status. The order total is $1,999.75 ($79.99 per unit). Would you like me to help you with anything else?\n",
            "\n",
            "--- Starting Evaluation by Gemini ---\n",
            "Gemini Raw Initial Evaluation:\n",
            "## Evaluation of AI Assistant Response\n",
            "\n",
            "**1. Accuracy:**\n",
            "*   **Score:** 10/10\n",
            "*   **Reasoning:** The AI assistant's response is entirely accurate when compared to the Ground Truth Data Store State.\n",
            "    *   Order ID: O3 (Correct, as per context and Ground Truth)\n",
            "    *   Product: Perplexinator (Correct, P3 is Perplexinator, as per Ground Truth)\n",
            "    *   Quantity: 25 (Correct, as per context and Ground Truth for O3)\n",
            "    *   Status: \"Processing\" (Correct, as per context and Ground Truth for O3)\n",
            "    *   Price per unit: $79.99 (Correct, as per Ground Truth for P3)\n",
            "    *   Order total: $1,999.75 (Correct, 25 * $79.99 = $1999.75)\n",
            "\n",
            "**2. Efficiency:**\n",
            "*   **Score:** 10/10\n",
            "*   **Reasoning:** The assistant did not ask any clarification questions. This was appropriate because the \"Current context\" clearly indicated a recent order (O3) that the user had just interacted with (\"Last action: create_order_Anthropic ... OrderID: O3\"). It was highly probable that \"my order\" referred to this specific order. The assistant efficiently used this context to provide a direct answer.\n",
            "\n",
            "**3. Context Awareness:**\n",
            "*   **Score:** 10/10\n",
            "*   **Reasoning:** The assistant demonstrated excellent context awareness. It correctly inferred that the user's query \"Show me the status of my order\" referred to the order O3, which was listed in \"Recent orders\" and was the subject of the \"Last action.\" It used all relevant details associated with O3 (product, quantity, status) accurately. The RAG learning about inventory levels was not directly used in the response but also not misused.\n",
            "\n",
            "**4. Helpfulness:**\n",
            "*   **Score:** 10/10\n",
            "*   **Reasoning:** The response was very helpful. It directly answered the user's query about the order status. Additionally, it proactively provided the order ID, product details, quantity, and the total cost, which are all relevant and useful pieces of information. The offer for further assistance (\"Would you like me to help you with anything else?\") is also a good customer service practice.\n",
            "\n",
            "**Overall Score:**\n",
            "*   **Score:** 10/10\n",
            "*   **Reasoning:** The AI assistant provided a perfectly accurate, efficient, and contextually aware response. It fully addressed the user's query and offered additional helpful information without unnecessary interaction.\n",
            "\n",
            "---\n",
            "\n",
            "**DATA STORE CONSISTENCY CHECK (Next Step):**\n",
            "No tool calls were made that would alter the Data Store State (e.g., `update_order_status`, `create_order`). The query was informational (`get_order_details` was implicitly used based on context). Therefore, the Data Store State should remain unchanged. I will verify this in the next step.\n",
            "Gemini Full Evaluation (including Data Store Consistency Check):\n",
            "Okay, I will now perform the Data Store Consistency Check.\n",
            "\n",
            "**1. Does this final data store state accurately reflect the outcomes of any tool calls Anthropic made (or should have made based on its response)?**\n",
            "\n",
            "Yes. Your evaluation correctly noted: \"No tool calls were made that would alter the Data Store State (e.g., `update_order_status`, `create_order`). The query was informational (`get_order_details` was implicitly used based on context).\"\n",
            "The AI assistant's task was to provide the status of an existing order (O3). This is an informational retrieval task (like a `get_order_details` call). Such a call would not modify the data store.\n",
            "The \"Anthropic's Data Store State (After Action)\" shows order O3 exactly as described in your evaluation of the AI's response (product, quantity, price, status). This means the data store correctly reflects that no *modifying* actions were taken by the AI in this specific turn. The state presented is consistent with the information the AI would have retrieved.\n",
            "\n",
            "**2. Are there any inconsistencies between the agent's textual response and this final data state?**\n",
            "\n",
            "No, there are no inconsistencies.\n",
            "The AI assistant's response, as summarized in your evaluation, stated:\n",
            "*   Order ID: O3\n",
            "*   Product: Perplexinator\n",
            "*   Quantity: 25\n",
            "*   Status: \"Processing\"\n",
            "*   Price per unit: $79.99\n",
            "*   Order total: $1,999.75\n",
            "\n",
            "The \"Anthropic's Data Store State (After Action)\" for order O3 shows:\n",
            "*   `id: \"O3\"`\n",
            "*   `product_id: \"P3\"` (which is \"Perplexinator\" in the products table)\n",
            "*   `product_name: \"Perplexinator\"`\n",
            "*   `quantity: 25`\n",
            "*   `price: 79.99` (this is the unit price)\n",
            "*   `status: \"Processing\"`\n",
            "\n",
            "The product P3 (\"Perplexinator\") details also match:\n",
            "*   `price: 79.99`\n",
            "\n",
            "All details align perfectly. The total order cost ($1,999.75) is correctly derived from quantity (25) * price ($79.99).\n",
            "\n",
            "**3. State whether this review of the final data store causes you to update your previous scores or assessment for Anthropic. If so, provide the updated scores and rationale.**\n",
            "\n",
            "No, this review of the final data store does not cause me to update the previous scores or assessment.\n",
            "The AI's action was purely informational, and the data store correctly reflects no changes were made during this turn. The state is consistent with the information the AI provided, which your initial evaluation found to be 100% accurate based on the \"Ground Truth Data Store State\" (which must have been identical to this \"After Action\" state regarding order O3 for your accuracy score to be 10/10).\n",
            "The system behaved as expected.\n",
            "Extracted score 10 for 'Anthropic' (or generic) using pattern: Anthropic.*?\\b(\\d+)/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6e3d65628e2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \"\"\"\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-1eee863a9d24>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_user_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mresults_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# To catch exits from within the agent processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-03ae30cef88c>\u001b[0m in \u001b[0;36mprocess_user_request\u001b[0;34m(self, user_message)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mhuman_feedback_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mhuman_general_learning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Do you want to add any general learnings from this turn? (Type your learning or 'skip'): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhuman_general_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'skip'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhuman_general_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mhuman_feedback_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhuman_general_learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}