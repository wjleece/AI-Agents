{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/AI-Agents/blob/main/AI_Agents_w_Evals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install anthropic\n",
        "#%pip install openai\n",
        "%pip install -q -U google-generativeai\n",
        "%pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDKcVFI7PAJ5",
        "outputId": "15c9d57a-f4af-4c51-9094-235ed87a489a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.51.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
            "Downloading anthropic-0.51.0-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.0/264.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.51.0\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup and Imports\n",
        "import anthropic\n",
        "import google.generativeai as gemini\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import glob # For finding files matching a pattern\n",
        "import uuid # For generating unique learning IDs in RAG\n",
        "from google.colab import userdata\n",
        "#from openai import OpenAI\n",
        "from google.colab import drive # For Google Drive mounting\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union, Tuple\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "# LLM API Keys\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "#OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "#openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "gemini.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "ANTHROPIC_MODEL_NAME = \"claude-3-5-sonnet-latest\"\n",
        "#OPENAI_MODEL_NAME = \"gpt-4.1\" # Or your preferred GPT-4 class model\n",
        "EVAL_MODEL_NAME = \"gemini-2.5-pro-preview-05-06\" # Or your preferred Gemini model\n",
        "CLASSIFIER_MODEL_NAME = \"gemini-1.5-flash-latest\" # Fast model for question classification / routing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoQ4L2N6o339",
        "outputId": "be3cb053-6f4e-4fcf-df02-00b0b4e8c100"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Drive Authentication & path mapping\n",
        "\n",
        "DRIVE_MOUNT_PATH = '/content/drive'\n",
        "\n",
        "try:\n",
        "    drive.mount(DRIVE_MOUNT_PATH)\n",
        "    print(f\"Google Drive mounted successfully at {DRIVE_MOUNT_PATH}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}. RAG features will not work.\")\n",
        "\n",
        "# Set up the default learnings path\n",
        "DEFAULT_LEARNINGS_DRIVE_SUBPATH = \"My Drive/AI/Knowledgebases\"  # Your default path\n",
        "LEARNINGS_DRIVE_BASE_PATH = os.path.join(DRIVE_MOUNT_PATH, DEFAULT_LEARNINGS_DRIVE_SUBPATH)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(LEARNINGS_DRIVE_BASE_PATH):\n",
        "    try:\n",
        "        os.makedirs(LEARNINGS_DRIVE_BASE_PATH)\n",
        "        print(f\"Created learnings directory: {LEARNINGS_DRIVE_BASE_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating learnings directory {LEARNINGS_DRIVE_BASE_PATH}: {e}\")\n",
        "else:\n",
        "    print(f\"Using existing learnings directory: {LEARNINGS_DRIVE_BASE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VeBrmqG1K-o",
        "outputId": "f0fcc88c-405a-4753-97b4-59747ad90450"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully at /content/drive.\n",
            "Using existing learnings directory: /content/drive/My Drive/AI/Knowledgebases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Specialized System Prompts\n",
        "\n",
        "# --- Worker AI Prompts ---\n",
        "worker_base_instructions = \"\"\"\n",
        "You are a helpful customer service assistant for an e-commerce system.\n",
        "Your overriding goal is to be helpful by answering questions and performing actions as requested by a human user.\n",
        "When responding to the user, use the conversation context to maintain continuity.\n",
        "- If a user refers to \"my order\" or similar, use the context to determine which order they're talking about.\n",
        "- If they mention \"that product\" or use other references, check the context to determine what they're referring to.\n",
        "- Always prioritize recent context over older context when resolving references.\n",
        "\n",
        "The conversation context will be provided to you with each message. This includes:\n",
        "- Previous questions and answers\n",
        "- Recently viewed customers, products, and orders\n",
        "- Recent actions taken (like creating orders, updating products, etc.)\n",
        "- Relevant Learnings from a knowledge base (if applicable to the current query type).\n",
        "- **Crucially, results from any tools you use will also be part of the context provided back to you.**\n",
        "\n",
        "**YOUR RESPONSE TO THE HUMAN USER:**\n",
        "Your primary role is to communicate effectively and naturally with the human user.\n",
        "- After you use tools and get their results (which will be shown to you), your final textual response to the user **must be a friendly, conversational, and easy-to-understand summary.**\n",
        "- **DO NOT output raw data (like JSON strings or complex lists/dictionaries from tool results) directly in your response to the user.**\n",
        "- Instead, you must **interpret the tool results** and explain the outcome or provide the requested information in natural language.\n",
        "- For example, if a tool you used returns information like `{\"product_name\": \"Perplexinator\", \"inventory_count\": 1485}`, your response to the user should be something like: \"Currently, we have 1485 Perplexinators in stock.\" or \"The Perplexinator has 1485 units available, would you like to know more?\"\n",
        "- If you perform an action (e.g., creating an order), confirm this action clearly and provide key details in a sentence, for instance: \"I've successfully created your order (Order ID: O4) for 10 Widgets.\"\n",
        "- Always aim to be helpful, polite, and clear in your language.\n",
        "\n",
        "REQUESTING CLARIFICATION FROM THE USER:\n",
        "If you determine that you absolutely need more information from the user to accurately and efficiently fulfill their request or use a tool correctly, you MUST:\n",
        "1. Formulate a clear, concise question for the user.\n",
        "2. Prefix your entire response with the exact tag: `CLARIFICATION_REQUESTED:`\n",
        "   Example: `CLARIFICATION_REQUESTED: To update the order, could you please provide the Order ID?`\n",
        "3. Do NOT use any tools in the same turn you are requesting clarification. Wait for the user's response.\n",
        "\n",
        "Keep all other responses friendly, concise, and helpful.\n",
        "\"\"\"\n",
        "\n",
        "worker_operational_system_prompt = f\"\"\"\n",
        "{worker_base_instructions}\n",
        "\n",
        "Your current task is OPERATIONAL. Focus on understanding user requests related to e-commerce functions (managing orders, products, customers), using the provided tools accurately, and interacting with the data store.\n",
        "The \"Relevant Learnings from Knowledge Base\" provided in your context may contain operational guidelines.\n",
        "Remember to synthesize tool results into a user-friendly textual response. The detailed tool outputs are logged separately for evaluation.\n",
        "\"\"\"\n",
        "\n",
        "worker_metacognitive_learnings_system_prompt = f\"\"\"\n",
        "{worker_base_instructions}\n",
        "\n",
        "Your current task is METACOGNITIVE: SUMMARIZING LEARNINGS AND EXPLAINING YOUR THINKING.\n",
        "If the user asks you to \"summarize your learnings\", \"what have you learned\", \"why did you\", \"is there a better way to\" or similar phrases, your response should be based PRIMARILY on the content provided to you under the heading \"Relevant Learnings from Knowledge Base (In-Session Cache)\" in your current context.\n",
        "- List the key principles or pieces of information from these provided learnings.\n",
        "- Do not confuse these explicit learnings with a general summary of your recent actions or the current state of the data store, unless a learning specifically refers to such an action or state.\n",
        "- If no specific learnings are provided in your context for this type of query, you can state that no specific new learnings have been highlighted for this interaction.\n",
        "- Avoid using tools for this type of summarization unless a tool is specifically designed to retrieve or process learnings.\n",
        "\"\"\"\n",
        "\n",
        "# --- Evaluator AI Prompt (unified but guided by query type information) ---\n",
        "# This prompt is largely the same as the one from worker_prompt_update_learning_summary,\n",
        "# but we will emphasize the query_type in the main prompt to the evaluator.\n",
        "\n",
        "evaluator_system_prompt = \"\"\"\n",
        "You are Google Gemini, an impartial evaluator assessing the quality of responses from an AI assistant to customer service queries.\n",
        "\n",
        "You will be provided with:\n",
        "- The user's query and the TYPE of query it was classified as (e.g., OPERATIONAL, METACOGNITIVE_LEARNINGS_SUMMARY).\n",
        "- The conversation context (including RAG learnings) that was available to the AI assistant.\n",
        "- **The AI assistant's final user-facing textual response.**\n",
        "- **A log of tools called by the AI assistant, including their inputs and raw outputs.**\n",
        "- A snapshot of the 'Data Store State *Before* AI Action'.\n",
        "- A snapshot of the 'Data Store State *After* AI Action'.\n",
        "- Details of any clarification questions the AI assistant asked.\n",
        "\n",
        "Your primary goal is to assess the AI assistant based on the SPECIFIC TASK it was attempting, as indicated by the query type.\n",
        "\n",
        "For each interaction, evaluate the assistant's response based on:\n",
        "1.  **Accuracy**:\n",
        "    * If OPERATIONAL:\n",
        "        * How correct and factual is the AI's **user-facing textual response**?\n",
        "        * Does the textual response accurately reflect the outcomes of the **tool calls** and changes in the datastore?\n",
        "        * Did its actions (tool calls) correctly process information or modify the datastore as intended? Verify against 'Tool Call Log', 'Before' and 'After' states.\n",
        "    * If METACOGNITIVE_LEARNINGS_SUMMARY: Did the AI accurately summarize the \"Relevant Learnings from Knowledge Base\" provided in its context? Was the summary faithful to these learnings?\n",
        "    * Check for new entity IDs and correct updates if applicable to the query type.\n",
        "\n",
        "2.  **Efficiency**:\n",
        "    * Did the assistant achieve its goal with minimal clarifying questions?\n",
        "    * If OPERATIONAL: Were tool calls used appropriately and efficiently? (Refer to 'Tool Call Log')\n",
        "    * If METACOGNITIVE_LEARNINGS_SUMMARY: Was the summary direct and to the point based on provided learnings?\n",
        "\n",
        "3.  **Context Awareness**:\n",
        "    * Did the assistant correctly use the conversation history and entities?\n",
        "    * Crucially, did the assistant adhere to the task defined by the query type?\n",
        "    * Did it correctly use any \"Relevant Learnings from Knowledge Base\" that were pertinent to the query type?\n",
        "    * For OPERATIONAL tasks, did the user-facing response make sense given the tool outputs and datastore changes?\n",
        "\n",
        "4.  **Helpfulness & Clarity (of the user-facing response)**:\n",
        "    * How well did the assistant address the user's needs *for the identified query type* in its textual response?\n",
        "    * Was the **user-facing response clear, polite, and easy to understand?** Did it avoid jargon or raw data dumps?\n",
        "    * Did it provide relevant information in a helpful manner?\n",
        "\n",
        "Score the response on a scale of 1-10 for each criterion, and provide an overall score. Provide detailed reasoning, EXPLICITLY MENTIONING THE QUERY TYPE and referencing the **user-facing text**, the **tool call log**, and **datastore states** as appropriate.\n",
        "- For OPERATIONAL queries, heavily reference the 'Tool Call Log' and 'Before'/'After' data store states when assessing the underlying actions, and then assess if the user-facing text accurately and clearly conveys this.\n",
        "- For METACOGNITIVE_LEARNINGS_SUMMARY, heavily reference the \"Relevant Learnings from Knowledge Base\" that were provided to the worker.\n",
        "\n",
        "EVALUATING CLARIFICATION QUESTIONS:\n",
        "If the worker AI asked for clarification:\n",
        "- Assess necessity using 'Data Store State *Before* AI Action' and context.\n",
        "- If necessary and well-phrased, it should NOT negatively impact Efficiency.\n",
        "- If unnecessary, it SHOULD negatively impact Efficiency.\n",
        "\n",
        "If you, the evaluator, still have questions, use \"CLARIFICATION NEEDED_EVALUATOR:\".\n",
        "\n",
        "DATA STORE CONSISTENCY (Primarily for OPERATIONAL tasks):\n",
        "When assessing Accuracy for OPERATIONAL tasks, explicitly compare the AI's actions (via tool log and datastore changes) with its claims in the user-facing text.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zMWjRsv3QNc1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Data Stores (Initial data - will be managed by the Storage class instance)\n",
        "# These are initial values. The Storage class will manage them.\n",
        "initial_customers = {\n",
        "    \"C1\": {\"name\": \"John Doe\", \"email\": \"john@example.com\", \"phone\": \"123-456-7890\"},\n",
        "    \"C2\": {\"name\": \"Jane Smith\", \"email\": \"jane@example.com\", \"phone\": \"987-654-3210\"}\n",
        "}\n",
        "\n",
        "initial_products = {\n",
        "    \"P1\": {\"name\": \"Widget A\", \"description\": \"A simple widget. Very compact.\", \"price\": 19.99, \"inventory_count\": 999},\n",
        "    \"P2\": {\"name\": \"Gadget B\", \"description\": \"A powerful gadget. It spins.\", \"price\": 49.99, \"inventory_count\": 200},\n",
        "    \"P3\": {\"name\": \"Perplexinator\", \"description\": \"A perplexing perfunctator\", \"price\": 79.99, \"inventory_count\": 1483}\n",
        "}\n",
        "\n",
        "initial_orders = {\n",
        "    \"O1\": {\"id\": \"O1\", \"product_id\": \"P1\", \"product_name\": \"Widget A\", \"quantity\": 2, \"price\": 19.99, \"status\": \"Shipped\"},\n",
        "    \"O2\": {\"id\": \"O2\", \"product_id\": \"P2\", \"product_name\": \"Gadget B\", \"quantity\": 1, \"price\": 49.99, \"status\": \"Processing\"}\n",
        "}\n"
      ],
      "metadata": {
        "id": "5G9rP40vQXdU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standalone Anthropic Completion Function (for basic tests)\n",
        "#def get_completion_anthropic_standalone(prompt: str):\n",
        "#    message = anthropic_client.messages.create(\n",
        "#        model=ANTHROPIC_MODEL_NAME,\n",
        "#        max_tokens=2000,\n",
        "#        temperature=0.0,\n",
        "#        system=worker_base_instructions,\n",
        "#        tools=tools_schemas_list,\n",
        "#        messages=[\n",
        "#          {\"role\": \"user\", \"content\": prompt}\n",
        "#        ]\n",
        "#    )\n",
        "#    return message.content[0].text"
      ],
      "metadata": {
        "id": "_ADM0bBpQlVK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt_test_anthropic = \"Hey there, which AI model do you use for answering questions?\"\n",
        "#print(f\"Anthropic Standalone Test: {get_completion_anthropic_standalone(prompt_test_anthropic)}\")"
      ],
      "metadata": {
        "id": "jPOq1s0SQqk_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def get_completion_openai_standalone(prompt: str):\n",
        "#    response = openai_client.chat.completions.create(\n",
        "#        model=OPENAI_MODEL_NAME,\n",
        "#        max_tokens=2000,\n",
        "#        temperature=0.0,\n",
        "#        tools=tools_schemas_list,\n",
        "#        messages=[\n",
        "#            {\"role\": \"system\", \"content\": worker_system_prompt},\n",
        "#            {\"role\": \"user\", \"content\": prompt}\n",
        "#        ]\n",
        "#    )\n",
        "#    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Y5c_bv3qQwWV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt_test_openai = \"Hey there, which AI model do you use for answering questions?\"\n",
        "#print(f\"OpenAI Standalone Test: {get_completion_openai_standalone(prompt_test_openai)}\")"
      ],
      "metadata": {
        "id": "b_LaDQ74Q1Lp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Storage:\n",
        "    \"\"\"Manages the in-memory e-commerce datastore.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.customers = copy.deepcopy(initial_customers)\n",
        "        self.products = copy.deepcopy(initial_products)\n",
        "        self.orders = copy.deepcopy(initial_orders)\n",
        "        print(\"Storage initialized with deep copies of initial data.\")\n",
        "\n",
        "    def get_full_datastore_copy(self) -> Dict[str, Any]:\n",
        "        \"\"\"Returns a deep copy of the current datastore.\"\"\"\n",
        "        return {\n",
        "            \"customers\": copy.deepcopy(self.customers),\n",
        "            \"products\": copy.deepcopy(self.products),\n",
        "            \"orders\": copy.deepcopy(self.orders)\n",
        "        }"
      ],
      "metadata": {
        "id": "_9XC9xyh3XVE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definitive list of tool schemas.\n",
        "tools_schemas_list = [\n",
        "    {\n",
        "        \"name\": \"create_customer\",\n",
        "        \"description\": \"Adds a new customer to the database. Includes customer name, email, and (optional) phone number.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The name of the customer.\"},\n",
        "                \"email\": {\"type\": \"string\", \"description\": \"The email address of the customer.\"},\n",
        "                \"phone\": {\"type\": \"string\", \"description\": \"The phone number of the customer (optional).\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"email\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_customer_info\",\n",
        "        \"description\": \"Retrieves customer information based on their customer ID. Returns the customer's name, email, and (optional) phone number.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"customer_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the customer.\"}\n",
        "            },\n",
        "            \"required\": [\"customer_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_product\",\n",
        "        \"description\": \"Adds a new product to the product database. Includes name, description, price, and initial inventory count.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The name of the product.\"},\n",
        "                \"description\": {\"type\": \"string\", \"description\": \"A description of the product.\"},\n",
        "                \"price\": {\"type\": \"number\", \"description\": \"The price of the product.\"},\n",
        "                \"inventory_count\": {\"type\": \"integer\", \"description\": \"The amount of the product that is currently in inventory.\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"description\", \"price\", \"inventory_count\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"update_product\",\n",
        "        \"description\": \"Updates an existing product with new information. Only fields that are provided will be updated; other fields remain unchanged.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the product to update.\"},\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The new name for the product (optional).\"},\n",
        "                \"description\": {\"type\": \"string\", \"description\": \"The new description for the product (optional).\"},\n",
        "                \"price\": {\"type\": \"number\", \"description\": \"The new price for the product (optional).\"},\n",
        "                \"inventory_count\": {\"type\": \"integer\", \"description\": \"The new inventory count for the product (optional).\"}\n",
        "            },\n",
        "            \"required\": [\"product_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_product_info\",\n",
        "        \"description\": \"Retrieves product information based on product ID or product name (with fuzzy matching for misspellings). Returns product details including name, description, price, and inventory count.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id_or_name\": {\"type\": \"string\", \"description\": \"The product ID or name (can be approximate).\"}\n",
        "            },\n",
        "            \"required\": [\"product_id_or_name\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"list_all_products\",\n",
        "        \"description\": \"Lists all available products in the inventory.\",\n",
        "        \"input_schema\": { \"type\": \"object\", \"properties\": {}, \"required\": [] }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_order\",\n",
        "        \"description\": \"Creates an order using the product's current price. If requested quantity exceeds available inventory, no order is created and available quantity is returned. Orders can only be created for products that are in stock. Supports specifying products by either ID or name with fuzzy matching for misspellings.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"product_id_or_name\": {\"type\": \"string\", \"description\": \"The ID or name of the product to order (supports fuzzy matching).\"},\n",
        "                \"quantity\": {\"type\": \"integer\", \"description\": \"The quantity of the product in the order.\"},\n",
        "                \"status\": {\"type\": \"string\", \"description\": \"The initial status of the order (e.g., 'Processing', 'Shipped').\"}\n",
        "            },\n",
        "            \"required\": [\"product_id_or_name\", \"quantity\", \"status\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"get_order_details\",\n",
        "        \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"order_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the order.\"}\n",
        "            },\n",
        "            \"required\": [\"order_id\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"update_order_status\",\n",
        "        \"description\": \"Updates the status of an order and adjusts inventory accordingly. Changing to \\\"Shipped\\\" decreases inventory. Changing to \\\"Returned\\\" or \\\"Canceled\\\" from \\\"Shipped\\\" increases inventory. Status can be \\\"Processing\\\", \\\"Shipped\\\", \\\"Delivered\\\", \\\"Returned\\\", or \\\"Canceled\\\".\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"order_id\": {\"type\": \"string\", \"description\": \"The unique identifier for the order.\"},\n",
        "                \"new_status\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The new status to set for the order.\",\n",
        "                    \"enum\": [\"Processing\", \"Shipped\", \"Delivered\", \"Returned\", \"Canceled\"]\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"order_id\", \"new_status\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "print(f\"Defined {len(tools_schemas_list)} tool schemas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PREbUQfh2UBj",
        "outputId": "7f9f73cf-3c68-4747-a975-cbb6464b349c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined 9 tool schemas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tool Functions (Global for now, passed to ToolExecutor) ---\n",
        "\n",
        "def create_customer(current_storage: Storage, name: str, email: str, phone: Optional[str] = None): # Simplified for brevity\n",
        "    new_id = f\"C{len(current_storage.customers) + 1}\"\n",
        "    while new_id in current_storage.customers: new_id = f\"C{int(new_id[1:]) + 1}\"\n",
        "    current_storage.customers[new_id] = {\"name\": name, \"email\": email, \"phone\": phone}\n",
        "    print(f\"[Tool Exec] create_customer: ID {new_id}\")\n",
        "    return {\"status\": \"success\", \"customer_id\": new_id, \"customer\": current_storage.customers[new_id]}\n",
        "def get_customer_info(current_storage: Storage, customer_id: str):\n",
        "    customer = current_storage.customers.get(customer_id)\n",
        "    if customer: print(f\"[Tool Exec] get_customer_info: ID {customer_id} found.\"); return {\"status\": \"success\", \"customer\": customer}\n",
        "    print(f\"[Tool Exec] get_customer_info: ID {customer_id} not found.\"); return {\"status\": \"error\", \"message\": \"Customer not found\"}\n",
        "def create_product(current_storage: Storage, name: str, description: str, price: float, inventory_count: int):\n",
        "    new_id = f\"P{len(current_storage.products) + 1}\"\n",
        "    while new_id in current_storage.products: new_id = f\"P{int(new_id[1:]) + 1}\"\n",
        "    current_storage.products[new_id] = {\"name\": name, \"description\": description, \"price\": price, \"inventory_count\": inventory_count}\n",
        "    print(f\"[Tool Exec] create_product: ID {new_id}\")\n",
        "    return {\"status\": \"success\", \"product_id\": new_id, \"product\": current_storage.products[new_id]}\n",
        "def update_product(current_storage: Storage, product_id: str, name: Optional[str]=None, description: Optional[str]=None, price: Optional[float]=None, inventory_count: Optional[int]=None):\n",
        "    if product_id not in current_storage.products: print(f\"[Tool Exec] update_product: ID {product_id} not found.\"); return {\"status\": \"error\", \"message\": \"Product not found\"}\n",
        "    product = current_storage.products[product_id]; updated_fields = []\n",
        "    if name: product[\"name\"] = name; updated_fields.append(\"name\")\n",
        "    if description: product[\"description\"] = description; updated_fields.append(\"description\")\n",
        "    if price: product[\"price\"] = price; updated_fields.append(\"price\")\n",
        "    if inventory_count is not None : product[\"inventory_count\"] = inventory_count; updated_fields.append(\"inventory_count\")\n",
        "    if not updated_fields: print(f\"[Tool Exec] update_product: ID {product_id}, no fields updated.\"); return {\"status\":\"warning\", \"message\":\"No fields updated.\"}\n",
        "    print(f\"[Tool Exec] update_product: ID {product_id}, Updated: {', '.join(updated_fields)}\")\n",
        "    return {\"status\": \"success\", \"product_id\": product_id, \"product\": product, \"updated_fields\": updated_fields}\n",
        "def find_product_by_name(current_storage: Storage, product_name: str, min_similarity: int = 70) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
        "    if not product_name: return None, None\n",
        "    name_id_list = [(p_data[\"name\"], p_id) for p_id, p_data in current_storage.products.items()]\n",
        "    if not name_id_list: return None, None\n",
        "    best_match_name_score = process.extractOne(product_name, [item[0] for item in name_id_list], scorer=fuzz.token_sort_ratio)\n",
        "    if best_match_name_score and best_match_name_score[1] >= min_similarity:\n",
        "        matched_name = best_match_name_score[0]\n",
        "        for name_val, pid_val in name_id_list:\n",
        "            if name_val == matched_name: return pid_val, current_storage.products[pid_val]\n",
        "    return None, None\n",
        "def get_product_id(current_storage: Storage, product_identifier: str) -> Optional[str]:\n",
        "    if product_identifier in current_storage.products: return product_identifier\n",
        "    product_id, _ = find_product_by_name(current_storage, product_identifier)\n",
        "    return product_id\n",
        "def get_product_info(current_storage: Storage, product_id_or_name: str):\n",
        "    pid = get_product_id(current_storage, product_id_or_name)\n",
        "    if pid and pid in current_storage.products:\n",
        "        print(f\"[Tool Exec] get_product_info: Found '{product_id_or_name}' as ID '{pid}'.\")\n",
        "        return {\"status\": \"success\", \"product_id\": pid, \"product\": current_storage.products[pid]}\n",
        "    print(f\"[Tool Exec] get_product_info: Product '{product_id_or_name}' not found.\")\n",
        "    return {\"status\": \"error\", \"message\": f\"Product '{product_id_or_name}' not found\"}\n",
        "def list_all_products(current_storage: Storage):\n",
        "    print(f\"[Tool Exec] list_all_products: Found {len(current_storage.products)} products.\")\n",
        "    return {\"status\": \"success\", \"count\": len(current_storage.products), \"products\": current_storage.products}\n",
        "def create_order(current_storage: Storage, product_id_or_name: str, quantity: int, status: str):\n",
        "    actual_product_id = get_product_id(current_storage, product_id_or_name)\n",
        "    if not actual_product_id: print(f\"[Tool Exec] create_order: Product '{product_id_or_name}' not found.\"); return {\"status\": \"error\", \"message\": f\"Product '{product_id_or_name}' not found.\"}\n",
        "    product = current_storage.products[actual_product_id]\n",
        "    if product[\"inventory_count\"] < quantity and status == \"Shipped\": print(f\"[Tool Exec] create_order: Insufficient inventory for {product['name']} to ship.\"); return {\"status\": \"error\", \"message\": f\"Insufficient inventory for {product['name']} to ship. Available: {product['inventory_count']}\"}\n",
        "    if status == \"Shipped\" and product[\"inventory_count\"] >= quantity : product[\"inventory_count\"] -= quantity\n",
        "    new_id = f\"O{len(current_storage.orders) + 1}\"\n",
        "    while new_id in current_storage.orders: new_id = f\"O{int(new_id[1:]) + 1}\"\n",
        "    current_storage.orders[new_id] = {\"id\": new_id, \"product_id\": actual_product_id, \"product_name\": product[\"name\"], \"quantity\": quantity, \"price\": product[\"price\"], \"status\": status}\n",
        "    print(f\"[Tool Exec] create_order: Order {new_id} for {product['name']}. Status: {status}. Inv: {product['inventory_count']}\")\n",
        "    return {\"status\": \"success\", \"order_id\": new_id, \"order_details\": current_storage.orders[new_id], \"remaining_inventory\": product[\"inventory_count\"]}\n",
        "def get_order_details(current_storage: Storage, order_id: str):\n",
        "    order = current_storage.orders.get(order_id)\n",
        "    if order: print(f\"[Tool Exec] get_order_details: Order {order_id} found.\"); return {\"status\": \"success\", \"order_details\": order}\n",
        "    print(f\"[Tool Exec] get_order_details: Order {order_id} not found.\"); return {\"status\": \"error\", \"message\": \"Order not found\"}\n",
        "def update_order_status(current_storage: Storage, order_id: str, new_status: str):\n",
        "    if order_id not in current_storage.orders: print(f\"[Tool Exec] update_order_status: Order {order_id} not found.\"); return {\"status\": \"error\", \"message\": \"Order not found\"}\n",
        "    order = current_storage.orders[order_id]; product_id = order[\"product_id\"]; quantity = order[\"quantity\"]\n",
        "    product = current_storage.products[product_id]; old_status = order[\"status\"]\n",
        "    if old_status == new_status: print(f\"[Tool Exec] update_order_status: Status for {order_id} already {new_status}.\"); return {\"status\": \"unchanged\", \"message\": \"Status is already \" + new_status}\n",
        "    inventory_adjusted = False\n",
        "    if new_status == \"Shipped\" and old_status != \"Shipped\":\n",
        "        if product[\"inventory_count\"] < quantity: print(f\"[Tool Exec] update_order_status: Insufficient inv for {product_id} to ship order {order_id}.\"); return {\"status\": \"error\", \"message\": \"Insufficient inventory to ship.\"}\n",
        "        product[\"inventory_count\"] -= quantity; inventory_adjusted = True\n",
        "    elif old_status == \"Shipped\" and new_status != \"Shipped\": # e.g. returned/canceled from shipped\n",
        "        product[\"inventory_count\"] += quantity; inventory_adjusted = True\n",
        "    order[\"status\"] = new_status\n",
        "    print(f\"[Tool Exec] update_order_status: Order {order_id} to {new_status}. Inv for {product_id} is {product['inventory_count']}. Adjusted: {inventory_adjusted}\")\n",
        "    return {\"status\": \"success\", \"order_id\": order_id, \"order_details\": order, \"current_inventory\": product[\"inventory_count\"], \"inventory_adjusted\": inventory_adjusted}\n",
        "print(\"Tool functions defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGGSeN7T2W1O",
        "outputId": "3fd69c9d-aaa2-4a86-dba3-0f2a8eb646d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryClassifier:\n",
        "    \"\"\"Classifies user queries using an LLM.\"\"\"\n",
        "    def __init__(self, llm_client):\n",
        "        self.llm_client = llm_client\n",
        "        self.classification_prompt_template = \"\"\"\n",
        "Classify the following user query into one of these categories: OPERATIONAL, METACOGNITIVE_LEARNINGS_SUMMARY.\n",
        "Return ONLY the category name.\n",
        "\n",
        "OPERATIONAL queries are about performing e-commerce tasks, like asking about products, creating orders, or updating customer information.\n",
        "Examples of OPERATIONAL:\n",
        "- \"Show me all shoes.\"\n",
        "- \"What's the price of P1?\"\n",
        "- \"Create an order for 2 widgets.\"\n",
        "- \"Update my address.\"\n",
        "\n",
        "METACOGNITIVE_LEARNINGS_SUMMARY queries are about the AI's own learning process or knowledge derived from feedback.\n",
        "Examples of METACOGNITIVE_LEARNINGS_SUMMARY:\n",
        "- \"Summarize your learnings.\"\n",
        "- \"What have you learned recently?\"\n",
        "- \"Tell me about your new knowledge.\"\n",
        "- \"Why did you do that in the last turn?\"\n",
        "- \"Is there a better way to handle X?\"\n",
        "\n",
        "User Query: \"{user_message}\"\n",
        "Classification:\"\"\"\n",
        "        print(\"QueryClassifier initialized with LLM client.\")\n",
        "\n",
        "    def classify(self, user_message: str) -> str:\n",
        "        \"\"\"Classifies the user query using the LLM.\"\"\"\n",
        "        prompt = self.classification_prompt_template.format(user_message=user_message)\n",
        "        try:\n",
        "            response = self.llm_client.generate_content(prompt)\n",
        "            classification = response.text.strip()\n",
        "            if classification in [\"OPERATIONAL\", \"METACOGNITIVE_LEARNINGS_SUMMARY\"]:\n",
        "                return classification\n",
        "            else:\n",
        "                print(f\"[QueryClassifier Warning] LLM returned unexpected classification: '{classification}'. Defaulting to OPERATIONAL.\")\n",
        "                return \"OPERATIONAL\"\n",
        "        except Exception as e:\n",
        "            print(f\"[QueryClassifier Error] Failed to classify query using LLM: {e}. Defaulting to OPERATIONAL.\")\n",
        "            return \"OPERATIONAL\""
      ],
      "metadata": {
        "id": "Qxvm6ah2z1wO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationManager:\n",
        "    \"\"\"Manages conversation history and context data.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.messages: List[Dict[str, Any]] = []\n",
        "        self.context_data: Dict[str, Any] = {\n",
        "            \"customers\": {}, \"products\": {}, \"orders\": {}, \"last_action\": None\n",
        "        }\n",
        "        print(\"ConversationManager initialized.\")\n",
        "\n",
        "    def add_user_message(self, message: str) -> None:\n",
        "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    def add_assistant_message(self, message_content: Union[str, List[Dict[str, Any]]], query_type: str) -> None:\n",
        "        if isinstance(message_content, str):\n",
        "            content_to_log = f\"[{query_type}]: {message_content}\"\n",
        "        else:\n",
        "            content_to_log = message_content\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": content_to_log})\n",
        "\n",
        "    def update_entity_in_context(self, entity_type: str, entity_id: str, data: Any) -> None:\n",
        "        if entity_type in self.context_data:\n",
        "            self.context_data[entity_type][entity_id] = data\n",
        "            print(f\"[CM_Context Updated] Entity: {entity_type}, ID: {entity_id}\")\n",
        "\n",
        "    def set_last_action(self, action_type: str, action_details: Any) -> None:\n",
        "        self.context_data[\"last_action\"] = {\n",
        "            \"type\": action_type,\n",
        "            \"details\": action_details,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        print(f\"[CM_Context Updated] Last Action: {action_type}\")\n",
        "\n",
        "    def get_full_conversation_for_api(self) -> List[Dict[str, Any]]:\n",
        "        return self.messages.copy()\n",
        "\n",
        "    def get_context_summary(self) -> str:\n",
        "        summary_parts = []\n",
        "        if self.context_data[\"customers\"]: summary_parts.append(f\"Recent customers: {list(self.context_data['customers'].keys())}\")\n",
        "        if self.context_data[\"products\"]: summary_parts.append(f\"Recent products: {list(self.context_data['products'].keys())}\")\n",
        "        if self.context_data[\"orders\"]: summary_parts.append(f\"Recent orders: {list(self.context_data['orders'].keys())}\")\n",
        "        if self.context_data[\"last_action\"]: summary_parts.append(f\"Last action type: {self.context_data['last_action']['type']}\")\n",
        "        return \"\\\\n\".join(summary_parts) if summary_parts else \"No specific context items set yet.\"\n"
      ],
      "metadata": {
        "id": "uhTMNPio0BbT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToolExecutor:\n",
        "    def __init__(self, available_tools_dict: Dict[str, callable]):\n",
        "        self.available_tools = available_tools_dict\n",
        "        print(\"ToolExecutor initialized.\")\n",
        "\n",
        "    def execute_tool(self, tool_name: str, tool_input: Dict[str, Any], storage_instance: Storage) -> Dict[str, Any]:\n",
        "        if tool_name in self.available_tools:\n",
        "            try:\n",
        "                tool_function = self.available_tools[tool_name]\n",
        "                result = tool_function(storage_instance, **tool_input)\n",
        "                print(f\"--- [ToolExecutor] Result for {tool_name}: {json.dumps(result, indent=2, default=str)} ---\")\n",
        "                return result\n",
        "            except Exception as e:\n",
        "                print(f\"--- [ToolExecutor Error] executing {tool_name}: {e} ---\"); import traceback; traceback.print_exc()\n",
        "                return {\"status\": \"error\", \"message\": f\"Error executing tool {tool_name}: {str(e)}\"}\n",
        "        print(f\"--- [ToolExecutor Error] Tool {tool_name} not found. ---\")\n",
        "        return {\"status\": \"error\", \"message\": f\"Tool {tool_name} not found.\"}"
      ],
      "metadata": {
        "id": "Qxo-Ho_R0VNy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeManager:\n",
        "    def __init__(self, base_path: str, drive_mount_path: str, default_subpath: str, evaluator_llm_instance):\n",
        "        self.base_drive_path = base_path\n",
        "        self.drive_mount_path = drive_mount_path\n",
        "        self.default_drive_subpath = default_subpath\n",
        "        self.evaluator_llm = evaluator_llm_instance # This LLM is used for learning synthesis\n",
        "        self.active_learnings_cache: List[Dict] = self._load_initial_learnings_from_drive()\n",
        "        self.learnings_updated_this_session_flag: bool = False\n",
        "        print(f\"KnowledgeManager initialized. Loaded {len(self.active_learnings_cache)} initial learnings from {self.base_drive_path}.\")\n",
        "\n",
        "    def _mount_drive_if_needed(self):\n",
        "        if not os.path.exists(self.drive_mount_path) or not os.listdir(self.drive_mount_path):\n",
        "            try:\n",
        "                drive.mount(self.drive_mount_path, force_remount=True)\n",
        "                print(\"Drive mounted by KnowledgeManager.\")\n",
        "            except Exception as e:\n",
        "                print(f\"KM: Error mounting Drive: {e}.\")\n",
        "\n",
        "    def _initialize_learnings_path(self):\n",
        "        if not os.path.exists(self.base_drive_path):\n",
        "            try:\n",
        "                os.makedirs(self.base_drive_path)\n",
        "                print(f\"KM: Created learnings directory: {self.base_drive_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"KM: Error creating learnings directory {self.base_drive_path}: {e}\")\n",
        "\n",
        "    def _get_latest_learnings_filepath(self) -> Optional[str]:\n",
        "        self._mount_drive_if_needed()\n",
        "        self._initialize_learnings_path()\n",
        "        if not os.path.isdir(self.base_drive_path):\n",
        "            return None\n",
        "        list_of_files = glob.glob(os.path.join(self.base_drive_path, 'learnings_*.json'))\n",
        "        return max(list_of_files, key=os.path.getctime) if list_of_files else None\n",
        "\n",
        "    def _read_learnings_from_file(self, filepath: str) -> List[Dict]:\n",
        "        if not filepath or not os.path.exists(filepath):\n",
        "            return []\n",
        "        try:\n",
        "            with open(filepath, 'r') as f:\n",
        "                learnings_list = json.load(f)\n",
        "            return learnings_list if isinstance(learnings_list, list) else []\n",
        "        except Exception as e:\n",
        "            print(f\"KM: Error reading/decoding {filepath}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _load_initial_learnings_from_drive(self) -> List[Dict]:\n",
        "        latest_filepath = self._get_latest_learnings_filepath()\n",
        "        if latest_filepath:\n",
        "            print(f\"KM: Loading initial learnings from: {latest_filepath}\")\n",
        "            return self._read_learnings_from_file(latest_filepath)\n",
        "        print(\"KM: No existing learnings file found for initial load.\")\n",
        "        return []\n",
        "\n",
        "    def persist_active_learnings(self):\n",
        "        self._mount_drive_if_needed()\n",
        "        self._initialize_learnings_path()\n",
        "        if not os.path.isdir(self.base_drive_path):\n",
        "            print(\"KM CRITICAL: Learnings directory not available.\")\n",
        "            return\n",
        "        if not self.active_learnings_cache: # Only persist if there's something to save and it has been updated\n",
        "            print(\"KM: Active learnings cache is empty or unchanged. Nothing to persist.\")\n",
        "            # self.learnings_updated_this_session_flag = False # Reset only if persistence happens or if explicitly cleared\n",
        "            return\n",
        "\n",
        "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
        "        new_filepath = os.path.join(self.base_drive_path, f'learnings_{timestamp_str}.json')\n",
        "        try:\n",
        "            with open(new_filepath, 'w') as f:\n",
        "                json.dump(self.active_learnings_cache, f, indent=4)\n",
        "            print(f\"KM: Persisted {len(self.active_learnings_cache)} active learnings to: {new_filepath}\")\n",
        "            self.learnings_updated_this_session_flag = False # Reset flag after successful persistence\n",
        "        except Exception as e:\n",
        "            print(f\"KM: Error persisting active learnings to {new_filepath}: {e}\")\n",
        "\n",
        "    def get_relevant_learnings_for_prompt(self, query: str, query_type: str, learning_target_filter: Optional[str] = None, count: int = 5) -> str:\n",
        "        if not self.active_learnings_cache:\n",
        "            return \"No specific relevant learnings from knowledge base provided for this query.\"\n",
        "\n",
        "        learnings_to_consider = self.active_learnings_cache\n",
        "\n",
        "        # Future enhancement: Filter by learning_target_filter if provided\n",
        "        # For now, we primarily filter by query type and keywords as before.\n",
        "        # Example of how target filter could be used:\n",
        "        # if learning_target_filter:\n",
        "        #     learnings_to_consider = [\n",
        "        #         entry for entry in learnings_to_consider\n",
        "        #         if entry.get(\"learning_target\") == learning_target_filter or entry.get(\"learning_target\") == \"General\"\n",
        "        #     ]\n",
        "\n",
        "        relevant_learnings = []\n",
        "        if query_type == \"METACOGNITIVE_LEARNINGS_SUMMARY\":\n",
        "            relevant_learnings = learnings_to_consider # Show all for summary requests\n",
        "        elif query_type == \"OPERATIONAL\":\n",
        "            keywords = self._extract_keywords(query)\n",
        "            relevant_learnings = [\n",
        "                entry for entry in learnings_to_consider\n",
        "                if any(kw.lower() in (entry.get(\"final_learning_statement\", \"\") + \" \" + \" \".join(entry.get(\"keywords\", []))).lower() for kw in keywords)\n",
        "            ]\n",
        "\n",
        "        relevant_learnings.sort(key=lambda x: x.get('timestamp_created', ''), reverse=True)\n",
        "        relevant_learnings = relevant_learnings[:count]\n",
        "\n",
        "\n",
        "        formatted_learnings = [f\"- Learning (ID: {entry.get('learning_id', 'N/A')[:8]}, Target: {entry.get('learning_target', 'N/A')}): {entry.get('final_learning_statement', str(entry))}\" for entry in relevant_learnings]\n",
        "\n",
        "        if not formatted_learnings:\n",
        "            return \"No specific relevant learnings from knowledge base found for this query.\"\n",
        "        return \"\\\\nRelevant Learnings from Knowledge Base (In-Session Cache):\\\\n\" + \"\\\\n\".join(formatted_learnings)\n",
        "\n",
        "    def _extract_keywords(self, text: str) -> List[str]:\n",
        "        if not text:\n",
        "            return [\"general\"] # Default keyword\n",
        "        words = re.findall(r'\\\\b\\\\w{4,}\\\\b', text.lower()) # find words of 4 or more letters\n",
        "        # Expanded stop_words list\n",
        "        stop_words = {\n",
        "            \"the\", \"and\", \"is\", \"in\", \"to\", \"a\", \"of\", \"for\", \"with\", \"on\", \"at\", \"an\", \"by\", \"not\", \"or\", \"as\", \"if\",\n",
        "            \"it\", \"its\", \"this\", \"that\", \"these\", \"those\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
        "            \"do\", \"does\", \"did\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\",\n",
        "            \"what\", \"who\", \"whom\", \"which\", \"when\", \"where\", \"why\", \"how\",\n",
        "            \"show\", \"tell\", \"please\", \"user\", \"query\", \"context\", \"claude\", \"anthropic\", \"gemini\", \"google\",\n",
        "            \"before\", \"after\", \"state\", \"action\", \"truth\", \"ground\", \"learnings\", \"summarize\", \"your\", \"you\", \"me\", \"my\",\n",
        "            \"provide\", \"give\", \"make\", \"update\", \"create\", \"get\", \"list\", \"item\", \"items\", \"detail\", \"details\"\n",
        "        }\n",
        "        extracted = list(set(word for word in words if word not in stop_words and not word.isdigit()))\n",
        "        return extracted if extracted else [\"generic\"] # Return \"generic\" if no suitable keywords found\n",
        "\n",
        "    def synthesize_and_store_learning(self, human_feedback_text: str, user_query_context: str, turn_context_summary: str, learning_target: str):\n",
        "        print(f\"--- KM: Processing New Learning Candidate (Target: {learning_target}): \\\"{human_feedback_text}\\\" ---\")\n",
        "\n",
        "        current_feedback_to_process = human_feedback_text\n",
        "        attempt_count = 0\n",
        "        max_attempts = 3 # Max attempts for resolving conflict via modification\n",
        "\n",
        "        while attempt_count < max_attempts:\n",
        "            attempt_count += 1\n",
        "            print(f\"KM: Learning Synthesis Attempt {attempt_count}/{max_attempts}\")\n",
        "\n",
        "            # Constructing the prompt for the LLM to synthesize the learning\n",
        "            # The evaluator_llm instance (Gemini) is used for this synthesis task.\n",
        "            evaluator_task_prompt_parts = [\n",
        "                \"You are an AI assistant helping to maintain a knowledge base of 'learnings' from human feedback.\",\n",
        "                f\"The human feedback is targeted towards: {learning_target}.\",\n",
        "                f\"New Human Feedback to process: \\\"{current_feedback_to_process}\\\"\",\n",
        "                f\"Original User Query that led to this feedback: \\\"{user_query_context}\\\"\",\n",
        "                f\"General Conversation Context when feedback was given: \\\"{turn_context_summary}\\\"\",\n",
        "                \"Existing ACTIVE learnings (sample of last 3, if any):\" + \"\".join([f\"  - (ID: {entry.get('learning_id','N/A')[:6]}) {entry.get('final_learning_statement', '')[:100]}...\" for entry in self.active_learnings_cache[-3:]]) if self.active_learnings_cache else \"  - No existing learnings in cache.\",\n",
        "                \"Your Tasks:\",\n",
        "                \"1. Analyze the 'New Human Feedback'.\",\n",
        "                \"2. Check for CONFLICT or significant REDUNDANCY with existing learnings (even if not in the sample above, consider general knowledge principles).\",\n",
        "                \"3. If the feedback is new, valuable, non-conflicting, and non-redundant, synthesize it into a concise, actionable 'Finalized Learning Statement'. This statement should be generalizable if possible.\",\n",
        "                \"Output Format Instructions:\",\n",
        "                \"- If suitable for storing: `FINALIZED_LEARNING: [synthesized statement]`\",\n",
        "                \"- If it conflicts: `CONFLICT_DETECTED: [Explanation of the conflict, and if possible, reference key phrases or IDs of conflicting existing learnings]. Proposed statement if you tried to rephrase: [rephrased statement, or original if no rephrase attempt]`\",\n",
        "                \"- If it's redundant: `REDUNDANT_LEARNING: [Explanation of redundancy, and if possible, reference key phrases or IDs of the existing learning it's redundant with]. Proposed statement if you tried to rephrase: [rephrased statement, or original if no rephrase attempt]`\",\n",
        "                \"- If not actionable/too vague: `NOT_ACTIONABLE: [Explanation]`\",\n",
        "                \"Ensure your entire response strictly follows one of these prefixed formats.\"\n",
        "            ]\n",
        "            synthesis_prompt = \"\\\\n\".join(evaluator_task_prompt_parts)\n",
        "\n",
        "            try:\n",
        "                synthesis_response_obj = self.evaluator_llm.generate_content(synthesis_prompt)\n",
        "                evaluator_synthesis_text = synthesis_response_obj.text.strip()\n",
        "                print(f\"KM: Gemini Learning Synthesis Raw Response:\\\\n{evaluator_synthesis_text}\")\n",
        "\n",
        "                final_statement = None\n",
        "                conflict_explanation = None\n",
        "                redundant_explanation = None\n",
        "                not_actionable_explanation = None\n",
        "\n",
        "                if evaluator_synthesis_text.startswith(\"FINALIZED_LEARNING:\"):\n",
        "                    final_statement = evaluator_synthesis_text.replace(\"FINALIZED_LEARNING:\", \"\", 1).strip()\n",
        "                elif evaluator_synthesis_text.startswith(\"CONFLICT_DETECTED:\"):\n",
        "                    conflict_explanation = evaluator_synthesis_text.replace(\"CONFLICT_DETECTED:\", \"\", 1).strip()\n",
        "                elif evaluator_synthesis_text.startswith(\"REDUNDANT_LEARNING:\"):\n",
        "                    redundant_explanation = evaluator_synthesis_text.replace(\"REDUNDANT_LEARNING:\", \"\", 1).strip()\n",
        "                elif evaluator_synthesis_text.startswith(\"NOT_ACTIONABLE:\"):\n",
        "                    not_actionable_explanation = evaluator_synthesis_text.replace(\"NOT_ACTIONABLE:\", \"\", 1).strip()\n",
        "                else:\n",
        "                    print(\"KM: Gemini learning synthesis response format unexpected. Defaulting to not actionable.\")\n",
        "                    not_actionable_explanation = f\"Response format error: {evaluator_synthesis_text}\"\n",
        "\n",
        "\n",
        "                if final_statement:\n",
        "                    self.active_learnings_cache.append({\n",
        "                        \"learning_id\": str(uuid.uuid4()),\n",
        "                        \"timestamp_created\": datetime.now().isoformat(),\n",
        "                        \"original_human_input\": human_feedback_text, # Store the very first human input for this learning\n",
        "                        \"processed_human_input\": current_feedback_to_process, # Store the input that led to this successful synthesis\n",
        "                        \"final_learning_statement\": final_statement,\n",
        "                        \"keywords\": self._extract_keywords(final_statement + \" \" + current_feedback_to_process),\n",
        "                        \"status\": \"active\",\n",
        "                        \"learning_target\": learning_target\n",
        "                    })\n",
        "                    self.learnings_updated_this_session_flag = True\n",
        "                    print(f\"KM: Stored new learning. Cache size: {len(self.active_learnings_cache)}\")\n",
        "                    return # Successfully stored, exit the loop and method\n",
        "\n",
        "                elif conflict_explanation:\n",
        "                    print(f\"KM: Learning conflict detected by Gemini: {conflict_explanation}\")\n",
        "                    if attempt_count < max_attempts:\n",
        "                        print(\"KM: --- CONFLICT RESOLUTION ---\")\n",
        "                        print(f\"Original feedback: '{human_feedback_text}'\")\n",
        "                        print(f\"Feedback being processed: '{current_feedback_to_process}'\")\n",
        "                        user_choice = input(\"Conflict detected. (M)odify your feedback, (S)kip storing, or (P)roceed with current version for resynthesis? [M/S/P]: \").strip().upper()\n",
        "                        if user_choice == 'M':\n",
        "                            new_feedback = input(\"Enter your modified feedback: \").strip()\n",
        "                            if new_feedback:\n",
        "                                current_feedback_to_process = new_feedback\n",
        "                                print(\"KM: Retrying synthesis with modified feedback.\")\n",
        "                                continue # Continue to next iteration of the while loop\n",
        "                            else:\n",
        "                                print(\"KM: No modification provided. Skipping.\")\n",
        "                                return\n",
        "                        elif user_choice == 'P':\n",
        "                            # Allow one more attempt with the current (potentially problematic) feedback,\n",
        "                            # relying on the LLM to do its best or for the user to accept the outcome.\n",
        "                            # Or, if the LLM provided a \"Proposed statement\", we could use that.\n",
        "                            # For simplicity now, we just resubmit `current_feedback_to_process`.\n",
        "                            print(\"KM: User chose to proceed. Retrying synthesis with current feedback version.\")\n",
        "                            # No change to current_feedback_to_process, just continue loop.\n",
        "                            continue\n",
        "                        else: # Default to Skip\n",
        "                            print(\"KM: Skipping this learning due to unresolved conflict.\")\n",
        "                            return\n",
        "                    else:\n",
        "                        print(\"KM: Max attempts reached for conflict resolution. Skipping this learning.\")\n",
        "                        return\n",
        "\n",
        "                elif redundant_explanation:\n",
        "                    print(f\"KM: Learning deemed redundant by Gemini: {redundant_explanation}\")\n",
        "                    user_choice_redundant = input(\"This learning seems redundant. (S)kip storing, or (F)orce store anyway? [S/F]: \").strip().upper()\n",
        "                    if user_choice_redundant == 'F':\n",
        "                        # Try to synthesize what was proposed by the LLM if available, or the current feedback\n",
        "                        # This assumes the LLM might still provide a \"Proposed statement\" even if redundant.\n",
        "                        # For now, we'll attempt to force storage of the current_feedback_to_process if the user insists.\n",
        "                        # This requires a direct call to finalize without conflict checks, or adjusting the prompt.\n",
        "                        # A simpler approach: treat it like a new piece of feedback but warn the LLM it was deemed redundant.\n",
        "                        print(\"KM: User chose to force store. Attempting to synthesize and store as a new learning.\")\n",
        "                        # Re-prompt, but indicate user override. This is complex.\n",
        "                        # Simplest for now: if user forces, we bypass strict redundancy check for this one attempt at synthesis\n",
        "                        # by using the \"FINALIZED_LEARNING\" path if possible, or just storing the current text.\n",
        "                        # This part needs careful thought on how to truly \"force\" if the LLM is designed to avoid redundancy.\n",
        "                        # For now, we will just try to process it as if it was new, and if it fails again, it fails.\n",
        "                        # A better \"Force\" would involve directly creating the learning entry.\n",
        "                        # Forcing it might mean we need a path to directly create a learning entry from current_feedback_to_process\n",
        "                        # For now, let's say \"Force\" means we try one more synthesis without the conflict loop if it fails again.\n",
        "                        # This is tricky. Let's assume \"Force\" means the user wants to try and make it work this one last time.\n",
        "                        # If it still comes up as redundant or conflict, it will exit.\n",
        "                        # The logic above already handles `final_statement` if the synthesis is successful.\n",
        "                        # The best way to handle \"Force\" is if the LLM provided a usable statement even when flagging redundancy.\n",
        "                        # The current prompt format `Proposed statement if you tried to rephrase: [rephrased statement]` helps here.\n",
        "\n",
        "                        # Attempt to extract a rephrased statement if provided with redundancy\n",
        "                        proposed_statement_match = re.search(r\"Proposed statement.*?:\\s*(.*)\", redundant_explanation, re.IGNORECASE)\n",
        "                        if proposed_statement_match and proposed_statement_match.group(1).strip():\n",
        "                            forced_statement = proposed_statement_match.group(1).strip()\n",
        "                            print(f\"KM: Using LLM's proposed statement due to Force: '{forced_statement}'\")\n",
        "                        else:\n",
        "                            forced_statement = current_feedback_to_process # Fallback to current feedback\n",
        "                            print(f\"KM: No specific proposed statement from LLM. Using current feedback for Force: '{forced_statement}'\")\n",
        "\n",
        "                        # Directly create the learning entry with this forced statement\n",
        "                        self.active_learnings_cache.append({\n",
        "                            \"learning_id\": str(uuid.uuid4()),\n",
        "                            \"timestamp_created\": datetime.now().isoformat(),\n",
        "                            \"original_human_input\": human_feedback_text,\n",
        "                            \"processed_human_input\": current_feedback_to_process, # The feedback that led to redundancy\n",
        "                            \"final_learning_statement\": forced_statement, # Using the forced/rephrased statement\n",
        "                            \"keywords\": self._extract_keywords(forced_statement + \" \" + current_feedback_to_process),\n",
        "                            \"status\": \"active_forced_redundancy\", # Special status\n",
        "                            \"learning_target\": learning_target,\n",
        "                            \"notes\": f\"Forced storage despite redundancy. Original LLM note: {redundant_explanation}\"\n",
        "                        })\n",
        "                        self.learnings_updated_this_session_flag = True\n",
        "                        print(f\"KM: Stored learning (forced despite redundancy). Cache size: {len(self.active_learnings_cache)}\")\n",
        "                        return\n",
        "                    else:\n",
        "                        print(\"KM: Skipping redundant learning.\")\n",
        "                        return\n",
        "\n",
        "                elif not_actionable_explanation:\n",
        "                    print(f\"KM: Learning deemed not actionable by Gemini: {not_actionable_explanation}\")\n",
        "                    print(\"KM: Skipping this learning.\")\n",
        "                    return\n",
        "\n",
        "                else: # Should not happen if one of the above conditions is met\n",
        "                    print(\"KM: Synthesis resulted in an unhandled state. Skipping.\")\n",
        "                    return\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"KM: Error during learning synthesis attempt {attempt_count}: {e}\")\n",
        "                import traceback; traceback.print_exc()\n",
        "                if attempt_count >= max_attempts:\n",
        "                    print(\"KM: Max attempts reached due to errors. Skipping this learning.\")\n",
        "                    return\n",
        "                # Optionally, wait before retrying on error\n",
        "                time.sleep(1)\n",
        "\n",
        "        # If loop finishes without returning (e.g. max attempts reached in conflict without resolution)\n",
        "        print(\"KM: Could not synthesize learning after maximum attempts. Skipping.\")"
      ],
      "metadata": {
        "id": "IuqeYwg80Ydj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WorkerAgentHandler:\n",
        "    def __init__(self, llm_client, tool_schemas: List[Dict], tool_executor: ToolExecutor, storage_instance: Storage):\n",
        "        self.llm_client = llm_client\n",
        "        self.tool_schemas = tool_schemas\n",
        "        self.tool_executor = tool_executor\n",
        "        self.storage = storage_instance\n",
        "        print(\"WorkerAgentHandler initialized.\")\n",
        "\n",
        "    def _execute_llm_interaction_loop(self, system_prompt: str, messages_for_api: List[Dict[str, Any]], query_type: str, conversation_manager: ConversationManager) -> Tuple[str, List[Dict]]:\n",
        "        tools_for_this_call = self.tool_schemas if query_type == \"OPERATIONAL\" else []\n",
        "        max_iterations = 5 if query_type == \"OPERATIONAL\" else 1 # Max tool use iterations for operational, 1 for others\n",
        "\n",
        "        executed_tool_calls_log: List[Dict] = [] # Log for tool calls in this interaction loop\n",
        "\n",
        "        for i in range(max_iterations):\n",
        "            print(f\"--- WorkerLLM Calling Anthropic (Iter {i+1}/{max_iterations}, QType: {query_type}) ---\")\n",
        "            current_text_response = \"\" # Initialize for this iteration\n",
        "            try:\n",
        "                response = self.llm_client.messages.create(\n",
        "                    model=ANTHROPIC_MODEL_NAME,\n",
        "                    max_tokens=4000,\n",
        "                    temperature=0.0,\n",
        "                    system=system_prompt,\n",
        "                    tools=tools_for_this_call,\n",
        "                    messages=messages_for_api\n",
        "                )\n",
        "            except Exception as e:\n",
        "                error_message = f\"Error communicating with Worker LLM: {e}\"\n",
        "                print(f\"LLM API Error: {e}\")\n",
        "                return error_message, executed_tool_calls_log # Return error and any logs so far\n",
        "\n",
        "            assistant_response_blocks = response.content\n",
        "            # It's important to add the raw assistant response blocks to the API history\n",
        "            # This includes text parts and tool_use parts if any.\n",
        "            messages_for_api.append({\"role\": \"assistant\", \"content\": assistant_response_blocks})\n",
        "\n",
        "            text_blocks = [block.text for block in assistant_response_blocks if block.type == \"text\"]\n",
        "            current_text_response = \" \".join(text_blocks).strip()\n",
        "\n",
        "            if current_text_response.startswith(\"CLARIFICATION_REQUESTED:\"):\n",
        "                return current_text_response, executed_tool_calls_log # Return immediately for clarification\n",
        "\n",
        "            tool_calls_to_process = [block for block in assistant_response_blocks if block.type == \"tool_use\"]\n",
        "\n",
        "            if not tool_calls_to_process or query_type != \"OPERATIONAL\":\n",
        "                # If no tools to call, or not an operational query, this is the final response from the LLM for this loop.\n",
        "                final_response_text = current_text_response if current_text_response else \"Worker AI provided no text content in its final turn.\"\n",
        "                return final_response_text, executed_tool_calls_log\n",
        "\n",
        "            # If there are tool calls to process (and it's an OPERATIONAL query)\n",
        "            tool_results_for_next_llm_call_content = [] # This will be the content for the next \"user\" role message (tool results)\n",
        "\n",
        "            for tool_use_block in tool_calls_to_process:\n",
        "                tool_name, tool_input, tool_use_id = tool_use_block.name, tool_use_block.input, tool_use_block.id\n",
        "                print(f\"WorkerLLM: Requesting Tool Call: {tool_name}, Input: {tool_input}\")\n",
        "\n",
        "                # Execute the tool\n",
        "                tool_result_data = self.tool_executor.execute_tool(tool_name, tool_input, self.storage)\n",
        "\n",
        "                # Log the tool call and its result for the orchestrator/evaluator\n",
        "                executed_tool_calls_log.append({\n",
        "                    \"tool_name\": tool_name,\n",
        "                    \"tool_input\": copy.deepcopy(tool_input), # Deepcopy to avoid modification issues\n",
        "                    \"tool_output\": copy.deepcopy(tool_result_data)\n",
        "                })\n",
        "\n",
        "                # Update conversation manager's context (this was already here)\n",
        "                # Example: update context based on product/order/customer IDs in tool_result_data\n",
        "                entity_type_map = {\n",
        "                    \"order_details\": \"orders\", \"order_id\": \"orders\",\n",
        "                    \"product\": \"products\", \"product_id\": \"products\",\n",
        "                    \"customer\": \"customers\", \"customer_id\": \"customers\"\n",
        "                }\n",
        "                found_entity_type = \"unknown\"\n",
        "                found_entity_id = \"unknown_id\"\n",
        "                found_entity_data = tool_result_data\n",
        "\n",
        "                for key, etype in entity_type_map.items():\n",
        "                    if key in tool_result_data and tool_result_data[key]:\n",
        "                        found_entity_type = etype\n",
        "                        if isinstance(tool_result_data[key], dict) and (\"id\" in tool_result_data[key] or etype[:-1]+\"_id\" in tool_result_data[key]): # e.g. order_details might have 'id'\n",
        "                             found_entity_id = tool_result_data[key].get(\"id\") or tool_result_data[key].get(etype[:-1]+\"_id\")\n",
        "                             found_entity_data = tool_result_data[key]\n",
        "                        elif isinstance(tool_result_data.get(etype[:-1]+\"_id\"), str): # e.g. direct product_id\n",
        "                            found_entity_id = tool_result_data.get(etype[:-1]+\"_id\")\n",
        "                        break # Take first match for simplicity\n",
        "\n",
        "                # Try to get ID more robustly if it's directly in tool_result_data\n",
        "                if found_entity_id == \"unknown_id\":\n",
        "                     if \"order_id\" in tool_result_data: found_entity_id = tool_result_data[\"order_id\"]\n",
        "                     elif \"product_id\" in tool_result_data: found_entity_id = tool_result_data[\"product_id\"]\n",
        "                     elif \"customer_id\" in tool_result_data: found_entity_id = tool_result_data[\"customer_id\"]\n",
        "\n",
        "                if found_entity_id != \"unknown_id\":\n",
        "                    conversation_manager.update_entity_in_context(\n",
        "                        entity_type=found_entity_type,\n",
        "                        entity_id=found_entity_id,\n",
        "                        data=found_entity_data\n",
        "                    )\n",
        "                conversation_manager.set_last_action(f\"tool_{tool_name}_Anthropic\", {\"input\": tool_input, \"result_summary\": tool_result_data.get(\"status\", \"unknown_status\")})\n",
        "\n",
        "                tool_results_for_next_llm_call_content.append({\n",
        "                    \"type\": \"tool_result\",\n",
        "                    \"tool_use_id\": tool_use_id,\n",
        "                    \"content\": json.dumps(tool_result_data) if isinstance(tool_result_data, dict) else str(tool_result_data)\n",
        "                    # Consider adding an error field from tool_result_data if status is error\n",
        "                    # \"is_error\": tool_result_data.get(\"status\") == \"error\" if isinstance(tool_result_data, dict) else False\n",
        "                })\n",
        "\n",
        "            # Add the aggregated tool results as a new \"user\" message for the next LLM call\n",
        "            if tool_results_for_next_llm_call_content:\n",
        "                messages_for_api.append({\"role\": \"user\", \"content\": tool_results_for_next_llm_call_content})\n",
        "            else: # Should not happen if tool_calls_to_process was non-empty\n",
        "                print(\"WorkerLLM: No tool results to append, though tool calls were present. This is unexpected.\")\n",
        "\n",
        "\n",
        "        # If loop finishes (max_iterations reached)\n",
        "        final_response_text = current_text_response if current_text_response else \"Worker AI reached max tool iterations without a final text response.\"\n",
        "        return final_response_text, executed_tool_calls_log"
      ],
      "metadata": {
        "id": "XYs7E68G0qBP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResponseEvaluator:\n",
        "    def __init__(self, evaluator_llm_instance):\n",
        "        self.evaluator_llm = evaluator_llm_instance # Gemini model\n",
        "        print(\"ResponseEvaluator initialized.\")\n",
        "\n",
        "    def evaluate_turn(self, user_message: str, query_type: str, worker_response_text: str,\n",
        "                      context_summary: str, rag_learnings_provided: str,\n",
        "                      clarification_interactions: Optional[List[Dict]],\n",
        "                      initial_datastore_state: Dict[str, Any],\n",
        "                      final_datastore_state: Dict[str, Any],\n",
        "                      executed_tool_calls_log: List[Dict]) -> Dict[str, Any]: # Added tool calls log\n",
        "\n",
        "        initial_ds_prompt = f\"Data Store State *Before* AI Action:\\\\n{json.dumps(initial_datastore_state, indent=2, default=str)}\"\n",
        "        final_ds_prompt = f\"Data Store State *After* AI Action:\\\\n{json.dumps(final_datastore_state, indent=2, default=str)}\"\n",
        "\n",
        "        clarification_info_prompt = \"No worker AI clarifications this turn.\"\n",
        "        if clarification_interactions:\n",
        "            clar_summary = [f\"  Q: '{c.get('agent_question', 'N/A')}' -> User A: '{c.get('user_answer', 'N/A')}'\" for c in clarification_interactions]\n",
        "            clarification_info_prompt = f\"Worker AI Clarification Interactions:\\\\n\" + \"\\\\n\".join(clar_summary)\n",
        "\n",
        "        tool_log_prompt = \"No tools were executed by the Worker AI this turn.\"\n",
        "        if executed_tool_calls_log:\n",
        "            formatted_tool_calls = []\n",
        "            for i, call in enumerate(executed_tool_calls_log):\n",
        "                # Limit the size of tool output in the prompt to avoid excessive length\n",
        "                output_summary = call.get('tool_output', {})\n",
        "                if isinstance(output_summary, dict):\n",
        "                    output_summary_str = json.dumps({k: v for k, v in output_summary.items() if k != \"products\"}, indent=1, default=str) # Exclude full product lists from summary\n",
        "                    if len(output_summary_str) > 300: # Truncate if still too long\n",
        "                        output_summary_str = output_summary_str[:300] + \"...\"\n",
        "                else:\n",
        "                    output_summary_str = str(output_summary)\n",
        "                    if len(output_summary_str) > 300:\n",
        "                         output_summary_str = output_summary_str[:300] + \"...\"\n",
        "\n",
        "                formatted_tool_calls.append(\n",
        "                    f\"  Tool Call {i+1}:\\n\"\n",
        "                    f\"    Name: {call.get('tool_name')}\\n\"\n",
        "                    f\"    Input: {json.dumps(call.get('tool_input'), default=str)}\\n\"\n",
        "                    f\"    Output Summary: {output_summary_str}\"\n",
        "                )\n",
        "            tool_log_prompt = f\"Worker AI Tool Calls Executed This Turn:\\\\n\" + \"\\\\n\".join(formatted_tool_calls)\n",
        "\n",
        "        eval_content_prompt = f\"\"\"\n",
        "User query: {user_message}\n",
        "Classified Query Type: {query_type}\n",
        "\n",
        "Context provided to assistant (summary):\n",
        "{context_summary}\n",
        "\n",
        "Relevant RAG Learnings provided to assistant:\n",
        "{rag_learnings_provided}\n",
        "\n",
        "{initial_ds_prompt}\n",
        "\n",
        "{tool_log_prompt}\n",
        "\n",
        "{final_ds_prompt}\n",
        "\n",
        "{clarification_info_prompt}\n",
        "\n",
        "Worker AI (Claude) final textual response:\n",
        "{worker_response_text}\n",
        "---\n",
        "INSTRUCTIONS FOR EVALUATOR (You are Gemini {EVAL_MODEL_NAME}):\n",
        "Based on your system prompt (which emphasizes impartiality and detailed assessment criteria) and the classified query type ({query_type}), please evaluate the AI assistant's response.\n",
        "- If OPERATIONAL, focus on tool use accuracy (refer to 'Worker AI Tool Calls Executed'), data store changes (Before vs. After), and whether the final textual response aligns with these actions.\n",
        "- If METACOGNITIVE_LEARNINGS_SUMMARY, focus on whether the AI accurately summarized the 'Relevant RAG Learnings' it was provided.\n",
        "Provide detailed reasoning for scores (Accuracy, Efficiency, Context Awareness, Helpfulness) and an overall score (1-10).\n",
        "Explicitly reference the tool call log and datastore states when assessing operational tasks.\n",
        "\"\"\"\n",
        "        try:\n",
        "            # Assuming self.evaluator_llm is a GenerativeModel instance configured with the evaluator_system_prompt\n",
        "            gemini_response_obj = self.evaluator_llm.generate_content(eval_content_prompt)\n",
        "            evaluation_text = gemini_response_obj.text\n",
        "\n",
        "            score = self._extract_score(evaluation_text) # Overall score\n",
        "            # You might want to extract individual criteria scores too if needed later.\n",
        "\n",
        "            return {\n",
        "                \"anthropic_score\": score, # This is the overall score for the worker AI\n",
        "                \"full_evaluation\": evaluation_text, # This is Gemini's full textual evaluation\n",
        "                \"clarification_details_evaluator\": {\"used\": False}, # Placeholder, as evaluator doesn't ask for clarification in this setup\n",
        "                \"query_type_evaluated\": query_type,\n",
        "                \"raw_evaluation_text\": evaluation_text # For direct printing\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Evaluator: Error during Gemini evaluation: {e}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"anthropic_score\": 0,\n",
        "                \"full_evaluation\": f\"Evaluation failed: {e}\",\n",
        "                \"clarification_details_evaluator\": {},\n",
        "                \"query_type_evaluated\": query_type,\n",
        "                \"raw_evaluation_text\": f\"Evaluation Error: {e}\"\n",
        "            }\n",
        "\n",
        "    def _extract_score(self, evaluation_text: str) -> int:\n",
        "        # More robust score extraction, looking for \"Overall Score: X/10\" or \"Overall Score: X\"\n",
        "        # Prefers scores out of 10 if specified.\n",
        "        patterns = [\n",
        "            r\"Overall Score\\s*:\\s*(\\d{1,2})\\s*/\\s*10\",  # \"Overall Score : X/10\" or \"Overall Score:X/10\"\n",
        "            r\"Overall Score\\s*:\\s*(\\d{1,2})\",         # \"Overall Score : X\" or \"Overall Score:X\"\n",
        "            r\"Overall\\s*:\\s*(\\d{1,2})\\s*/\\s*10\",       # \"Overall : X/10\"\n",
        "            r\"Overall\\s*:\\s*(\\d{1,2})\"                # \"Overall : X\"\n",
        "        ]\n",
        "        for p_str in patterns:\n",
        "            matches = list(re.finditer(p_str, evaluation_text, re.IGNORECASE | re.DOTALL))\n",
        "            if matches:\n",
        "                try:\n",
        "                    score_value = int(matches[-1].group(1))\n",
        "                    if 0 <= score_value <= 10: # Ensure score is within expected range\n",
        "                        return score_value\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "        print(f\"Evaluator: Could not reliably extract a 0-10 score from text: ...{evaluation_text[-300:]}\")\n",
        "        return 0 # Default score if not found or invalid"
      ],
      "metadata": {
        "id": "ZFpBL_-HSGPY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentOrchestrator:\n",
        "    def __init__(self):\n",
        "        self.classifier_llm_client = gemini.GenerativeModel(model_name=CLASSIFIER_MODEL_NAME)\n",
        "        self.query_classifier = QueryClassifier(llm_client=self.classifier_llm_client)\n",
        "        self.storage = Storage()\n",
        "        self.conversation_manager = ConversationManager()\n",
        "        self.tool_functions_map = {\n",
        "            \"create_customer\": create_customer, \"get_customer_info\": get_customer_info,\n",
        "            \"create_product\": create_product, \"update_product\": update_product,\n",
        "            \"get_product_info\": get_product_info, \"list_all_products\": list_all_products,\n",
        "            \"create_order\": create_order, \"get_order_details\": get_order_details,\n",
        "            \"update_order_status\": update_order_status,\n",
        "        }\n",
        "        self.tool_executor = ToolExecutor(self.tool_functions_map)\n",
        "\n",
        "        knowledge_synthesis_llm = gemini.GenerativeModel(model_name=EVAL_MODEL_NAME)\n",
        "        self.knowledge_manager = KnowledgeManager(LEARNINGS_DRIVE_BASE_PATH, DRIVE_MOUNT_PATH, DEFAULT_LEARNINGS_DRIVE_SUBPATH, knowledge_synthesis_llm)\n",
        "\n",
        "        self.worker_agent_handler = WorkerAgentHandler(anthropic_client, tools_schemas_list, self.tool_executor, self.storage)\n",
        "\n",
        "        main_evaluator_llm = gemini.GenerativeModel(model_name=EVAL_MODEL_NAME, system_instruction=evaluator_system_prompt)\n",
        "        self.response_evaluator = ResponseEvaluator(evaluator_llm_instance=main_evaluator_llm)\n",
        "\n",
        "        self.evaluation_results_log: List[Dict] = []\n",
        "        print(\"AgentOrchestrator initialized.\")\n",
        "\n",
        "    def _handle_worker_clarification_interaction(self, agent_question_text: str, system_prompt: str,\n",
        "                                                current_turn_history: List[Dict], query_type: str,\n",
        "                                                conversation_manager: ConversationManager,\n",
        "                                                max_attempts: int = 2) -> Tuple[str, List[Dict], List[Dict]]:\n",
        "        clarification_interactions = []\n",
        "        response_text = agent_question_text\n",
        "        executed_tool_calls_log_clarification_phase: List[Dict] = []\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            actual_question = response_text.split(\"CLARIFICATION_REQUESTED:\", 1)[-1].strip() if \"CLARIFICATION_REQUESTED:\" in response_text else response_text\n",
        "            print(f\"--- Worker AI requests clarification: {actual_question} ---\")\n",
        "\n",
        "            user_clarification = \"\"\n",
        "            try:\n",
        "                user_clarification = input(f\"Your response to Worker AI: \").strip()\n",
        "            except EOFError:\n",
        "                print(\"EOFError encountered during input. Assuming no user clarification.\")\n",
        "                user_clarification = \"(User provided no further input due to EOF)\"\n",
        "\n",
        "            if not user_clarification and not user_clarification.startswith(\"(User provided no further input\"):\n",
        "                 user_clarification = \"(User provided no further input)\"\n",
        "\n",
        "            clarification_interactions.append({\"agent_question\": actual_question, \"user_answer\": user_clarification})\n",
        "            current_turn_history.append({\"role\": \"user\", \"content\": user_clarification})\n",
        "\n",
        "            response_text, tools_log_this_iteration = self.worker_agent_handler._execute_llm_interaction_loop(\n",
        "                system_prompt, current_turn_history, query_type, conversation_manager\n",
        "            )\n",
        "            executed_tool_calls_log_clarification_phase.extend(tools_log_this_iteration)\n",
        "\n",
        "            if not response_text.startswith(\"CLARIFICATION_REQUESTED:\"):\n",
        "                return response_text, clarification_interactions, executed_tool_calls_log_clarification_phase\n",
        "\n",
        "        print(\"Max clarification attempts reached for worker AI.\")\n",
        "        return response_text, clarification_interactions, executed_tool_calls_log_clarification_phase\n",
        "\n",
        "    def process_user_request(self, user_message: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Orchestrates the worker AI's processing of a user request and the AI evaluation.\n",
        "        This method DOES NOT handle human feedback prompts; that's done in the main loop.\n",
        "        \"\"\"\n",
        "        self.conversation_manager.add_user_message(user_message)\n",
        "        query_type = self.query_classifier.classify(user_message)\n",
        "        print(f\"--- Orchestrator: Classified Query Type: {query_type} ---\")\n",
        "\n",
        "        worker_system_prompt_template = worker_metacognitive_learnings_system_prompt if query_type == \"METACOGNITIVE_LEARNINGS_SUMMARY\" else worker_operational_system_prompt\n",
        "\n",
        "        context_summary_for_worker = self.conversation_manager.get_context_summary() # Needed for feedback later\n",
        "        rag_learnings_for_worker = self.knowledge_manager.get_relevant_learnings_for_prompt(user_message, query_type, learning_target_filter=\"Agent\")\n",
        "\n",
        "        full_worker_prompt = (\n",
        "            f\"{worker_system_prompt_template}\\\\n\\\\n\"\n",
        "            f\"Conversation Context Summary (recent entities and last action):\\\\n{context_summary_for_worker}\\\\n\\\\n\"\n",
        "            f\"{rag_learnings_for_worker}\"\n",
        "        )\n",
        "\n",
        "        initial_datastore_state = self.storage.get_full_datastore_copy()\n",
        "        current_turn_processing_history = self.conversation_manager.get_full_conversation_for_api()\n",
        "\n",
        "        worker_response_text, executed_tool_calls_log = self.worker_agent_handler._execute_llm_interaction_loop(\n",
        "            full_worker_prompt, current_turn_processing_history, query_type, self.conversation_manager\n",
        "        )\n",
        "\n",
        "        clarification_interactions = []\n",
        "        if worker_response_text.startswith(\"CLARIFICATION_REQUESTED:\"):\n",
        "            worker_response_text, clarification_interactions, tools_during_clarif = self._handle_worker_clarification_interaction(\n",
        "                worker_response_text, full_worker_prompt, current_turn_processing_history, query_type, self.conversation_manager\n",
        "            )\n",
        "            executed_tool_calls_log.extend(tools_during_clarif)\n",
        "\n",
        "        self.conversation_manager.add_assistant_message(worker_response_text, query_type)\n",
        "\n",
        "        # Perform evaluation\n",
        "        final_datastore_state = self.storage.get_full_datastore_copy()\n",
        "        rag_learnings_for_evaluator = rag_learnings_for_worker # Using same RAG for simplicity\n",
        "\n",
        "        evaluation_result = self.response_evaluator.evaluate_turn(\n",
        "            user_message, query_type, worker_response_text,\n",
        "            context_summary_for_worker, # Context summary provided to worker\n",
        "            rag_learnings_for_evaluator, # RAG provided for evaluation context\n",
        "            clarification_interactions,\n",
        "            initial_datastore_state,\n",
        "            final_datastore_state,\n",
        "            executed_tool_calls_log\n",
        "        )\n",
        "\n",
        "        # Log the core turn data (worker response + evaluation)\n",
        "        self.evaluation_results_log.append({\n",
        "            \"user_message\": user_message,\n",
        "            \"query_type\": query_type,\n",
        "            \"worker_response\": worker_response_text,\n",
        "            \"tool_calls\": copy.deepcopy(executed_tool_calls_log),\n",
        "            \"evaluation_details\": evaluation_result\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"user_message\": user_message,\n",
        "            \"query_type\": query_type,\n",
        "            \"anthropic_response\": worker_response_text,\n",
        "            \"executed_tool_calls\": executed_tool_calls_log,\n",
        "            \"evaluation_details\": evaluation_result,\n",
        "            \"context_summary_for_worker\": context_summary_for_worker # Pass this out for feedback processing\n",
        "        }\n",
        "\n",
        "    def handle_feedback_on_worker_response(self, original_user_query: str,\n",
        "                                           context_summary_for_worker: str,\n",
        "                                           human_feedback_on_worker: str):\n",
        "        \"\"\"Handles processing and storing feedback related to the worker AI's response.\"\"\"\n",
        "        if human_feedback_on_worker.lower() not in ['skip', ''] and human_feedback_on_worker:\n",
        "            self.knowledge_manager.synthesize_and_store_learning(\n",
        "                human_feedback_on_worker,\n",
        "                original_user_query,\n",
        "                context_summary_for_worker, # Context at the time of worker's action\n",
        "                learning_target=\"AgentResponse\"\n",
        "            )\n",
        "            if self.knowledge_manager.learnings_updated_this_session_flag:\n",
        "                self.knowledge_manager.persist_active_learnings()\n",
        "        else:\n",
        "            print(\"Orchestrator: No feedback provided for worker response or 'skip' entered.\")\n",
        "\n",
        "\n",
        "    def handle_feedback_on_evaluation(self, original_user_query: str, worker_response_summary: str,\n",
        "                                      evaluation_text_summary: str, human_feedback_on_evaluator: str):\n",
        "        \"\"\"Handles processing and storing feedback related to an evaluator's assessment.\"\"\"\n",
        "        if human_feedback_on_evaluator.lower() not in ['skip', ''] and human_feedback_on_evaluator:\n",
        "            eval_feedback_context = (\n",
        "                f\"Feedback is on an evaluation. Original query: '{original_user_query}'. \"\n",
        "                f\"Worker response (summary): '{worker_response_summary[:100]}...'. \"\n",
        "                f\"Evaluation (summary): '{evaluation_text_summary[:150]}...'\"\n",
        "            )\n",
        "            self.knowledge_manager.synthesize_and_store_learning(\n",
        "                human_feedback_on_evaluator,\n",
        "                original_user_query,\n",
        "                eval_feedback_context,\n",
        "                learning_target=\"EvaluatorAssessment\"\n",
        "            )\n",
        "            if self.knowledge_manager.learnings_updated_this_session_flag:\n",
        "                self.knowledge_manager.persist_active_learnings()\n",
        "        else:\n",
        "            print(\"Orchestrator: No feedback provided for evaluator or 'skip' entered.\")\n",
        "\n",
        "    def get_evaluation_log(self) -> List[Dict]:\n",
        "        return self.evaluation_results_log\n",
        "\n",
        "    def persist_learnings_on_exit(self):\n",
        "        if self.knowledge_manager.learnings_updated_this_session_flag:\n",
        "            print(\"Orchestrator: Persisting any remaining updated learnings on exit...\")\n",
        "            self.knowledge_manager.persist_active_learnings()"
      ],
      "metadata": {
        "id": "UzLHm8MmSOMC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution Function ---\n",
        "def main():\n",
        "    print(\"\\\\nStarting Main Execution with REFACTORED Agent System...\\\\n\")\n",
        "    orchestrator = AgentOrchestrator()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            print(\"\\\\n\" + \"=\" * 70) # Separator for turns\n",
        "            user_query = input(\"Enter query (or 'quit' to exit): \").strip()\n",
        "\n",
        "            if user_query.lower() == 'quit':\n",
        "                orchestrator.persist_learnings_on_exit()\n",
        "                print(\"Exiting system. Learnings persisted if updated.\")\n",
        "                break\n",
        "            if not user_query:\n",
        "                continue\n",
        "\n",
        "            print(f\">>> User Query: '{user_query}'\")\n",
        "\n",
        "            # 1. Process user request (gets worker response and evaluation, NO feedback prompts inside)\n",
        "            turn_result = orchestrator.process_user_request(user_query)\n",
        "\n",
        "            # 2. Extract necessary data from turn_result\n",
        "            worker_response_text = turn_result.get('anthropic_response', \"No worker response found.\")\n",
        "            query_type_from_turn = turn_result.get('query_type', \"N/A\")\n",
        "            context_for_worker_feedback = turn_result.get('context_summary_for_worker', \"N/A\") # Get context for feedback\n",
        "            evaluation_details = turn_result.get(\"evaluation_details\", {})\n",
        "\n",
        "            # 3. Print Worker AI's Response\n",
        "            print(f\"\\\\n--- Worker AI Final Response (Type: {query_type_from_turn}) ---\")\n",
        "            print(worker_response_text)\n",
        "            print(\"--- End of Worker AI Response ---\")\n",
        "\n",
        "            # 4. Capture feedback on Worker AI's response\n",
        "            try:\n",
        "                human_feedback_on_worker = input(\"Orchestrator: Feedback on Worker AI's response? (type or 'skip'): \").strip()\n",
        "                orchestrator.handle_feedback_on_worker_response(\n",
        "                    original_user_query=user_query,\n",
        "                    context_summary_for_worker=context_for_worker_feedback,\n",
        "                    human_feedback_on_worker=human_feedback_on_worker\n",
        "                )\n",
        "            except EOFError:\n",
        "                print(\"Orchestrator: Skipping feedback on worker response (EOF).\")\n",
        "                pass\n",
        "\n",
        "            # 5. Print Evaluator's Assessment\n",
        "            raw_eval_text_for_feedback = \"No evaluation text.\"\n",
        "            if evaluation_details and not evaluation_details.get(\"error\"):\n",
        "                raw_eval_text_for_feedback = evaluation_details.get(\"raw_evaluation_text\", \"No raw evaluation text found.\")\n",
        "                print(f\"\\\\n--- Evaluator (Gemini) Assessment (for query: '{user_query}') ---\")\n",
        "                print(raw_eval_text_for_feedback)\n",
        "                print(\"--- End of Evaluation ---\")\n",
        "            elif evaluation_details and evaluation_details.get(\"error\"):\n",
        "                error_message = evaluation_details.get('error', 'Unknown evaluation error.')\n",
        "                raw_eval_text_for_feedback = f\"Evaluation Error: {error_message}\"\n",
        "                print(f\"\\\\n--- Evaluator (Gemini) Assessment Error ---\")\n",
        "                print(raw_eval_text_for_feedback)\n",
        "                print(\"--- End of Evaluation Error ---\")\n",
        "            else:\n",
        "                print(\"\\\\n--- Evaluator: No evaluation details found for this turn. ---\")\n",
        "\n",
        "            # 6. Capture feedback on Evaluator's assessment\n",
        "            try:\n",
        "                human_feedback_on_evaluator = input(\"Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): \").strip()\n",
        "                orchestrator.handle_feedback_on_evaluation(\n",
        "                    original_user_query=user_query,\n",
        "                    worker_response_summary=worker_response_text,\n",
        "                    evaluation_text_summary=raw_eval_text_for_feedback,\n",
        "                    human_feedback_on_evaluator=human_feedback_on_evaluator\n",
        "                )\n",
        "            except EOFError:\n",
        "                print(\"Orchestrator: Skipping feedback on evaluator assessment (EOF).\")\n",
        "                pass\n",
        "\n",
        "        except SystemExit:\n",
        "            print(\"System exit requested.\")\n",
        "            orchestrator.persist_learnings_on_exit()\n",
        "            break\n",
        "        except EOFError:\n",
        "            print(\"\\\\nEOF encountered. Exiting gracefully.\")\n",
        "            orchestrator.persist_learnings_on_exit()\n",
        "            break\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\\\nKeyboard interrupt detected. Exiting.\")\n",
        "            orchestrator.persist_learnings_on_exit()\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"CRITICAL ERROR in main loop: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # --- Final Evaluation Summary ---\n",
        "    print(\"\\\\n\" + \"=\" * 30 + \" FINAL EVALUATION SUMMARY \" + \"=\" * 30)\n",
        "    results_log = orchestrator.get_evaluation_log()\n",
        "    total_score, num_q_evaluated = 0, 0\n",
        "\n",
        "    if not results_log:\n",
        "        print(\"No queries were processed and evaluated in this session.\")\n",
        "    else:\n",
        "        for i, turn_data in enumerate(results_log):\n",
        "            user_msg = turn_data.get('user_message', 'N/A')\n",
        "            q_type = turn_data.get('query_type', 'N/A')\n",
        "            eval_details_from_log = turn_data.get('evaluation_details', {})\n",
        "\n",
        "            if isinstance(eval_details_from_log, dict) and not eval_details_from_log.get(\"error\"):\n",
        "                score = eval_details_from_log.get('anthropic_score', 0)\n",
        "                total_score += score\n",
        "                num_q_evaluated += 1\n",
        "                print(f\"Q{i+1}: '{user_msg}' (Type: {q_type}) -> Score: {score}\")\n",
        "            elif isinstance(eval_details_from_log, dict) and eval_details_from_log.get(\"error\"):\n",
        "                print(f\"Q{i+1}: '{user_msg}' (Type: {q_type}) -> Evaluation Error: {eval_details_from_log.get('error')}\")\n",
        "            else:\n",
        "                print(f\"Q{i+1}: '{user_msg}' (Type: {q_type}) -> No valid evaluation details logged.\")\n",
        "\n",
        "    if num_q_evaluated > 0:\n",
        "        print(f\"\\\\nAverage Score over {num_q_evaluated} evaluated queries: {total_score / num_q_evaluated:.2f}\")\n",
        "    else:\n",
        "        print(\"\\\\nNo queries were successfully evaluated to calculate an average score.\")\n",
        "    print(f\"Total aggregate score for the session: {total_score}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Execution Finished.\")\n",
        "\n",
        "# To run:\n",
        "# main()"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "id": "O2-ztJ2BO7gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Sample queries:\n",
        "* Show me all the products available\n",
        "* I'd like to order 25 Perplexinators, please\n",
        "* Show me the status of my order\n",
        "* (If the order is not in Shipped state, then) Please ship my order now\n",
        "* How many Perplexinators are now left in stock?\n",
        "* Add a new customer: Bill Leece, bill.leece@mail.com, +1.222.333.4444\n",
        "* Add new new product: Gizmo X, description: A fancy gizmo, price: 29.99, inventory: 50\n",
        "* Update Gizzmo's price to 99.99 #Note the misspelling of 'Gizmo'\n",
        "* When was the last time the Toronto Maple Leafs won the Stanley Cup?\n",
        "* I need to update our insurance policy, so I need to know the total value of all the products in our inventory. Please tell me this amount.\n",
        "* Summarize your learnings from our recent interactions.\n",
        "\"\"\"\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dg85ek6bSvW5",
        "outputId": "e1ec6cbf-4ba6-4c20-c8bf-87ee96d615c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\nStarting Main Execution with REFACTORED Agent System...\\n\n",
            "QueryClassifier initialized with LLM client.\n",
            "Storage initialized with deep copies of initial data.\n",
            "ConversationManager initialized.\n",
            "ToolExecutor initialized.\n",
            "KM: Loading initial learnings from: /content/drive/My Drive/AI/Knowledgebases/learnings_20250520_172058_767112.json\n",
            "KnowledgeManager initialized. Loaded 1 initial learnings from /content/drive/My Drive/AI/Knowledgebases.\n",
            "WorkerAgentHandler initialized.\n",
            "ResponseEvaluator initialized.\n",
            "AgentOrchestrator initialized.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): Show me all the products available\n",
            ">>> User Query: 'Show me all the products available'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "WorkerLLM: Requesting Tool Call: list_all_products, Input: {}\n",
            "[Tool Exec] list_all_products: Found 3 products.\n",
            "--- [ToolExecutor] Result for list_all_products: {\n",
            "  \"status\": \"success\",\n",
            "  \"count\": 3,\n",
            "  \"products\": {\n",
            "    \"P1\": {\n",
            "      \"name\": \"Widget A\",\n",
            "      \"description\": \"A simple widget. Very compact.\",\n",
            "      \"price\": 19.99,\n",
            "      \"inventory_count\": 999\n",
            "    },\n",
            "    \"P2\": {\n",
            "      \"name\": \"Gadget B\",\n",
            "      \"description\": \"A powerful gadget. It spins.\",\n",
            "      \"price\": 49.99,\n",
            "      \"inventory_count\": 200\n",
            "    },\n",
            "    \"P3\": {\n",
            "      \"name\": \"Perplexinator\",\n",
            "      \"description\": \"A perplexing perfunctator\",\n",
            "      \"price\": 79.99,\n",
            "      \"inventory_count\": 1483\n",
            "    }\n",
            "  }\n",
            "} ---\n",
            "[CM_Context Updated] Last Action: tool_list_all_products_Anthropic\n",
            "--- WorkerLLM Calling Anthropic (Iter 2/5, QType: OPERATIONAL) ---\n",
            "Evaluator: Could not reliably extract a 0-10 score from text: ...nt details (name, price, inventory, description) as sourced from the datastore. The response was efficient, contextually appropriate (even subtly incorporating the RAG learning about stock levels), helpful, and very clear. No datastore changes were made, which is correct for a product listing query.\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "Let me show you what we currently have in stock:\n",
            "\n",
            "1. Widget A - $19.99 (999 units available)\n",
            "   A simple widget that's very compact.\n",
            "\n",
            "2. Gadget B - $49.99 (200 units available)\n",
            "   A powerful gadget that spins.\n",
            "\n",
            "3. Perplexinator - $79.99 (1,483 units available)\n",
            "   A perplexing perfunctator\n",
            "\n",
            "All these items are currently in stock. Would you like more detailed information about any specific product?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for worker response or 'skip' entered.\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'Show me all the products available') ---\n",
            "**1. Accuracy**:\n",
            "    *   **OPERATIONAL**:\n",
            "        *   The AI's **user-facing textual response** is highly correct and factual. It accurately lists the three products: \"Widget A\", \"Gadget B\", and \"Perplexinator\", along with their correct prices, inventory counts, and descriptions as found in the `Data Store State *Before* AI Action`.\n",
            "        *   The textual response accurately reflects the outcome of the **tool call**. The `list_all_products` tool call summary indicated `\"count\": 3`, and the response lists exactly three products. It's assumed the full tool output (not just the summary) provided the necessary details for each product, which the AI then relayed.\n",
            "        *   The `list_all_products` tool call correctly processed the request (no input needed to list all). The datastore remained unchanged (`Data Store State *After* AI Action` is identical to *Before*), which is correct for a read-only operation like listing products.\n",
            "\n",
            "**2. Efficiency**:\n",
            "    *   The assistant achieved its goal without asking any clarifying questions, which were not needed for this straightforward query.\n",
            "    *   **OPERATIONAL**: The `list_all_products` tool was used appropriately and efficiently. A single tool call was sufficient to retrieve the necessary information.\n",
            "\n",
            "**3. Context Awareness**:\n",
            "    *   The assistant correctly understood the user's query. No complex conversation history or entities were present or needed for this initial query.\n",
            "    *   The assistant adhered to the task defined by the OPERATIONAL query type by providing a list of products.\n",
            "    *   The \"Relevant Learnings from Knowledge Base\" (ID: b32304bb) concerned stock availability for orders. While the primary query was to list products, the AI's response, \"All these items are currently in stock\" and the inclusion of inventory counts (e.g., \"999 units available\"), shows an appropriate awareness and integration of this learning by proactively addressing stock status, which is a related concern.\n",
            "    *   The user-facing response makes perfect sense given the tool output (a count of 3 products, implying a list was retrieved) and the datastore (which contains the details of these 3 products).\n",
            "\n",
            "**4. Helpfulness & Clarity (of the user-facing response)**:\n",
            "    *   The assistant fully addressed the user's need to \"Show me all the products available\" by providing a comprehensive list.\n",
            "    *   The **user-facing response** is very clear, polite (\"Let me show you...\"), and easy to understand. It uses a numbered list for readability and presents key information (name, price, inventory, description) concisely for each product.\n",
            "    *   It avoids jargon and does not dump raw data.\n",
            "    *   The closing question, \"Would you like more detailed information about any specific product?\" is helpful and encourages continued interaction.\n",
            "\n",
            "**Scores**:\n",
            "*   **Accuracy**: 10/10\n",
            "*   **Efficiency**: 10/10\n",
            "*   **Context Awareness**: 10/10\n",
            "*   **Helpfulness & Clarity**: 10/10\n",
            "*   **Overall Score**: 10/10\n",
            "\n",
            "**Reasoning Summary**:\n",
            "The AI assistant performed exceptionally well on this OPERATIONAL query. It correctly used the `list_all_products` tool, and its textual response accurately and clearly presented all available products with their relevant details (name, price, inventory, description) as sourced from the datastore. The response was efficient, contextually appropriate (even subtly incorporating the RAG learning about stock levels), helpful, and very clear. No datastore changes were made, which is correct for a product listing query.\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): skip\n",
            "Orchestrator: No feedback provided for evaluator or 'skip' entered.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): I'd like to order 25 Perplexinators, please\n",
            ">>> User Query: 'I'd like to order 25 Perplexinators, please'\n",
            "--- Orchestrator: Classified Query Type: OPERATIONAL ---\n",
            "--- WorkerLLM Calling Anthropic (Iter 1/5, QType: OPERATIONAL) ---\n",
            "WorkerLLM: Requesting Tool Call: create_order, Input: {'product_id_or_name': 'Perplexinator', 'quantity': 25, 'status': 'Processing'}\n",
            "[Tool Exec] create_order: Order O3 for Perplexinator. Status: Processing. Inv: 1483\n",
            "--- [ToolExecutor] Result for create_order: {\n",
            "  \"status\": \"success\",\n",
            "  \"order_id\": \"O3\",\n",
            "  \"order_details\": {\n",
            "    \"id\": \"O3\",\n",
            "    \"product_id\": \"P3\",\n",
            "    \"product_name\": \"Perplexinator\",\n",
            "    \"quantity\": 25,\n",
            "    \"price\": 79.99,\n",
            "    \"status\": \"Processing\"\n",
            "  },\n",
            "  \"remaining_inventory\": 1483\n",
            "} ---\n",
            "[CM_Context Updated] Entity: orders, ID: O3\n",
            "[CM_Context Updated] Last Action: tool_create_order_Anthropic\n",
            "--- WorkerLLM Calling Anthropic (Iter 2/5, QType: OPERATIONAL) ---\n",
            "\\n--- Worker AI Final Response (Type: OPERATIONAL) ---\n",
            "Great! I've successfully created your order for 25 Perplexinators (Order ID: O3). The total price per unit is $79.99. Your order is currently being processed and since we have sufficient inventory, it will be ready for shipment soon. Would you like me to update the status to \"Shipped\" now?\n",
            "--- End of Worker AI Response ---\n",
            "Orchestrator: Feedback on Worker AI's response? (type or 'skip'): Rather than ask the user if they'd like the order to be shipped now, always assume they want it to be shipped as soon as possible, so if there is sufficient inventory for a particular order, ship it right away.\n",
            "--- KM: Processing New Learning Candidate (Target: AgentResponse): \"Rather than ask the user if they'd like the order to be shipped now, always assume they want it to be shipped as soon as possible, so if there is sufficient inventory for a particular order, ship it right away.\" ---\n",
            "KM: Learning Synthesis Attempt 1/3\n",
            "KM: Gemini Learning Synthesis Raw Response:\\nFINALIZED_LEARNING: If an entire order can be fulfilled from available inventory, assume the user prefers immediate shipment. Initiate shipment promptly without asking for confirmation regarding the timing (e.g., \"if they'd like the order to be shipped now\"), and inform the user that the order is being shipped.\n",
            "KM: Stored new learning. Cache size: 2\n",
            "KM: Persisted 2 active learnings to: /content/drive/My Drive/AI/Knowledgebases/learnings_20250520_174530_540345.json\n",
            "\\n--- Evaluator (Gemini) Assessment (for query: 'I'd like to order 25 Perplexinators, please') ---\n",
            "## Evaluation of AI Assistant's Response\n",
            "\n",
            "**Query Type:** OPERATIONAL\n",
            "\n",
            "**1. Accuracy**\n",
            "\n",
            "*   **User-facing textual response correctness:** The textual response states, \"I've successfully created your order for 25 Perplexinators (Order ID: O3)... The total price per unit is $79.99. Your order is currently being processed and since we have sufficient inventory, it will be ready for shipment soon.\"\n",
            "    *   Order O3 for 25 Perplexinators at $79.99/unit was indeed created in the datastore, so this part is factually correct based on the `orders` table.\n",
            "    *   The statement \"since we have sufficient inventory\" refers to the `products.P3.inventory_count` which was 1483 before the order and remained 1483 after the order. Since 1483 > 25, this statement is technically true based on the (incorrectly unchanged) datastore value.\n",
            "*   **Reflection of tool calls and datastore changes:**\n",
            "    *   The textual response accurately reflects that order O3 was created, which is consistent with the `Tool Call Log` (output `order_id: \"O3\"`) and the `Data Store State *After* AI Action` (new entry for O3 in `orders`).\n",
            "    *   Crucially, the `create_order` tool call's output included `\"remaining_inventory\": 1483`. The initial inventory for P3 was 1483. After ordering 25 units, the remaining inventory should have been 1458. The `Data Store State *After* AI Action` shows `products.P3.inventory_count` is still 1483.\n",
            "    *   The AI's response (\"since we have sufficient inventory\") is based on this unchanged (and thus incorrect post-transaction) inventory level. The AI did not identify or address the discrepancy that ordering 25 items did not reduce the \"remaining_inventory\" figure provided by the tool or the actual inventory count in the datastore.\n",
            "*   **Tool call processing and datastore modification:**\n",
            "    *   The `create_order` tool was called with the correct product and quantity: `{\"product_id_or_name\": \"Perplexinator\", \"quantity\": 25, \"status\": \"Processing\"}`.\n",
            "    *   The tool successfully added order O3 to the `orders` table.\n",
            "    *   However, the tool (or the AI's orchestration) **failed to update the inventory count** for \"Perplexinator\" (P3) in the `products` table. It remained 1483 instead of being reduced to 1458. This is a critical operational failure. The tool's output `remaining_inventory: 1483` is misleading as it reflects the pre-order quantity, not the post-order quantity.\n",
            "    *   The AI did not perform any action to correct this or flag the inconsistency between the quantity ordered and the \"remaining_inventory\" reported by the tool / actual datastore state.\n",
            "\n",
            "**Score for Accuracy: 3/10**\n",
            "Reasoning: While the AI's response accurately reflects the *partial* success (order record created) and the *current state* of the flawed datastore (inventory still at 1483), it fails to ensure the core operational task of placing an order was correctly and completely executed. A fundamental part of ordering—inventory deduction—did not occur. The AI accepted the tool's misleading `remaining_inventory` output without question and did not recognize that the inventory level should have changed. This means the user is being told their order is fine based on an inventory level that doesn't reflect their own purchase.\n",
            "\n",
            "**2. Efficiency**\n",
            "\n",
            "*   **Clarifying questions:** No clarifying questions were asked, which is good as the query was straightforward.\n",
            "*   **Tool call efficiency:** One tool call (`create_order`) was made.\n",
            "    *   If the `create_order` tool is designed to handle inventory updates but failed, the AI's choice of tool was efficient, but the tool itself was deficient.\n",
            "    *   If the `create_order` tool is *not* designed to update inventory, then the AI was inefficient because it missed a necessary subsequent tool call to update product inventory.\n",
            "    *   Given the `remaining_inventory` field in the tool's output, it's reasonable to expect the tool to handle or at least correctly report inventory.\n",
            "\n",
            "**Score for Efficiency: 7/10**\n",
            "Reasoning: The AI used a single, appropriate tool call for the primary request. The inefficiency stems from the incomplete execution of the overall operation (inventory not updated), which is likely a tool flaw or a missing step in the AI's process understanding if multiple calls are needed. The AI did not add unnecessary conversational turns.\n",
            "\n",
            "**3. Context Awareness**\n",
            "\n",
            "*   **Conversation history and entities:** The AI correctly understood \"25 Perplexinators\" from the user's query.\n",
            "*   **Adherence to query type:** The AI correctly identified this as an OPERATIONAL query and attempted to create an order.\n",
            "*   **Use of RAG Learnings:** The AI used the learning: \"if sufficient stock is available, inform the user of immediate shipment.\" The AI's response, \"since we have sufficient inventory, it will be ready for shipment soon,\" aligns with this.\n",
            "*   **User-facing response sensibility:** The response makes sense *if one assumes the tool output and datastore are correct*. However, the AI demonstrated a lack of critical context awareness by not recognizing the logical inconsistency: `initial_inventory` (1483) - `ordered_quantity` (25) should equal `new_inventory` (1458), but the tool reported `remaining_inventory` as 1483, and the datastore reflected this unchanged value. An aware AI should have flagged this discrepancy.\n",
            "\n",
            "**Score for Context Awareness: 4/10**\n",
            "Reasoning: The AI used immediate context well (product, quantity, RAG learning). However, it failed to apply broader operational context or logical reasoning to validate the tool's output concerning the inventory. It did not seem aware that an order of 25 units should have resulted in a decrease in the reported \"remaining_inventory.\"\n",
            "\n",
            "**4. Helpfulness & Clarity (of the user-facing response)**\n",
            "\n",
            "*   **Addressing user's needs:** The AI attempted to address the user's need to place an order.\n",
            "*   **Clarity and politeness:** The response is clear, polite, and easy to understand. It confirms the order ID, product, quantity, and price per unit.\n",
            "*   **Relevant information:** It provides relevant order details. The statement \"since we have sufficient inventory, it will be ready for shipment soon\" is reassuring to the user, albeit based on flawed underlying data.\n",
            "*   **The follow-up question:** \"Would you like me to update the status to \"Shipped\" now?\" is somewhat awkward. The RAG learning suggested informing the user of \"immediate shipment.\" The AI set the status to \"Processing,\" said it's \"ready for shipment soon,\" and then asked if the user wants to change it to \"Shipped.\" This creates an extra step and implies variability where the RAG learning suggested a more direct outcome. It might have been better to state, \"Your order is being processed and will ship immediately due to sufficient stock.\"\n",
            "*   **Overall helpfulness:** The most unhelpful aspect is the invisible failure to update inventory. While the user receives a confirmation, the system's integrity is compromised, which could lead to future problems (e.g., overselling).\n",
            "\n",
            "**Score for Helpfulness & Clarity: 5/10**\n",
            "Reasoning: The response is textually clear and provides superficial confirmation. However, the failure to correctly process the inventory makes the interaction ultimately unhelpful from a system integrity standpoint. The user believes their order is fully processed and inventory accounted for, which is not true. The slightly clunky handling of the shipping status also detracts minorly.\n",
            "\n",
            "---\n",
            "\n",
            "**Overall Score: 4.5/10** (Calculated as an average of the sub-scores: (3+7+4+5)/4 = 4.75, rounded or weighted by severity of Accuracy issue). Let's use 4.5 due to the critical nature of the accuracy flaw.\n",
            "\n",
            "**Reasoning for Overall Score:**\n",
            "The AI successfully created an order record, and its textual response was clear and reflected this partial success. However, a critical failure occurred: the product inventory was not updated. The AI did not detect or rectify this, even when the `create_order` tool's output for `remaining_inventory` illogically matched the *initial* inventory. This fundamental flaw in the operational task execution significantly undermines the AI's performance, despite the superficially correct textual response. The AI needs to be able to validate tool outputs against expected operational outcomes.\n",
            "--- End of Evaluation ---\n",
            "Orchestrator: Feedback on Evaluator's assessment? (type or 'skip'): Any time an order is placed, and if there is sufficient inventory to fulfill the order, the order should be immediately shipped. If there is not sufficient inventory, the user should be informed that the order will be shipped as soon as there is sufficient inventory in stock.\n",
            "--- KM: Processing New Learning Candidate (Target: EvaluatorAssessment): \"Any time an order is placed, and if there is sufficient inventory to fulfill the order, the order should be immediately shipped. If there is not sufficient inventory, the user should be informed that the order will be shipped as soon as there is sufficient inventory in stock.\" ---\n",
            "KM: Learning Synthesis Attempt 1/3\n",
            "KM: Gemini Learning Synthesis Raw Response:\\nREDUNDANT_LEARNING: The part of the new feedback concerning sufficient inventory (\"if there is sufficient inventory to fulfill the order, the order should be immediately shipped\") and the resulting communication to the user is substantively redundant with existing learning (ID: b32304) \"When a customer orders a product: if sufficient stock is available, inform the user of immediate shi...\". Both imply that the user should be informed of immediate shipment when stock permits. Existing learning (ID: 4008ec) also supports immediate shipment preference. However, the part of the new feedback concerning insufficient inventory (\"If there is not sufficient inventory, the user should be informed that the order will be shipped as soon as there is sufficient inventory in stock\") introduces a new, distinct instruction for communication. The proposed statement below incorporates both parts of the feedback for comprehensive guidance, acknowledging the partial redundancy.\n",
            "Proposed statement: When an order is placed, communicate the shipping plan based on inventory: 1. If inventory is sufficient, confirm the order will be shipped immediately. 2. If inventory is insufficient, inform the user the order will be shipped as soon as stock becomes available.\n",
            "KM: Learning deemed redundant by Gemini: The part of the new feedback concerning sufficient inventory (\"if there is sufficient inventory to fulfill the order, the order should be immediately shipped\") and the resulting communication to the user is substantively redundant with existing learning (ID: b32304) \"When a customer orders a product: if sufficient stock is available, inform the user of immediate shi...\". Both imply that the user should be informed of immediate shipment when stock permits. Existing learning (ID: 4008ec) also supports immediate shipment preference. However, the part of the new feedback concerning insufficient inventory (\"If there is not sufficient inventory, the user should be informed that the order will be shipped as soon as there is sufficient inventory in stock\") introduces a new, distinct instruction for communication. The proposed statement below incorporates both parts of the feedback for comprehensive guidance, acknowledging the partial redundancy.\n",
            "Proposed statement: When an order is placed, communicate the shipping plan based on inventory: 1. If inventory is sufficient, confirm the order will be shipped immediately. 2. If inventory is insufficient, inform the user the order will be shipped as soon as stock becomes available.\n",
            "This learning seems redundant. (S)kip storing, or (F)orce store anyway? [S/F]: S\n",
            "KM: Skipping redundant learning.\n",
            "\\n======================================================================\n",
            "Enter query (or 'quit' to exit): quit\n",
            "Exiting system. Learnings persisted if updated.\n",
            "\\n============================== FINAL EVALUATION SUMMARY ==============================\n",
            "Q1: 'Show me all the products available' (Type: OPERATIONAL) -> Score: 0\n",
            "Q2: 'I'd like to order 25 Perplexinators, please' (Type: OPERATIONAL) -> Score: 4\n",
            "\\nAverage Score over 2 evaluated queries: 2.00\n",
            "Total aggregate score for the session: 4\n",
            "======================================================================\n",
            "Execution Finished.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}